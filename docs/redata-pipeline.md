# Pipeline REData - Red El√©ctrica de Espa√±a

## Descripci√≥n General

Este documento describe el pipeline completo de ingesta, transformaci√≥n y modelado de datos desde la API REData de Red El√©ctrica de Espa√±a (REE). El pipeline sigue la arquitectura Medallion con tres capas: Bronze (ingesta), Silver (limpieza) y Gold (modelo dimensional).

---

## üîó Fuente de Datos

- **API**: https://apidatos.ree.es
- **Tipo**: REST API p√∫blica (sin autenticaci√≥n)
- **Formato**: JSON
- **Documentaci√≥n oficial**: [Red El√©ctrica API](https://www.ree.es/es/apidatos)

### Endpoints Utilizados

| Categor√≠a      | Endpoint                                    | Descripci√≥n                                              |
| -------------- | ------------------------------------------- | -------------------------------------------------------- |
| **balance**    | `balance-electrico`                         | Balance del sistema el√©ctrico con m√©tricas anidadas      |
| **demanda**    | `evolucion`                                 | Evoluci√≥n temporal de la demanda el√©ctrica               |
| **generacion** | `estructura-generacion`                     | Generaci√≥n por tecnolog√≠a (nuclear, e√≥lica, solar, etc.) |
| **generacion** | `estructura-generacion-emisiones-asociadas` | Generaci√≥n con clasificaci√≥n de emisiones CO‚ÇÇ            |
| **generacion** | `estructura-renovables`                     | Desglose detallado de tecnolog√≠as renovables             |
| **generacion** | `evolucion-renovable-no-renovable`          | Comparativa renovable vs no renovable                    |

### Par√°metros de Consulta

- **time_trunc**: `month` (agregaci√≥n mensual)
- **geo_trunc**: `electric_system`
- **geo_limit**: `ccaa` (comunidades aut√≥nomas)
- **geo_ids**: 20 comunidades + sistemas insulares + territorios
- **start_date** / **end_date**: Rango temporal (m√°x. 24 meses por petici√≥n)

---

## üì¶ Capa Bronze - Ingesta

### Notebook: `nb_redata_01_ingest.ipynb`

#### Objetivo

Extraer datos crudos desde la API REData y almacenarlos sin transformaciones en formato JSON/Delta.

#### Configuraci√≥n Externa

El pipeline utiliza un archivo de configuraci√≥n centralizado:

```json
{
  "ccaa_ids": {
    "Pen√≠nsula": 8741,
    "Andaluc√≠a": 4,
    "Arag√≥n": 5,
    ...
  },
  "api_config": {
    "balance": ["balance-electrico"],
    "demanda": ["evolucion"],
    "generacion": [
      "estructura-generacion",
      "estructura-generacion-emisiones-asociadas",
      "estructura-renovables",
      "evolucion-renovable-no-renovable"
    ]
  }
}
```

**Ubicaci√≥n**: `Files/config/redata_config.json`

#### Clase Principal: `REDataAPIExplorer`

```python
class REDataAPIExplorer:
    def __init__(self, base_lakehouse_path, start_date, end_date):
        self.base_url = "https://apidatos.ree.es"
        self.ccaa_ids = config["ccaa_ids"]
        self.api_config = config["api_config"]
        self.start_date = datetime.fromisoformat(start_date)
        self.end_date = datetime.fromisoformat(end_date)
```

#### Funcionalidades Clave

##### 1. Construcci√≥n de URLs

```python
def build_api_url(self, lang, category, widget, time_trunc, geo_id, geo_name):
    """Construye URL con par√°metros - SIEMPRE usando geo_limit=ccaa"""
    params = {
        "start_date": start_str,
        "end_date": end_str,
        "time_trunc": "month",
        "geo_trunc": "electric_system",
        "geo_limit": "ccaa",
        "geo_ids": str(geo_id)
    }
```

##### 2. Almacenamiento con Metadata Enriquecida

```python
def save_to_bronze(self, data, category, widget, region, timestamp, ...):
    enriched_data = {
        "request_metadata": {
            "geo_id": geo_id,
            "geo_name": geo_name,
            "category": category,
            "widget": widget,
            "time_trunc": time_trunc,
            "ingestion_timestamp": timestamp,
            "start_date": self.start_date.isoformat(),
            "end_date": self.end_date.isoformat()
        },
        "api_response": data
    }
```

##### 3. Sistema de Logging

- **Success Log**: `Files/bronze/REDATA/logs/success.log`
- **Error Log**: `Files/bronze/REDATA/logs/error.log`

Cada petici√≥n registra:

- Timestamp UTC con microsegundos
- URL completa de la petici√≥n
- Estado (OK/FAIL)

#### Estructura de Salida

```
Files/bronze/REDATA/data/
‚îú‚îÄ‚îÄ balance/
‚îÇ   ‚îî‚îÄ‚îÄ balance-electrico/
‚îÇ       ‚îî‚îÄ‚îÄ month/
‚îÇ           ‚îî‚îÄ‚îÄ brz-andalucia-balance-balance-electrico-month-2025-10-15t12-30-45-123456z.json
‚îú‚îÄ‚îÄ demanda/
‚îÇ   ‚îî‚îÄ‚îÄ evolucion/
‚îÇ       ‚îî‚îÄ‚îÄ month/
‚îÇ           ‚îî‚îÄ‚îÄ brz-peninsula-demanda-evolucion-month-2025-10-15t12-31-02-654321z.json
‚îî‚îÄ‚îÄ generacion/
    ‚îú‚îÄ‚îÄ estructura-generacion/
    ‚îú‚îÄ‚îÄ estructura-generacion-emisiones-asociadas/
    ‚îú‚îÄ‚îÄ estructura-renovables/
    ‚îî‚îÄ‚îÄ evolucion-renovable-no-renovable/
```

#### Decisiones de Dise√±o

**‚úÖ SOLO datos mensuales**

- Se descartaron datos diarios por volumen excesivo y granularidad innecesaria para el alcance actual
- Facilita el an√°lisis de tendencias a medio/largo plazo
- Reduce la complejidad del modelo dimensional

**‚úÖ SOLO geo_limit=ccaa**

- Se excluyen otros niveles geogr√°ficos (pen√≠nsula, sistema el√©ctrico nacional)
- Evita duplicaci√≥n de datos a diferentes niveles de agregaci√≥n
- Mantiene coherencia en el modelo relacional

**‚úÖ Limitaci√≥n de 24 meses por petici√≥n**

- Para cubrir enero 2023 - junio 2025 (30 meses) se requieren 2 llamadas por endpoint
- Se implement√≥ gesti√≥n iterativa de rangos temporales

#### Ejemplo de Ejecuci√≥n

```python
explorer = REDataAPIExplorer(
    base_lakehouse_path="Files/bronze/REDATA",
    start_date="2023-01-01T00:00:00Z",
    end_date="2025-06-30T23:59:59Z"
)

results = explorer.explore_all(max_combinations=None)
summary = explorer.analyze_results(results)

# Output esperado:
# üöÄ Ejecutando 120 combinaciones (SOLO MENSUALES + CCAA)
# üìä Resumen: {'total': 120, 'ok': 118, 'with_data': 116, 'failures': 2}
```

---

## üßπ Capa Silver - Limpieza y Normalizaci√≥n

### Notebook: `nb_redata_02_json_to_delta.ipynb` (JSON ‚Üí Delta)

#### Objetivo

Transformar los archivos JSON crudos de Bronze en tablas Delta estructuradas, aplicando el primer nivel de limpieza.

#### Funciones Clave

##### 1. Detecci√≥n de Archivos Nuevos

```python
def get_new_json_files(category, widget, time_trunc):
    """
    Obtiene lista de archivos JSON a√∫n no procesados.
    ‚úÖ SOLO procesa 'month' - ignora 'day'
    """
    if time_trunc != "month":
        return []

    # Verificar en tabla de log si ya fue procesado
    if not check_json_log_exists(...):
        json_paths.append(full_path)
```

##### 2. Extracci√≥n de Metadata

```python
def extract_metadata(df_raw, category, widget, time_trunc):
    """Extrae metadatos del nodo api_response.data"""
    sample = df_raw.select(
        col("api_response.data.type").alias("data_type"),
        col("api_response.data.id").alias("data_id"),
        col("api_response.data.attributes.title").alias("data_title"),
        col("api_response.data.attributes.last-update").alias("data_last_update")
    ).first()
```

Estos metadatos se guardan en una tabla separada `brz_redata_metadata` para consulta r√°pida.

##### 3. Normalizaci√≥n de Fechas (CR√çTICO)

**‚ö†Ô∏è PROBLEMA IDENTIFICADO**: Las fechas ven√≠an con offset UTC (`+02:00` en verano, `+01:00` en invierno) y al parsearse se aplicaban conversiones err√≥neas que desplazaban las fechas hasta 2 horas.

**‚úÖ SOLUCI√ìN IMPLEMENTADA**:

```python
def parse_datetime_local(datetime_str):
    """
    Convierte datetime preservando la fecha local sin convertir a UTC

    Entrada:  "2024-10-01T00:00:00.000+02:00"
    Salida:   "2024-10-01 00:00:00" (mantiene fecha/hora local)
    """
    return regexp_replace(
        datetime_str,
        r"^(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}).*$",
        "$1"
    )

# Uso en transformaci√≥n:
to_timestamp(parse_datetime_local(col("val.datetime"))).alias("datetime")
```

##### 4. Procesamiento seg√∫n Estructura

La API REData tiene dos tipos de estructura JSON:

**A. Balance El√©ctrico** (con `content` anidado)

```python
def process_balance_electrico(df_raw, category, widget, time_trunc):
    # Nivel 1: Explotar series
    df_included = df_meta.select("*", explode("included").alias("series"))

    # Nivel 2: Explotar content (m√©tricas anidadas)
    df_step1 = df_included.select(..., col("series.attributes.content").alias("content"))
    df_step2 = df_step1.select("*", explode("content").alias("metric"))

    # Nivel 3: Explotar values
    df_step3 = df_step2.select(..., col("metric.attributes.values").alias("values"))
    df_final = df_step3.select("*", explode("values").alias("val"))
```

**B. Widgets Est√°ndar** (sin `content`)

```python
def process_standard_widget(df_raw, category, widget, time_trunc):
    # Nivel 1: Explotar series
    df_included = df_meta.select("*", explode("included").alias("series"))

    # Nivel 2: Explotar values directamente
    df_step1 = df_included.select(..., col("series.attributes.values").alias("values"))
    df_final = df_step1.select("*", explode("values").alias("val"))
```

#### Tablas Generadas

| Tabla Bronze                                                            | Descripci√≥n                             | Registros |
| ----------------------------------------------------------------------- | --------------------------------------- | --------- |
| `brz_redata_balance_balance_electrico_month`                            | Balance el√©ctrico con m√©tricas anidadas | ~8,460    |
| `brz_redata_demanda_evolucion_month`                                    | Evoluci√≥n de la demanda                 | ~640      |
| `brz_redata_generacion_estructura_generacion_month`                     | Generaci√≥n por tecnolog√≠a               | ~5,312    |
| `brz_redata_generacion_estructura_generacion_emisiones_asociadas_month` | Generaci√≥n con emisiones                | ~4,672    |
| `brz_redata_generacion_estructura_renovables_month`                     | Desglose renovables                     | ~3,365    |
| `brz_redata_generacion_evolucion_renovable_no_renovable_month`          | Renovable vs no renovable               | ~1,263    |
| `brz_redata_metadata`                                                   | Metadatos de endpoints                  | ~6        |

#### Sistema de Auditor√≠a

Tabla de log de procesamiento:

```python
# Estructura de brz_json_log
{
    "api_name": "generacion/estructura-generacion/month",
    "source_file": "Files/bronze/.../brz-andalucia-generacion-...-month-2025-10-15...json",
    "target_table": "brz_redata_generacion_estructura_generacion_month",
    "ingestion_date": "2025-10-15T12:45:30"
}
```

Esto evita reprocesar archivos ya transformados.

---

### Notebook: `nb_redata_04_brz_to_slv.ipynb` (Bronze ‚Üí Silver)

#### Objetivo

Aplicar transformaciones avanzadas de calidad de datos, incluyendo deduplicaci√≥n, normalizaci√≥n de IDs y optimizaci√≥n de esquema.

#### Configuraci√≥n de Claves de Negocio

El notebook carga una configuraci√≥n que define las **claves de unicidad** (business keys) para cada tipo de tabla:

```json
{
  "business_keys": {
    "balance_balance_electrico": {
      "keys": ["geo_id", "datetime", "series_type", "metric_type"],
      "description": "Balance el√©ctrico con m√©tricas anidadas"
    },
    "demanda_evolucion": {
      "keys": ["geo_id", "datetime", "series_type"],
      "description": "Evoluci√≥n de la demanda el√©ctrica"
    },
    "generacion_estructura_generacion": {
      "keys": ["geo_id", "datetime", "series_type"],
      "description": "Estructura de generaci√≥n por tecnolog√≠a"
    }
  }
}
```

#### Transformaciones Aplicadas

##### 1. Deduplicaci√≥n por Claves de Negocio

**‚ö†Ô∏è PROBLEMA**: M√∫ltiples registros para la misma combinaci√≥n (geo_id, datetime, series_type) debido a reingestas con diferentes timestamps de captura.

**‚úÖ SOLUCI√ìN**:

```python
def deduplicate_by_business_keys(df, business_keys: list):
    """
    Deduplica usando SOLO claves de negocio.
    Mantiene el registro con ingestion_timestamp m√°s reciente.
    """
    window = Window.partitionBy(business_keys).orderBy(desc("ingestion_timestamp"))

    df_deduped = df.withColumn("_row_num", row_number().over(window)) \
                   .filter(col("_row_num") == 1) \
                   .drop("_row_num")

    return df_deduped
```

**Resultado**: Se eliminaron 720 registros duplicados (3.0% del total de 23,712).

##### 2. Normalizaci√≥n de IDs

Algunos IDs act√∫an como placeholders redundantes:

```python
def normalize_ids(df):
    """Normaliza IDs: NULL si son placeholders"""
    if 'metric_id' in df.columns and 'metric_group_id' in df.columns:
        df = df.withColumn("metric_id",
            when(col("metric_id") == col("metric_group_id"), None)
            .otherwise(col("metric_id")))

    return df
```

##### 3. Eliminaci√≥n de Columnas

**Columnas eliminadas autom√°ticamente**:

1. **Decorativas** (sin valor anal√≠tico):

   - `metric_icon`, `metric_color`
   - `series_icon`, `series_color`

2. **Columnas con >95% nulos** (umbral configurable)

3. **Redundantes** (detectadas mediante an√°lisis de cardinalidad):
   - Si `series_type` tiene 1:1 con `series_title`, se elimina `series_title`
   - Si `metric_type` tiene 1:1 con `metric_title`, se elimina `metric_title`

```python
def get_columns_to_drop(df):
    to_drop = []

    # 1. Decorativas
    to_drop.extend(["metric_icon", "metric_color", "series_icon", "series_color"])

    # 2. Alto % nulos
    null_pct = calculate_null_percentage(df)
    to_drop.extend([c for c, p in null_pct.items() if p > 0.95])

    # 3. Redundantes (conservar _type)
    for keep, *check in [('series_type', 'series_title', 'series_id')]:
        # An√°lisis de cardinalidad...

    return list(set(to_drop))
```

##### 4. Adici√≥n de Columnas de Particionamiento

```python
def add_partition_columns(df):
    """Agrega year/month desde datetime"""
    return df.withColumn("year", year(col("datetime"))) \
             .withColumn("month", month(col("datetime")))
```

##### 5. Particionamiento Estrat√©gico

```python
def should_partition(table_name, record_count):
    """Decide estrategia de particionamiento"""
    if '_month' in table_name and record_count > 500:
        return True, ["year"]
    return False, []
```

**Resultado**: Todas las tablas Silver se particionan por `year` para optimizaci√≥n de queries.

#### Validaci√≥n de Calidad

Tras cada transformaci√≥n, se validan:

- ‚úÖ Ausencia de duplicados seg√∫n claves de negocio
- ‚úÖ Valores no nulos en campos cr√≠ticos (`datetime`, `value`)
- ‚úÖ Integridad referencial de claves for√°neas

```python
# Verificaci√≥n post-escritura
df_verify = spark.table(f"{LAKEHOUSE_SILVER}.{silver_table}")
verify_validation = validate_business_keys(df_verify, business_keys, silver_table)

if verify_validation["duplicate_groups"] > 0:
    print(f"‚ö†Ô∏è ADVERTENCIA: A√∫n hay duplicados")
else:
    print(f"‚úÖ Verificaci√≥n: Sin duplicados en Silver")
```

#### Tablas Silver Generadas

| Tabla Silver                                                            | Registros | Columnas | Duplicados Eliminados |
| ----------------------------------------------------------------------- | --------- | -------- | --------------------- |
| `slv_redata_balance_balance_electrico_month`                            | 8,202     | 17       | 258                   |
| `slv_redata_demanda_evolucion_month`                                    | 620       | 11       | 20                    |
| `slv_redata_generacion_estructura_generacion_month`                     | 5,152     | 13       | 160                   |
| `slv_redata_generacion_estructura_generacion_emisiones_asociadas_month` | 4,532     | 13       | 140                   |
| `slv_redata_generacion_estructura_renovables_month`                     | 3,262     | 14       | 103                   |
| `slv_redata_generacion_evolucion_renovable_no_renovable_month`          | 1,224     | 13       | 39                    |

**Reducci√≥n total**: 720 registros duplicados (3.0%)

---

## ‚≠ê Capa Gold - Modelo Dimensional

### Notebook: `nb_redata_06_create_dimensions.ipynb`

#### Objetivo

Crear dimensiones desnormalizadas que representan las entidades clave del dominio.

#### Dimensiones Creadas

##### 1. `dim_date` - Dimensi√≥n Temporal

```python
dim_date = df_dates.select(
    col("datetime").alias("date"),
    year("datetime").alias("year"),
    month("datetime").alias("month"),
    dayofmonth("datetime").alias("day"),
    quarter("datetime").alias("quarter"),
    (floor((month("datetime") - 1) / 6) + 1).cast("integer").alias("semester"),
    dayofweek("datetime").alias("day_of_week_num"),
    date_format("datetime", "EEEE").alias("day_of_week_name"),
    date_format("datetime", "MMMM").alias("month_name"),
    when(dayofweek("datetime").isin([1, 7]), True).otherwise(False).alias("is_weekend")
).distinct()

# A√±adir clave surrogada
window = Window.orderBy("date")
dim_date = dim_date.withColumn("date_key", row_number().over(window))
```

**Atributos**:

- `date_key` (PK): Clave surrogada autoincremental
- `date`: Fecha completa (formato YYYY-MM-DD)
- `year`, `month`, `day`: Componentes temporales
- `quarter`, `semester`: Agregaciones fiscales
- `day_of_week_num`, `day_of_week_name`: D√≠a de la semana
- `month_name`: Nombre del mes
- `is_weekend`: Flag booleano

**Registros**: 34 meses (enero 2023 - junio 2025)

##### 2. `dim_geography` - Dimensi√≥n Geogr√°fica

```python
df_geo = spark.table(f"{LAKEHOUSE_SILVER}.slv_redata_generacion_estructura_generacion_month") \
    .select("geo_id", "geo_name") \
    .distinct()

window = Window.orderBy("geo_id")
dim_geography = df_geo.withColumn("geography_key", row_number().over(window))
```

**Atributos**:

- `geography_key` (PK): Clave surrogada
- `geo_id`: ID oficial de REE
- `geo_name`: Nombre de la comunidad aut√≥noma

**Registros**: 20 comunidades aut√≥nomas

**Ejemplo de datos**:

```
+-------------+------+----------------------+
|geography_key|geo_id|geo_name              |
+-------------+------+----------------------+
|1            |4     |Andaluc√≠a             |
|2            |5     |Arag√≥n                |
|3            |6     |Cantabria             |
|4            |7     |Castilla-La Mancha    |
|5            |8     |Castilla y Le√≥n       |
|6            |9     |Catalu√±a              |
|7            |10    |Pa√≠s Vasco            |
|8            |11    |Principado de Asturias|
|9            |13    |Comunidad de Madrid   |
|10           |14    |Comunidad de Navarra  |
+-------------+------+----------------------+
```

##### 3. `dim_technology` - Dimensi√≥n Tecnol√≥gica

**Fuentes de datos**:

1. `slv_redata_generacion_estructura_generacion_month` ‚Üí categor√≠as
2. `slv_redata_generacion_estructura_generacion_emisiones_asociadas_month` ‚Üí flag de emisiones CO‚ÇÇ

```python
# Paso 1: Obtener categor√≠as
df_base = spark.table("slv_redata_generacion_estructura_generacion_month") \
    .select("series_type", "series_attribute_type") \
    .distinct()

# Paso 2: Obtener emisiones
df_emissions = spark.table("slv_redata_generacion_estructura_generacion_emisiones_asociadas_month") \
    .select("series_type", col("series_attribute_type").alias("emissions_attr")) \
    .distinct()

# Paso 3: JOIN para combinar
dim_technology = df_base.join(df_emissions, "series_type", "left")

# Paso 4: Crear flag de emisiones
dim_technology = dim_technology.withColumn(
    "has_co2_emissions",
    when(col("emissions_attr") == "Con emisiones de CO2 eq.", True)
    .when(col("emissions_attr") == "Sin emisiones de CO2 eq.", False)
    .otherwise(None)
)
```

**Atributos**:

- `technology_key` (PK): Clave surrogada
- `series_type`: Nombre de la tecnolog√≠a (ej: "E√≥lica", "Nuclear", "Solar fotovoltaica")
- `category`: Clasificaci√≥n (`Renovable`, `No-Renovable`, `Generaci√≥n total`)
- `has_co2_emissions`: Flag booleano (TRUE/FALSE/NULL)

**Registros**: 18 tecnolog√≠as

**Distribuci√≥n por categor√≠a**:

```
+----------------+-----+
|        category|count|
+----------------+-----+
|Generaci√≥n total|    1|
|    No-Renovable|    9|
|       Renovable|    8|
+----------------+-----+
```

**Tecnolog√≠as CON emisiones** (has_co2_emissions = TRUE):

- Carb√≥n
- Ciclo combinado
- Cogeneraci√≥n
- Fuel + Gas
- Motores di√©sel
- Residuos no renovables
- Turbina de gas
- Turbina de vapor

**Tecnolog√≠as SIN emisiones** (has_co2_emissions = FALSE):

- E√≥lica
- Hidroe√≥lica
- Hidr√°ulica
- Nuclear
- Otras renovables
- Residuos renovables
- Solar fotovoltaica
- Solar t√©rmica

---

### Notebook: `nb_redata_07_create_facts.ipynb`

#### Objetivo

Crear tablas de hechos que contienen las m√©tricas cuantitativas con referencias a las dimensiones.

#### Tabla de Hechos: `fact_generation_month`

```python
# Leer datos de generaci√≥n desde Silver
df_gen_month = spark.table("slv_redata_generacion_estructura_generacion_month") \
    .select("datetime", "geo_id", "series_type", "value", "percentage")

# Join con dimensiones
fact_gen_month = df_gen_month \
    .join(dim_date, df_gen_month.datetime == dim_date.date, "left") \
    .join(dim_geography, df_gen_month.geo_id == dim_geography.geo_id, "left") \
    .join(dim_technology, df_gen_month.series_type == dim_technology.series_type, "left") \
    .select(
        col("date_key"),
        col("geography_key"),
        col("technology_key"),
        col("value").alias("generation_mwh"),
        col("percentage").alias("generation_percentage"),
        year("datetime").alias("year")
    )
```

**Atributos**:

- `date_key` (FK ‚Üí `dim_date`)
- `geography_key` (FK ‚Üí `dim_geography`)
- `technology_key` (FK ‚Üí `dim_technology`)
- `generation_mwh`: Generaci√≥n en megavatios-hora
- `generation_percentage`: Porcentaje sobre el total
- `year`: Columna de particionamiento

**Registros**: 5,559

**Particionamiento**: `year` (optimizaci√≥n de queries)

**Validaci√≥n de Integridad Referencial**:

```python
nulls_date = fact_gen_month.filter(col("date_key").isNull()).count()
nulls_geo = fact_gen_month.filter(col("geography_key").isNull()).count()
nulls_tech = fact_gen_month.filter(col("technology_key").isNull()).count()

# Resultado esperado: 0 nulls en todas las claves for√°neas
```

---

## üìä Modelo Dimensional Final (Star Schema)

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   dim_date   ‚îÇ
                    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                    ‚îÇ date_key (PK)‚îÇ
                    ‚îÇ date         ‚îÇ
                    ‚îÇ year         ‚îÇ
                    ‚îÇ month        ‚îÇ
                    ‚îÇ quarter      ‚îÇ
                    ‚îÇ is_weekend   ‚îÇ
                    ‚îÇ ...          ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                           ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                  ‚îÇ                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ dim_geography  ‚îÇ  ‚îÇ fact_generation_    ‚îÇ  ‚îÇ dim_technology   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îÇ       month         ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇgeography_key(PK)‚îÇ‚óÑ‚îÄ‚î§‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§‚îÄ‚ñ∫‚îÇtechnology_key(PK)‚îÇ
‚îÇ geo_id         ‚îÇ  ‚îÇ date_key (FK)       ‚îÇ  ‚îÇ series_type      ‚îÇ
‚îÇ geo_name       ‚îÇ  ‚îÇ geography_key (FK)  ‚îÇ  ‚îÇ category         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ technology_key (FK) ‚îÇ  ‚îÇ has_co2_emissions‚îÇ
                    ‚îÇ generation_mwh      ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ generation_percentage‚îÇ
                    ‚îÇ year (partition)    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Cardinalidades**:

- `dim_date`: 34 registros (34 meses)
- `dim_geography`: 20 registros (20 CCAA)
- `dim_technology`: 18 registros (18 tecnolog√≠as)
- `fact_generation_month`: 5,559 registros

**Granularidad**: Mensual por comunidad aut√≥noma y tecnolog√≠a

---

## üîç An√°lisis Exploratorio de Datos (EDA)

Tras cada capa de transformaci√≥n, se ejecutaron an√°lisis exploratorios automatizados:

### Post-Bronze

- Verificaci√≥n de completitud de respuestas de la API
- An√°lisis de estructura y anidamiento de JSONs
- Cobertura temporal y geogr√°fica
- Identificaci√≥n de campos disponibles

### Post-Silver

- Distribuci√≥n estad√≠stica de variables num√©ricas
- Detecci√≥n de outliers y valores an√≥malos
- Verificaci√≥n de unicidad seg√∫n claves de negocio
- An√°lisis de series temporales (detecci√≥n de gaps)
- Matrices de correlaci√≥n entre variables

### Post-Gold

- Validaci√≥n de integridad referencial (0 nulls en FKs)
- Comprobaci√≥n de cardinalidades esperadas
- Coherencia de agregaciones (sumas por dimensiones = totales)

---

## üìà M√©tricas de Calidad del Pipeline

| M√©trica                               | Valor                          |
| ------------------------------------- | ------------------------------ |
| **Registros originales (Bronze)**     | 23,712                         |
| **Registros finales (Silver)**        | 22,992                         |
| **Duplicados eliminados**             | 720 (3.0%)                     |
| **Tablas Bronze**                     | 6 + 1 metadata                 |
| **Tablas Silver**                     | 6 + 1 metadata                 |
| **Dimensiones Gold**                  | 3                              |
| **Tablas de Hechos Gold**             | 1                              |
| **Cobertura temporal**                | 34 meses (ene 2023 - jun 2025) |
| **Cobertura geogr√°fica**              | 20 comunidades aut√≥nomas       |
| **Tecnolog√≠as catalogadas**           | 18                             |
| **Validaci√≥n integridad referencial** | 100% (0 nulls en FKs)          |
| **Tablas particionadas**              | 100% de Silver y Gold          |

---

## üöÄ Ejecuci√≥n del Pipeline Completo

### Orden de Ejecuci√≥n

1. **Ingesta Bronze**:

   ```python
   # Ejecutar: nb_redata_01_ingest.ipynb
   # Output: ~120 archivos JSON en Files/bronze/REDATA/data/
   ```

2. **Transformaci√≥n Bronze ‚Üí Delta**:

   ```python
   # Ejecutar: nb_redata_02_json_to_delta.ipynb
   # Output: 7 tablas Delta en lh_bronze
   ```

3. **Limpieza Silver**:

   ```python
   # Ejecutar: nb_redata_04_brz_to_slv.ipynb
   # Output: 7 tablas optimizadas en lh_silver
   ```

4. **Creaci√≥n de Dimensiones Gold**:

   ```python
   # Ejecutar: nb_redata_06_create_dimensions.ipynb
   # Output: 3 tablas de dimensiones en lh_golden
   ```

5. **Creaci√≥n de Hechos Gold**:
   ```python
   # Ejecutar: nb_redata_07_create_facts.ipynb
   # Output: 1 tabla de hechos en lh_golden
   ```

### Tiempos de Ejecuci√≥n Estimados

| Notebook                         | Duraci√≥n Aproximada |
| -------------------------------- | ------------------- |
| `nb_redata_01_ingest`            | 10-15 min           |
| `nb_redata_02_json_to_delta`     | 5-8 min             |
| `nb_redata_04_brz_to_slv`        | 3-5 min             |
| `nb_redata_06_create_dimensions` | 1-2 min             |
| `nb_redata_07_create_facts`      | 1-2 min             |
| **TOTAL**                        | **20-32 min**       |

---

## üõ†Ô∏è Troubleshooting

### Problema: Duplicados persistentes tras Silver

**S√≠ntoma**: La verificaci√≥n post-escritura detecta duplicados.

**Causa**: Claves de negocio mal definidas para ese endpoint.

**Soluci√≥n**: Revisar `redata_config.json` y ajustar las `business_keys` para esa tabla.

### Problema: Errores 408 (Timeout) en ingesta

**S√≠ntoma**: Algunos endpoints devuelven timeout.

**Causa**: El rango temporal solicitado es demasiado amplio.

**Soluci√≥n**: Reducir el rango en la llamada a `REDataAPIExplorer` (usar ventanas de 12 meses en lugar de 24).

### Problema: Fechas desplazadas 1-2 horas

**S√≠ntoma**: Los datos de generaci√≥n no coinciden con las horas esperadas.

**Causa**: Conversi√≥n err√≥nea de zona horaria.

**Soluci√≥n**: Verificar que se est√° usando `parse_datetime_local()` en `nb_redata_02_json_to_delta.ipynb`.

### Problema: Nulls en claves for√°neas de Gold

**S√≠ntoma**: `fact_generation_month` tiene nulls en `date_key`, `geography_key` o `technology_key`.

**Causa**: Datos en Silver que no tienen correspondencia en las dimensiones.

**Soluci√≥n**:

1. Verificar que las dimensiones se crearon correctamente
2. Ejecutar queries de diagn√≥stico:
   ```sql
   -- Encontrar series_type sin correspondencia
   SELECT DISTINCT series_type
   FROM lh_silver.slv_redata_generacion_estructura_generacion_month
   WHERE series_type NOT IN (SELECT series_type FROM lh_golden.dim_technology)
   ```

---

## üìö Referencias Adicionales

- [Documentaci√≥n oficial API REData](https://www.ree.es/es/apidatos)
- [Arquitectura Medallion](./arquitectura-medallion.md)
- [Modelo Dimensional](./modelo-dimensional.md)
- [Problemas T√©cnicos y Soluciones](./problemas-tecnicos.md)

---

**√öltima actualizaci√≥n**: Octubre 2025  
**Mantenido por**: Equipo Enerlytics - Factor√≠a F5
