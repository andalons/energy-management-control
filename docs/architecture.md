# Arquitectura Medallion en Enerlytics

## IntroducciÃ³n

La **Arquitectura Medallion** (tambiÃ©n conocida como arquitectura de medallas o en capas) es un patrÃ³n de diseÃ±o ampliamente adoptado en entornos **Lakehouse** modernos. Organiza los datos en tres capas progresivas de refinamiento â€” **Bronze, Silver y Gold** â€” donde cada capa aÃ±ade valor y calidad a los datos del sistema.

Este documento describe cÃ³mo Enerlytics implementa esta arquitectura para transformar datos crudos del sistema elÃ©ctrico espaÃ±ol en informaciÃ³n lista para anÃ¡lisis de negocio.

---

## ğŸ¯ Principios Fundamentales

### 1. SeparaciÃ³n de Responsabilidades

Cada capa tiene un propÃ³sito especÃ­fico y bien definido:

- **Bronze**: Preservar la verdad inmutable
- **Silver**: Garantizar calidad y confiabilidad
- **Gold**: Optimizar para consumo de negocio

### 2. Incrementalidad

Los datos fluyen de forma unidireccional: Bronze â†’ Silver â†’ Gold. Cada capa puede actualizarse independientemente sin afectar a las anteriores.

### 3. Trazabilidad

Es posible rastrear cualquier dato en Gold hasta su origen en Bronze, garantizando reproducibilidad y auditorÃ­a completa.

### 4. Idempotencia

Las transformaciones pueden ejecutarse mÃºltiples veces produciendo el mismo resultado, facilitando la correcciÃ³n de errores.

### 5. Escalabilidad

La arquitectura soporta volÃºmenes crecientes de datos mediante particionamiento estratÃ©gico y formatos optimizados (Delta Lake).

---

## ğŸ“Š Las Tres Capas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA SOURCES                        â”‚
â”‚        REData API  â”‚  ESIOS API  â”‚  Open-Meteo API      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ Ingesta sin transformaciones
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BRONZE LAYER                         â”‚
â”‚                   (Raw Data Lake)                       â”‚
â”‚                                                         â”‚
â”‚  â€¢ Datos crudos en JSON/Delta                          â”‚
â”‚  â€¢ Estructura original de la API preservada            â”‚
â”‚  â€¢ Metadata de captura enriquecida                     â”‚
â”‚  â€¢ Sistema de logging de peticiones                    â”‚
â”‚  â€¢ Fuente de verdad inmutable                          â”‚
â”‚                                                         â”‚
â”‚  Files/bronze/REDATA/data/                             â”‚
â”‚    â”œâ”€â”€ balance/balance-electrico/month/                â”‚
â”‚    â”œâ”€â”€ demanda/evolucion/month/                        â”‚
â”‚    â””â”€â”€ generacion/.../month/                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ Limpieza y normalizaciÃ³n
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SILVER LAYER                         â”‚
â”‚                  (Curated Data Lake)                    â”‚
â”‚                                                         â”‚
â”‚  â€¢ DeduplicaciÃ³n mediante claves de negocio            â”‚
â”‚  â€¢ CorrecciÃ³n de zonas horarias                        â”‚
â”‚  â€¢ NormalizaciÃ³n de IDs y nomenclatura                 â”‚
â”‚  â€¢ EliminaciÃ³n de columnas decorativas                 â”‚
â”‚  â€¢ ValidaciÃ³n de integridad de datos                   â”‚
â”‚  â€¢ AnÃ¡lisis exploratorio (EDA)                         â”‚
â”‚  â€¢ Tablas Delta particionadas                          â”‚
â”‚                                                         â”‚
â”‚  lh_silver.slv_redata_*_month                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ Modelado dimensional
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     GOLD LAYER                          â”‚
â”‚               (Business-Ready Analytics)                â”‚
â”‚                                                         â”‚
â”‚  â€¢ Modelo dimensional (star schema)                    â”‚
â”‚  â€¢ Dimensiones desnormalizadas                         â”‚
â”‚  â€¢ Tablas de hechos agregadas                          â”‚
â”‚  â€¢ Integridad referencial garantizada                  â”‚
â”‚  â€¢ Optimizado para queries de BI                       â”‚
â”‚  â€¢ Listo para Power BI                                 â”‚
â”‚                                                         â”‚
â”‚  lh_golden:                                            â”‚
â”‚    â”œâ”€â”€ dim_date                                        â”‚
â”‚    â”œâ”€â”€ dim_geography                                   â”‚
â”‚    â”œâ”€â”€ dim_technology                                  â”‚
â”‚    â””â”€â”€ fact_generation_month                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¥‰ Capa Bronze - Raw Data Lake

### PropÃ³sito

Actuar como **registro histÃ³rico inmutable** de todos los datos obtenidos de fuentes externas, preservando su estructura original.

### CaracterÃ­sticas

#### âœ… Inmutabilidad

- Los archivos Bronze **nunca** se modifican una vez escritos
- Si hay errores de ingesta, se corrigen reingestionando con nuevo timestamp
- Permite auditorÃ­a completa y reproducibilidad

#### âœ… Estructura Original

```json
{
  "request_metadata": {
    "geo_id": 4,
    "geo_name": "AndalucÃ­a",
    "category": "generacion",
    "widget": "estructura-generacion",
    "time_trunc": "month",
    "ingestion_timestamp": "2025-10-15T12:30:45.123456Z",
    "start_date": "2023-01-01T00:00:00",
    "end_date": "2025-06-30T23:59:59"
  },
  "api_response": {
    "included": [
      {
        "type": "Nuclear",
        "attributes": {
          "title": "Nuclear",
          "values": [
            { "datetime": "2023-01-01T00:00:00.000+01:00", "value": 12345.67 }
          ]
        }
      }
    ]
  }
}
```

Se preserva:

- La respuesta JSON completa de la API
- Metadata de contexto (quÃ© se pidiÃ³, cuÃ¡ndo, con quÃ© parÃ¡metros)
- Timestamps de captura para trazabilidad

#### âœ… Sistema de Logging

Cada peticiÃ³n a la API se registra en:

**Success Log** (`logs/success.log`):

```
2025-10-15T12:30:45.123456Z OK https://apidatos.ree.es/es/datos/generacion/...
2025-10-15T12:30:46.789012Z OK https://apidatos.ree.es/es/datos/demanda/...
```

**Error Log** (`logs/error.log`):

```
2025-10-15T12:31:00.555555Z FAIL https://apidatos.ree.es/es/datos/mercados/...
```

Esto permite:

- Diagnosticar problemas de conectividad
- Identificar endpoints problemÃ¡ticos
- Reintentar peticiones fallidas de forma selectiva

### OrganizaciÃ³n de Archivos

```
Files/bronze/REDATA/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ balance/
â”‚   â”‚   â””â”€â”€ balance-electrico/
â”‚   â”‚       â””â”€â”€ month/
â”‚   â”‚           â”œâ”€â”€ brz-andalucia-balance-balance-electrico-month-2025-10-15t12-30-45-123456z.json
â”‚   â”‚           â”œâ”€â”€ brz-aragon-balance-balance-electrico-month-2025-10-15t12-30-46-234567z.json
â”‚   â”‚           â””â”€â”€ ...
â”‚   â”œâ”€â”€ demanda/
â”‚   â”‚   â””â”€â”€ evolucion/
â”‚   â”‚       â””â”€â”€ month/
â”‚   â””â”€â”€ generacion/
â”‚       â”œâ”€â”€ estructura-generacion/
â”‚       â”‚   â””â”€â”€ month/
â”‚       â”œâ”€â”€ estructura-generacion-emisiones-asociadas/
â”‚       â”œâ”€â”€ estructura-renovables/
â”‚       â””â”€â”€ evolucion-renovable-no-renovable/
â””â”€â”€ logs/
    â”œâ”€â”€ success.log
    â””â”€â”€ error.log
```

**Nomenclatura de archivos**:

- `brz-{region}-{category}-{widget}-{time_trunc}-{timestamp}.json`
- El timestamp incluye microsegundos para evitar colisiones
- Caracteres especiales normalizados (slugify)

### Decisiones de DiseÃ±o en Bronze

#### 1. JSON vs. Delta

**DecisiÃ³n**: Usar JSON para archivos individuales, luego consolidar en Delta.

**Razones**:

- JSON preserva estructura anidada original
- Facilita inspecciÃ³n manual y debugging
- Delta se usa despuÃ©s para queries eficientes

#### 2. Un Archivo por PeticiÃ³n

**DecisiÃ³n**: Cada llamada a la API genera un archivo independiente.

**Razones**:

- Granularidad fina para reingesta selectiva
- Trazabilidad completa por peticiÃ³n
- Evita dependencias entre ingestas

#### 3. Metadata Enriquecida

**DecisiÃ³n**: Envolver respuesta de API en objeto con metadata.

**Razones**:

- Conocer contexto de cada dato (quÃ© se pidiÃ³, cuÃ¡ndo)
- Facilita diagnÃ³stico de problemas
- Permite filtrado eficiente en Silver

### Escenarios de Uso

#### ReingestiÃ³n Selectiva

Si se detecta un problema en los datos de octubre 2024:

```python
explorer = REDataAPIExplorer(
    base_lakehouse_path="Files/bronze/REDATA",
    start_date="2024-10-01T00:00:00Z",
    end_date="2024-10-31T23:59:59Z"
)
results = explorer.explore_all()
```

Los archivos antiguos se mantienen (inmutabilidad) y los nuevos se aÃ±aden con timestamp mÃ¡s reciente. Silver decidirÃ¡ cuÃ¡l conservar.

#### AuditorÃ­a de Fuente

Â¿QuÃ© nos devolviÃ³ exactamente la API para AndalucÃ­a en marzo 2024?

```bash
cat Files/bronze/REDATA/data/generacion/.../brz-andalucia-generacion-...-2024-03-01...json
```

---

## ğŸ¥ˆ Capa Silver - Curated Data Lake

### PropÃ³sito

Transformar datos crudos en **datasets confiables y consistentes**, aplicando reglas de negocio, validaciones de calidad y normalizaciones.

### CaracterÃ­sticas

#### âœ… DeduplicaciÃ³n Inteligente

**Problema**: Reingestas generan mÃºltiples registros para la misma combinaciÃ³n (geo_id, datetime, series_type).

**SoluciÃ³n**: DeduplicaciÃ³n basada en **claves de negocio** (business keys).

```python
# DefiniciÃ³n de claves Ãºnicas por tipo de tabla
business_keys = {
    "demanda_evolucion": ["geo_id", "datetime", "series_type"],
    "balance_balance_electrico": ["geo_id", "datetime", "series_type", "metric_type"]
}

# DeduplicaciÃ³n preservando registro mÃ¡s reciente
window = Window.partitionBy(business_keys).orderBy(desc("ingestion_timestamp"))
df_deduped = df.withColumn("_row_num", row_number().over(window)) \
               .filter(col("_row_num") == 1) \
               .drop("_row_num")
```

**Resultado**: EliminaciÃ³n de 720 registros duplicados (3.0% del total).

#### âœ… NormalizaciÃ³n Temporal

**Problema**: Fechas con offset UTC (`+02:00`, `+01:00`) se parseaban incorrectamente, desplazando datos hasta 2 horas.

**Impacto**: Un dato de "01-01-2024 00:00" se convertÃ­a en "31-12-2023 22:00", causando descuadres en agregaciones mensuales.

**SoluciÃ³n**: FunciÃ³n personalizada que preserva la hora local:

```python
def parse_datetime_local(datetime_str):
    """
    Entrada:  "2024-10-01T00:00:00.000+02:00"
    Salida:   "2024-10-01 00:00:00"

    Extrae solo YYYY-MM-DD HH:mm:ss, ignorando offset
    """
    return regexp_replace(
        datetime_str,
        r"^(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}).*$",
        "$1"
    )

# Uso:
to_timestamp(parse_datetime_local(col("val.datetime"))).alias("datetime")
```

**Consecuencia**: Reingesta completa del histÃ³rico para garantizar coherencia temporal en todo el dataset.

#### âœ… OptimizaciÃ³n de Esquema

**Columnas eliminadas automÃ¡ticamente**:

1. **Decorativas** (sin valor analÃ­tico):

   - `metric_icon`, `metric_color`, `series_icon`, `series_color`
   - No aportan informaciÃ³n cuantitativa ni categÃ³rica Ãºtil

2. **Alto % de nulos** (>95%):

   - Columnas con casi todos los valores NULL
   - ExcepciÃ³n: `ccaa_name` se conserva aunque tenga nulos (valor semÃ¡ntico)

3. **Redundantes** (detectadas por anÃ¡lisis de cardinalidad):
   - Si `series_type` â†” `series_title` tienen relaciÃ³n 1:1, eliminar `series_title`
   - Conservar siempre el campo `_type` (mÃ¡s conciso), eliminar `_title` y `_id`

**Ejemplo**:

```python
# AnÃ¡lisis de cardinalidad
sample = df.select("series_type", "series_title", "series_id").limit(100)

distinct_type_title = sample.select("series_type", "series_title").distinct().count()
distinct_type_only = sample.select("series_type").distinct().count()

if distinct_type_title <= distinct_type_only * 1.1:  # Tolerancia del 10%
    # series_title es redundante, eliminar
    to_drop.append("series_title")
```

#### âœ… NormalizaciÃ³n de Identificadores

Algunos IDs actÃºan como placeholders sin valor informativo:

```python
# Ejemplo: metric_id == metric_group_id â†’ el ID no aporta distinciÃ³n
df = df.withColumn("metric_id",
    when(col("metric_id") == col("metric_group_id"), None)
    .otherwise(col("metric_id")))
```

#### âœ… Particionamiento EstratÃ©gico

```python
# AÃ±adir columnas de particiÃ³n
df = df.withColumn("year", year(col("datetime"))) \
       .withColumn("month", month(col("datetime")))

# Escribir con particionamiento si el volumen lo justifica
if record_count > 500:
    df.write.partitionBy("year").saveAsTable(table_name)
```

**Beneficios**:

- Queries filtradas por aÃ±o se ejecutan mÃ¡s rÃ¡pido
- ParticiÃ³n fÃ­sica en storage para optimizaciÃ³n I/O
- Facilita eliminaciÃ³n selectiva de datos antiguos

### AnÃ¡lisis Exploratorio de Datos (EDA)

Tras cada transformaciÃ³n Silver, se ejecutan automÃ¡ticamente:

```python
# 1. Distribuciones estadÃ­sticas
df.select("value", "percentage").describe().show()

# 2. DetecciÃ³n de outliers
mean_val = df.select(mean("value")).first()[0]
std_val = df.select(stddev("value")).first()[0]
outliers = df.filter((col("value") > mean_val + 3*std_val) |
                     (col("value") < mean_val - 3*std_val))

# 3. AnÃ¡lisis de series temporales
df.groupBy(year("datetime"), month("datetime")) \
  .agg(count("*"), sum("value")) \
  .orderBy("year", "month") \
  .show()

# 4. VerificaciÃ³n de claves de negocio
duplicates = df.groupBy(business_keys).agg(count("*").alias("dup_count")) \
               .filter(col("dup_count") > 1)
```

### Tablas Silver Generadas

| Tabla                                                                   | Registros | Columnas | ReducciÃ³n |
| ----------------------------------------------------------------------- | --------- | -------- | --------- |
| `slv_redata_balance_balance_electrico_month`                            | 8,202     | 17       | -258      |
| `slv_redata_demanda_evolucion_month`                                    | 620       | 11       | -20       |
| `slv_redata_generacion_estructura_generacion_month`                     | 5,152     | 13       | -160      |
| `slv_redata_generacion_estructura_generacion_emisiones_asociadas_month` | 4,532     | 13       | -140      |
| `slv_redata_generacion_estructura_renovables_month`                     | 3,262     | 14       | -103      |
| `slv_redata_generacion_evolucion_renovable_no_renovable_month`          | 1,224     | 13       | -39       |

**Mejoras cualitativas**:

- âœ… Sin duplicados segÃºn claves de negocio
- âœ… Fechas corregidas y consistentes
- âœ… Esquema optimizado (eliminadas 5-8 columnas por tabla)
- âœ… Valores nulos validados
- âœ… Particionamiento por aÃ±o implementado

### ValidaciÃ³n de Calidad

```python
def validate_business_keys(df, business_keys, table_name):
    """Valida ausencia de duplicados y calcula mÃ©tricas"""
    duplicates = df.groupBy(business_keys).agg(count("*").alias("dup_count"))
    dup_groups = duplicates.filter(col("dup_count") > 1).count()

    return {
        "valid": dup_groups == 0,
        "duplicate_groups": dup_groups,
        "total_records": df.count()
    }

# Ejecutar tras escritura
df_verify = spark.table(f"lh_silver.{silver_table}")
validation = validate_business_keys(df_verify, business_keys, silver_table)

if not validation["valid"]:
    raise Exception(f"âŒ ValidaciÃ³n fallida: {validation['duplicate_groups']} duplicados")
```

---

## ğŸ¥‡ Capa Gold - Business-Ready Analytics

### PropÃ³sito

Crear un **modelo dimensional optimizado** (star schema) que responde directamente a preguntas de negocio, con performance optimizado para herramientas de BI.

### CaracterÃ­sticas

#### âœ… Modelo Estrella (Star Schema)

```
       dim_date
          â”‚
          â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”
   â”‚      â”‚      â”‚
dim_geo   â”‚   dim_tech
   â”‚      â”‚      â”‚
   â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜
          â”‚
    fact_generation
```

**Ventajas del Star Schema**:

- Queries simples y eficientes (pocos JOINs)
- FÃ¡cil de entender para usuarios de negocio
- Optimizado para agregaciones
- Compatible con herramientas OLAP (Power BI, Tableau)

#### âœ… Dimensiones Desnormalizadas

##### dim_date (DimensiÃ³n Temporal)

```sql
SELECT * FROM lh_golden.dim_date LIMIT 3;

+----------+------------+------+-------+-----+---------+----------+
| date_key | date       | year | month | day | quarter | semester |
+----------+------------+------+-------+-----+---------+----------+
| 1        | 2023-01-01 | 2023 | 1     | 1   | 1       | 1        |
| 2        | 2023-02-01 | 2023 | 2     | 1   | 1       | 1        |
| 3        | 2023-03-01 | 2023 | 3     | 1   | 1       | 1        |
+----------+------------+------+-------+-----+---------+----------+
```

**Atributos adicionales**:

- `day_of_week_num`, `day_of_week_name`
- `month_name`
- `is_weekend` (booleano)

**Uso en queries**:

```sql
-- GeneraciÃ³n total en fines de semana de 2024
SELECT SUM(generation_mwh)
FROM fact_generation_month f
JOIN dim_date d ON f.date_key = d.date_key
WHERE d.year = 2024 AND d.is_weekend = TRUE;
```

##### dim_geography (DimensiÃ³n GeogrÃ¡fica)

```sql
SELECT * FROM lh_golden.dim_geography WHERE geography_key <= 5;

+---------------+--------+--------------------+
| geography_key | geo_id | geo_name           |
+---------------+--------+--------------------+
| 1             | 4      | AndalucÃ­a          |
| 2             | 5      | AragÃ³n             |
| 3             | 6      | Cantabria          |
| 4             | 7      | Castilla-La Mancha |
| 5             | 8      | Castilla y LeÃ³n    |
+---------------+--------+--------------------+
```

**Uso en queries**:

```sql
-- Top 5 comunidades por generaciÃ³n renovable
SELECT g.geo_name, SUM(f.generation_mwh) as total_mwh
FROM fact_generation_month f
JOIN dim_geography g ON f.geography_key = g.geography_key
JOIN dim_technology t ON f.technology_key = t.technology_key
WHERE t.category = 'Renovable'
GROUP BY g.geo_name
ORDER BY total_mwh DESC
LIMIT 5;
```

##### dim_technology (DimensiÃ³n TecnolÃ³gica)

```sql
SELECT * FROM lh_golden.dim_technology WHERE has_co2_emissions = TRUE LIMIT 5;

+----------------+------------------+-------------+-------------------+
| technology_key | series_type      | category    | has_co2_emissions |
+----------------+------------------+-------------+-------------------+
| 1              | CarbÃ³n           | No-Renovable| true              |
| 2              | Ciclo combinado  | No-Renovable| true              |
| 3              | CogeneraciÃ³n     | No-Renovable| true              |
| 4              | Fuel + Gas       | No-Renovable| true              |
| 5              | Motores diÃ©sel   | No-Renovable| true              |
+----------------+------------------+-------------+-------------------+
```

**Enriquecimiento**:

- Combina datos de dos tablas Silver:
  1. `estructura-generacion` â†’ categorÃ­as (Renovable/No-Renovable)
  2. `estructura-generacion-emisiones-asociadas` â†’ flag de emisiones COâ‚‚

**Uso en queries**:

```sql
-- Porcentaje de generaciÃ³n con emisiones COâ‚‚
SELECT
    CASE WHEN has_co2_emissions THEN 'Con emisiones' ELSE 'Sin emisiones' END as tipo,
    SUM(generation_mwh) as total_mwh,
    ROUND(100.0 * SUM(generation_mwh) / SUM(SUM(generation_mwh)) OVER (), 2) as percentage
FROM fact_generation_month f
JOIN dim_technology t ON f.technology_key = t.technology_key
GROUP BY has_co2_emissions;
```

#### âœ… Tabla de Hechos

##### fact_generation_month

```sql
SELECT * FROM lh_golden.fact_generation_month LIMIT 5;

+----------+---------------+----------------+----------------+----------------------+------+
| date_key | geography_key | technology_key | generation_mwh | generation_percentage| year |
+----------+---------------+----------------+----------------+----------------------+------+
| 1        | 1             | 5              | 12345.67       | 8.5                  | 2023 |
| 1        | 1             | 7              | 23456.78       | 16.2                 | 2023 |
| 1        | 2             | 5              | 11223.45       | 9.1                  | 2023 |
| 2        | 1             | 5              | 12567.89       | 8.7                  | 2023 |
| 2        | 1             | 7              | 22890.12       | 15.9                 | 2023 |
+----------+---------------+----------------+----------------+----------------------+------+
```

**Granularidad**: Mensual por comunidad autÃ³noma y tecnologÃ­a

**MÃ©tricas**:

- `generation_mwh`: GeneraciÃ³n absoluta en megavatios-hora
- `generation_percentage`: Porcentaje sobre el total de esa comunidad en ese mes

**Particionamiento**: Por `year` para optimizaciÃ³n de queries temporales

#### âœ… Integridad Referencial

```python
# ValidaciÃ³n automÃ¡tica tras creaciÃ³n
nulls_date = fact_gen.filter(col("date_key").isNull()).count()
nulls_geo = fact_gen.filter(col("geography_key").isNull()).count()
nulls_tech = fact_gen.filter(col("technology_key").isNull()).count()

assert nulls_date == 0, "âŒ Hay nulls en date_key"
assert nulls_geo == 0, "âŒ Hay nulls en geography_key"
assert nulls_tech == 0, "âŒ Hay nulls en technology_key"

print("âœ… Integridad referencial verificada: 0 nulls en FKs")
```

**Resultado en Enerlytics**: 100% de integridad (0 nulls en claves forÃ¡neas)

### Queries de Negocio TÃ­picas

#### 1. EvoluciÃ³n de Renovables vs. No Renovables

```sql
SELECT
    d.year,
    d.month,
    t.category,
    SUM(f.generation_mwh) as total_mwh
FROM fact_generation_month f
JOIN dim_date d ON f.date_key = d.date_key
JOIN dim_technology t ON f.technology_key = t.technology_key
WHERE t.category IN ('Renovable', 'No-Renovable')
GROUP BY d.year, d.month, t.category
ORDER BY d.year, d.month, t.category;
```

#### 2. Mix EnergÃ©tico por Comunidad AutÃ³noma

```sql
SELECT
    g.geo_name,
    t.series_type,
    SUM(f.generation_mwh) as total_mwh,
    ROUND(100.0 * SUM(f.generation_mwh) / SUM(SUM(f.generation_mwh)) OVER (PARTITION BY g.geo_name), 2) as pct
FROM fact_generation_month f
JOIN dim_geography g ON f.geography_key = g.geography_key
JOIN dim_technology t ON f.technology_key = t.technology_key
JOIN dim_date d ON f.date_key = d.date_key
WHERE d.year = 2024
GROUP BY g.geo_name, t.series_type
ORDER BY g.geo_name, total_mwh DESC;
```

#### 3. GeneraciÃ³n con Emisiones COâ‚‚ por Trimestre

```sql
SELECT
    d.year,
    d.quarter,
    SUM(CASE WHEN t.has_co2_emissions THEN f.generation_mwh ELSE 0 END) as emisiones_mwh,
    SUM(CASE WHEN NOT t.has_co2_emissions THEN f.generation_mwh ELSE 0 END) as sin_emisiones_mwh
FROM fact_generation_month f
JOIN dim_date d ON f.date_key = d.date_key
JOIN dim_technology t ON f.technology_key = t.technology_key
GROUP BY d.year, d.quarter
ORDER BY d.year, d.quarter;
```

### Optimizaciones para Power BI

#### 1. Relaciones AutomÃ¡ticas

Power BI detecta automÃ¡ticamente las relaciones entre tablas basÃ¡ndose en nombres de columnas:

- `fact_generation_month[date_key]` â†’ `dim_date[date_key]`
- `fact_generation_month[geography_key]` â†’ `dim_geography[geography_key]`
- `fact_generation_month[technology_key]` â†’ `dim_technology[technology_key]`

#### 2. JerarquÃ­as Temporales

En Power BI, crear jerarquÃ­a en `dim_date`:

```
Fecha
  â””â”€â”€ AÃ±o
      â””â”€â”€ Trimestre
          â””â”€â”€ Mes
```

#### 3. Medidas DAX Precalculadas

```dax
// GeneraciÃ³n Total
Total GeneraciÃ³n = SUM(fact_generation_month[generation_mwh])

// Porcentaje Renovable
% Renovable =
DIVIDE(
    CALCULATE(
        SUM(fact_generation_month[generation_mwh]),
        dim_technology[category] = "Renovable"
    ),
    SUM(fact_generation_month[generation_mwh]),
    0
)

// GeneraciÃ³n Mes Anterior
GeneraciÃ³n Mes Anterior =
CALCULATE(
    SUM(fact_generation_month[generation_mwh]),
    DATEADD(dim_date[date], -1, MONTH)
)

// VariaciÃ³n Intermensual
VariaciÃ³n MoM =
DIVIDE(
    [Total GeneraciÃ³n] - [GeneraciÃ³n Mes Anterior],
    [GeneraciÃ³n Mes Anterior],
    0
)
```

---

## ğŸ”„ Flujo de Datos Completo

### Ejemplo: Tracking de un Registro

Seguimos un registro de generaciÃ³n eÃ³lica en AndalucÃ­a para marzo de 2024:

#### Bronze

```json
{
  "request_metadata": {
    "geo_id": 4,
    "geo_name": "AndalucÃ­a",
    "ingestion_timestamp": "2025-10-15T12:30:45.123456Z"
  },
  "api_response": {
    "included": [
      {
        "type": "EÃ³lica",
        "attributes": {
          "values": [
            {
              "datetime": "2024-03-01T00:00:00.000+01:00",
              "value": 1234567.89,
              "percentage": 22.5
            }
          ]
        }
      }
    ]
  }
}
```

#### Silver (tras transformaciones)

```sql
SELECT * FROM lh_silver.slv_redata_generacion_estructura_generacion_month
WHERE geo_id = 4 AND datetime = '2024-03-01' AND series_type = 'EÃ³lica';

+--------+------------+--------------+----------+------------+------+-------+
| geo_id | geo_name   | series_type  | datetime | value      | year | month |
+--------+------------+--------------+----------+------------+------+-------+
| 4      | AndalucÃ­a  | EÃ³lica       | 2024-03-01| 1234567.89| 2024 | 3     |
+--------+------------+--------------+----------+------------+------+-------+
```

#### Gold (modelo dimensional)

```sql
SELECT
    d.date,
    g.geo_name,
    t.series_type,
    t.category,
    t.has_co2_emissions,
    f.generation_mwh
FROM lh_golden.fact_generation_month f
JOIN lh_golden.dim_date d ON f.date_key = d.date_key
JOIN lh_golden.dim_geography g ON f.geography_key = g.geography_key
JOIN lh_golden.dim_technology t ON f.technology_key = t.technology_key
WHERE d.date = '2024-03-01' AND g.geo_id = 4 AND t.series_type = 'EÃ³lica';

+------------+-----------+-------------+-----------+-------------------+----------------+
| date       | geo_name  | series_type | category  | has_co2_emissions | generation_mwh |
+------------+-----------+-------------+-----------+-------------------+----------------+
| 2024-03-01 | AndalucÃ­a | EÃ³lica      | Renovable | false             | 1234567.89     |
+------------+-----------+-------------+-----------+-------------------+----------------+
```

---

## ğŸ“Š Beneficios de la Arquitectura Medallion en Enerlytics

### 1. Trazabilidad Completa

Cualquier valor en Gold puede rastrearse hasta el JSON original en Bronze:

```python
# Encontrar origen de un valor especÃ­fico
bronze_file = "Files/bronze/REDATA/data/generacion/.../brz-andalucia-...-2024-03-01...json"
# Contiene el JSON completo de la respuesta de la API
```

### 2. RecuperaciÃ³n ante Errores

Si se detecta un error en Silver:

- No afecta a Bronze (inmutable)
- Se corrige la transformaciÃ³n
- Se re-ejecuta Bronze â†’ Silver sin reingestiÃ³n

Si se detecta un error en Gold:

- No afecta a Bronze ni Silver
- Se corrige el modelado dimensional
- Se re-ejecuta Silver â†’ Gold rÃ¡pidamente

### 3. Flexibilidad en Modelado

El mismo Silver puede alimentar mÃºltiples modelos Gold:

- Gold para anÃ¡lisis mensual (actual)
- Gold para anÃ¡lisis diario (futuro)
- Gold para anÃ¡lisis horario (futuro con ESIOS)

### 4. OptimizaciÃ³n Incremental

Cada capa puede optimizarse independientemente:

- Bronze: OptimizaciÃ³n de ingesta (paralelizaciÃ³n)
- Silver: OptimizaciÃ³n de transformaciones (caching, broadcast joins)
- Gold: OptimizaciÃ³n de queries (Ã­ndices, particionamiento avanzado)

### 5. SeparaciÃ³n de Responsabilidades

- **Data Engineers**: Gestionan Bronze y Silver
- **Data Analysts**: Gestionan Gold y dashboards
- **Data Scientists**: Pueden trabajar en cualquier capa segÃºn necesidad

---

## ğŸ› ï¸ Herramientas EspecÃ­ficas en Microsoft Fabric

### Delta Lake

Formato de almacenamiento usado en todas las capas:

- **Transacciones ACID**: Garantiza consistencia
- **Time Travel**: Acceso a versiones histÃ³ricas de tablas
- **Merge/Update/Delete**: Operaciones avanzadas sobre datos

```python
# Ejemplo de time travel
df_v1 = spark.read.format("delta").option("versionAsOf", 1).table("slv_redata_generacion_...")
df_v2 = spark.read.format("delta").option("versionAsOf", 2).table("slv_redata_generacion_...")
```

### Lakehouse (OneLake)

- Almacenamiento unificado para todas las capas
- Acceso vÃ­a Spark, SQL, Power BI
- IntegraciÃ³n nativa con Azure Data Lake Storage Gen2

### PySpark en Notebooks

- Transformaciones distribuidas para grandes volÃºmenes
- IntegraciÃ³n con Pandas para operaciones locales
- EjecuciÃ³n interactiva para prototipado rÃ¡pido

---

## ğŸ“š Referencias

- [Delta Lake Documentation](https://docs.delta.io/)
- [Databricks Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)
- [Microsoft Fabric Lakehouse](https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-overview)

---

**Ãšltima actualizaciÃ³n**: Octubre 2025  
**Mantenido por**: Equipo Enerlytics - FactorÃ­a F5
