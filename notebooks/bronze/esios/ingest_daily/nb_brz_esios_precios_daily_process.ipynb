{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily_processing_init",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import os, re, unicodedata\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from notebookutils import mssparkutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark_session_init",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ---------- Spark ----------\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lakehouse_selection",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Seleccionar el Lakehouse destino (bronze)\n",
    "spark.sql(\"USE lh_bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "base_directories",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# üìÇ Carpeta base donde se guardan los JSON diarios del proceso de ingesti√≥n\n",
    "daily_json_base = \"Files/bronze/ESIOS/data/mercados/componentes-precio/day/\"+str(datetime.now().year)+\"/daily\"\n",
    "daily_json_base_physical = \"/lakehouse/default/Files/bronze/ESIOS/data/mercados/componentes-precio/day/\"+str(datetime.now().year)+\"/daily\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "slugify_function",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Funci√≥n utilitaria mejorada\n",
    "# --------------------------\n",
    "def slugify(name: str) -> str:\n",
    "    # Normalizar y quitar tildes\n",
    "    name = unicodedata.normalize(\"NFKD\", name)\n",
    "    name = name.encode(\"ascii\", \"ignore\").decode(\"utf-8\")  # quita acentos\n",
    "    # Sustituir caracteres no v√°lidos por \"_\"\n",
    "    return re.sub(r'[^a-z0-9_]', '_', name.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processing_logic",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Procesamiento de JSON diarios\n",
    "# --------------------------\n",
    "total_rows = 0\n",
    "processed_files = 0\n",
    "\n",
    "# Obtener el a√±o actual para procesar solo los datos del a√±o en curso\n",
    "current_year = datetime.now().year\n",
    "print(f\"üìÖ A√±o actual: {current_year}\")\n",
    "print(f\"üìÇ Directorio de procesamiento: {daily_json_base}\")\n",
    "\n",
    "# Listar archivos JSON directamente en el directorio daily del a√±o actual\n",
    "try:\n",
    "    json_files = mssparkutils.fs.ls(daily_json_base)\n",
    "    print(f\"üìÑ Archivos JSON encontrados: {len(json_files)}\")\n",
    "    for f in json_files:\n",
    "        print(f\"   - {f.path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al listar archivos en {daily_json_base}: {e}\")\n",
    "    json_files = []\n",
    "\n",
    "for f in json_files:\n",
    "    if not f.path.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    print(f\"üîé Procesando {f.path}\")\n",
    "\n",
    "    try:\n",
    "        # Leer JSON\n",
    "        df_raw = spark.read.option(\"multiline\", True).json(f.path)\n",
    "        \n",
    "        # Verificar si el JSON tiene la estructura esperada\n",
    "        if \"indicator\" not in df_raw.columns:\n",
    "            print(f\"‚ö†Ô∏è Estructura JSON inesperada en {f.path}, saltando\")\n",
    "            continue\n",
    "\n",
    "        # Extraer indicador, values\n",
    "        indicator_id = df_raw.select(\"indicator.id\").first()[0]\n",
    "        indicator_type = df_raw.select(\"indicator.short_name\").first()[0] \\\n",
    "                      if \"short_name\" in df_raw.select(\"indicator.*\").columns else indicator_id\n",
    "\n",
    "        values = df_raw.selectExpr(\"explode(indicator.values) as val\") \\\n",
    "                   .select(\"val.*\")\n",
    "\n",
    "        if values.rdd.isEmpty():\n",
    "            print(f\"‚ö†Ô∏è JSON vac√≠o (sin values) en {f.path}, saltando\")\n",
    "            continue\n",
    "\n",
    "        # Enriquecer con metadatos\n",
    "        df = (values\n",
    "              .withColumn(\"indicator_id\", F.lit(indicator_id))\n",
    "              .withColumn(\"indicator_type\", F.lit(indicator_type))\n",
    "              .withColumn(\"year\", F.lit(current_year))\n",
    "              .withColumn(\"time_trunc\", F.lit(\"day\"))  # Siempre es daily\n",
    "             )\n",
    "\n",
    "        # Nombre tabla ‚Üí seg√∫n tipo de indicador\n",
    "        indicator_slug = slugify(indicator_type)\n",
    "        table_name = f\"brz_esios_mercados_{indicator_slug}_day\"\n",
    "\n",
    "        # Verificar si la tabla existe antes de intentar append\n",
    "        table_exists = spark._jsparkSession.catalog().tableExists(\"lh_bronze\", table_name)\n",
    "        \n",
    "        if not table_exists:\n",
    "            print(f\"‚ö†Ô∏è La tabla {table_name} no existe. Cre√°ndola con overwrite.\")\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "        else:\n",
    "            # Guardar en Delta gestionado (append a tabla existente)\n",
    "            df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "        count = df.count()\n",
    "        total_rows += count\n",
    "        processed_files += 1\n",
    "        print(f\"‚úÖ Guardado en tabla {table_name} ({count} registros)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error procesando archivo {f.path}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"üìä Proceso terminado - {datetime.utcnow().isoformat()}Z\")\n",
    "print(f\"Total archivos procesados: {processed_files}\")\n",
    "print(f\"Total registros procesados: {total_rows}\")\n",
    "\n",
    "if processed_files == 0:\n",
    "    print(\"‚ÑπÔ∏è  No se encontraron archivos JSON para procesar.\")\n",
    "    print(\"‚ÑπÔ∏è  Verifica que:\")\n",
    "    print(\"   1. El notebook de ingesti√≥n diaria se ejecut√≥ correctamente\")\n",
    "    print(\"   2. Los archivos JSON est√°n en la ruta: {daily_json_base}\")\n",
    "    print(\"   3. Los archivos tienen nombres como: brz-mercados-pvpc_t_2_0td-daily.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "es"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}