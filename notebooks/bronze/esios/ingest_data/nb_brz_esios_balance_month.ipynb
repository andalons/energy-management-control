{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, re, unicodedata\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from notebookutils import mssparkutils"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "c039b8af-c9ab-418f-aeb1-7f31132d26c1"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Spark ----------\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "9b591fc5-8d7b-4b58-8dcd-f149839ab856"
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar el Lakehouse destino (bronze)\n",
        "spark.sql(\"USE lh_bronze\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "7858778f-00be-4c5b-8595-cb47fee89a9b"
    },
    {
      "cell_type": "code",
      "source": [
        "# üìÇ Carpetas bronze (donde ya est√°n los JSON)\n",
        "bronze_base_logical = \"Files/bronze/ESIOS/data/balance/balance-electrico\"\n",
        "bronze_base_physical = \"/lakehouse/default/Files/bronze/ESIOS/data/balance/balance-electrico\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "21078b55-0df2-442a-bbc2-b13c30f297cd"
    },
    {
      "cell_type": "code",
      "source": [
        "# üìÇ Salida (Tables/)\n",
        "tables_prefix = \"Tables\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "7bea3c92-3b62-485f-8bac-6d5b90aa9263"
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# Funci√≥n utilitaria mejorada\n",
        "# --------------------------\n",
        "def slugify(name: str) -> str:\n",
        "    # Normalizar y quitar tildes\n",
        "    name = unicodedata.normalize(\"NFKD\", name)\n",
        "    name = name.encode(\"ascii\", \"ignore\").decode(\"utf-8\")  # quita acentos\n",
        "    # Sustituir caracteres no v√°lidos por \"_\"\n",
        "    return re.sub(r'[^a-z0-9_]', '_', name.lower())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "921cccf5-f72c-4fac-98ff-f97703f7592a"
    },
    {
      "cell_type": "code",
      "source": [
        "## --------------------------\n",
        "# Procesamiento\n",
        "# --------------------------\n",
        "total_rows = 0\n",
        "\n",
        "# Listar granularidades (day, month)\n",
        "granularities = [g.name for g in mssparkutils.fs.ls(bronze_base_logical) if g.isDir]\n",
        "\n",
        "for trunc in granularities:\n",
        "    years_dirs = mssparkutils.fs.ls(f\"{bronze_base_logical}/{trunc}\")\n",
        "    for year_dir in years_dirs:\n",
        "        year = year_dir.name\n",
        "        json_files = mssparkutils.fs.ls(year_dir.path)\n",
        "        for f in json_files:\n",
        "            if not f.path.endswith(\".json\"):\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nüîé Procesando {f.path}\")\n",
        "\n",
        "            # Leer JSON\n",
        "            df_raw = spark.read.option(\"multiline\", True).json(f.path)\n",
        "\n",
        "            # Extraer indicador, values\n",
        "            indicator_id = df_raw.select(\"indicator.id\").first()[0]\n",
        "            indicator_type = df_raw.select(\"indicator.short_name\").first()[0] \\\n",
        "                              if \"short_name\" in df_raw.select(\"indicator.*\").columns else indicator_id\n",
        "\n",
        "            values = df_raw.selectExpr(\"explode(indicator.values) as val\") \\\n",
        "                           .select(\"val.*\")\n",
        "\n",
        "            if values.rdd.isEmpty():\n",
        "                print(\"‚ö†Ô∏è JSON vac√≠o (sin values), skipping\")\n",
        "                continue\n",
        "\n",
        "            # Enriquecer con metadatos y transformar datetime\n",
        "            df = (values\n",
        "                  .withColumn(\"indicator_id\", F.lit(indicator_id))\n",
        "                  .withColumn(\"indicator_type\", F.lit(indicator_type))\n",
        "                  .withColumn(\"year\", F.lit(int(year)))\n",
        "                  .withColumn(\"time_trunc\", F.lit(trunc))\n",
        "                  # Renombrar datetime a fecha y convertir a timestamp\n",
        "                  .withColumn(\"fecha\", F.to_timestamp(F.col(\"datetime\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\"))\n",
        "                  .drop(\"datetime\")\n",
        "                 )\n",
        "\n",
        "            # Nombre tabla ‚Üí seg√∫n tipo de indicador y trunc\n",
        "            indicator_slug = slugify(indicator_type)\n",
        "            table_name = f\"brz_esios_balance_{indicator_slug}_{trunc}\"\n",
        "\n",
        "            # Guardar en Delta gestionado (en Lakehouse slv_lkh_esios)\n",
        "            df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
        "\n",
        "            count = df.count()\n",
        "            total_rows += count\n",
        "            print(f\"‚úÖ Guardado en tabla {table_name} ({count} registros)\")\n",
        "\n",
        "print(f\"\\nProceso terminado - {datetime.utcnow().isoformat()}Z\")\n",
        "print(f\"Total registros procesados: {total_rows}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "ee826979-cdb0-4f82-b38a-70dd5a771948"
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "es"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    },
    "dependencies": {
      "lakehouse": {
        "known_lakehouses": [
          {
            "id": "0fd09a67-0164-4fb6-838e-02a27c823afc"
          }
        ],
        "default_lakehouse": "0fd09a67-0164-4fb6-838e-02a27c823afc",
        "default_lakehouse_name": "lh_bronze",
        "default_lakehouse_workspace_id": "ecf938c4-c449-48de-a07c-1d968a72b3d1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}