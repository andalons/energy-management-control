{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from notebookutils import mssparkutils\n","import sys\n","from datetime import datetime as dt "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"6669dea6-fad1-497f-a439-dbf04a00876f","normalized_state":"finished","queued_time":"2025-10-07T12:59:15.9798929Z","session_start_time":"2025-10-07T12:59:15.9809441Z","execution_start_time":"2025-10-07T12:59:29.9970694Z","execution_finish_time":"2025-10-07T12:59:30.4888673Z","parent_msg_id":"b2dd004c-151e-4580-bc02-836f32e4a765"},"text/plain":"StatementMeta(, 6669dea6-fad1-497f-a439-dbf04a00876f, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c0e1369e-83a8-42ae-bd3d-8fb39144bfc1"},{"cell_type":"code","source":["# ============================================\n","# Configuraci√≥n\n","# ============================================\n","bronze_path_param = \"Files/bronze/REDATA/data\"\n","category_param = \"\"\n","widget_param = \"\"\n","lakehouse_name = \"lh_bronze\"\n","\n","# Obtener par√°metros del pipeline\n","if 'bronze_path' in locals() or 'bronze_path' in globals():\n","    bronze_path_param = bronze_path if 'bronze_path' in locals() else globals().get('bronze_path', bronze_path_param)\n","if 'category' in locals() or 'category' in globals():\n","    category_param = category if 'category' in locals() else globals().get('category', \"\")\n","if 'widget' in locals() or 'widget' in globals():\n","    widget_param = widget if 'widget' in locals() else globals().get('widget', \"\")\n","\n","print(f\"üìÇ Bronze path: {bronze_path_param}\")\n","print(f\"üì¶ Category: {category_param or 'Todas'}\")\n","print(f\"üîß Widget: {widget_param or 'Todos'}\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"899a99ec-abd1-477f-97ce-41d9cbbdb1ed"},{"cell_type":"code","source":["# ============================================\n","# Carga json_log.py\n","# ============================================\n","json_log_path = \"Files/code/json_log.py\"\n","try:\n","    json_log_code = mssparkutils.fs.head(json_log_path, 100000)\n","    exec(json_log_code)\n","    print(\"‚úÖ json_log.py cargado correctamente\")\n","except Exception as e:\n","    print(f\"‚ö†Ô∏è Error cargando json_log.py: {e}\")\n","    print(\"Se usar√° l√≥gica de log inline como fallback\")\n","    \n","    def check_json_log_exists(lakehouse_name, medallion_short, source_file, api_name=None):\n","        log_table = f\"{medallion_short}_json_log\"\n","        if not spark._jsparkSession.catalog().tableExists(log_table):\n","            return False\n","        df = spark.table(log_table).filter(col(\"source_file\") == source_file)\n","        if api_name:\n","            df = df.filter(col(\"api_name\") == api_name)\n","        return df.limit(1).count() > 0\n","    \n","    def save_json_log(api_name, source_file, target_table, ingestion_date, \n","                      write_mode=\"append\", lakehouse_name=\"lh_bronze\", medallion_short=\"brz\"):\n","        from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n","        \n","        schema = StructType([\n","            StructField(\"api_name\", StringType(), True),\n","            StructField(\"source_file\", StringType(), True),\n","            StructField(\"target_table\", StringType(), True),\n","            StructField(\"ingestion_date\", TimestampType(), True),\n","        ])\n","        \n","        new_row = [(str(api_name), str(source_file), str(target_table), ingestion_date)]\n","        df_new = spark.createDataFrame(new_row, schema=schema)\n","        log_table = f\"{medallion_short}_json_log\"\n","        \n","        try:\n","            if spark._jsparkSession.catalog().tableExists(log_table):\n","                df_existing = spark.table(log_table)\n","                duplicate = (\n","                    df_existing\n","                    .filter((col(\"api_name\") == api_name) & (col(\"source_file\") == source_file))\n","                    .limit(1).count()\n","                )\n","                if duplicate > 0:\n","                    print(f\"\\t‚ö†Ô∏è Ya procesado: {source_file}\")\n","                    return None\n","            \n","            df_new.write.format(\"delta\").mode(write_mode).saveAsTable(log_table)\n","            print(f\"\\t‚úÖ Log guardado: {source_file}\")\n","        except Exception as e:\n","            print(f\"\\t‚õî Error guardando log: {str(e)}\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"919d0ebe-5744-4681-8438-c707cd4ada9a"},{"cell_type":"code","source":["# ============================================\n","# Inicializar Spark\n","# ============================================\n","\n","spark = SparkSession.builder.appName(\"REData_JSON_to_Delta\").getOrCreate()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"6669dea6-fad1-497f-a439-dbf04a00876f","normalized_state":"finished","queued_time":"2025-10-07T12:59:15.9914346Z","session_start_time":null,"execution_start_time":"2025-10-07T12:59:37.1522434Z","execution_finish_time":"2025-10-07T12:59:37.6373777Z","parent_msg_id":"f2d61f96-34f0-4fd1-b08d-cb9b6d936679"},"text/plain":"StatementMeta(, 6669dea6-fad1-497f-a439-dbf04a00876f, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ea0ea766-4624-48d0-a509-4fdd58c810f9"},{"cell_type":"code","source":["# ============================================\n","# Funciones auxiliares\n","# ============================================\n","\n","def list_categories():\n","    \"\"\"Lista categor√≠as disponibles en bronze\"\"\"\n","    try:\n","        files = mssparkutils.fs.ls(bronze_path_param)\n","        return [f.name for f in files if f.isDir]\n","    except:\n","        return []\n","\n","\n","def list_widgets(category):\n","    \"\"\"Lista widgets de una categor√≠a\"\"\"\n","    try:\n","        path = f\"{bronze_path_param}/{category}\"\n","        files = mssparkutils.fs.ls(path)\n","        return [f.name for f in files if f.isDir]\n","    except:\n","        return []\n","\n","\n","def get_new_json_files(category, widget, time_trunc):\n","    \"\"\"\n","    Obtiene lista de archivos JSON a√∫n no procesados para un time_trunc espec√≠fico.\n","    ‚úÖ SOLO PROCESA 'month' - ignora 'day'\n","    \"\"\"\n","    if time_trunc != \"month\":\n","        return []  # ‚úÖ SOLO MENSUALES\n","    \n","    json_paths = []\n","    api_name = f\"{category}/{widget}/{time_trunc}\"\n","    \n","    path = f\"{bronze_path_param}/{category}/{widget}/{time_trunc}\"\n","    try:\n","        files = mssparkutils.fs.ls(path)\n","        for f in files:\n","            if f.name.endswith('.json'):\n","                full_path = f\"{path}/{f.name}\"\n","                \n","                # Verificar si ya fue procesado\n","                if not check_json_log_exists(\n","                    lakehouse_name=lakehouse_name,\n","                    medallion_short=\"brz\",\n","                    source_file=full_path,\n","                    api_name=api_name\n","                ):\n","                    json_paths.append(full_path)\n","    except:\n","        pass\n","    \n","    return json_paths\n","\n","\n","def extract_metadata(df_raw, category, widget, time_trunc):\n","    \"\"\"Extrae metadatos del nodo api_response.data\"\"\"\n","    try:\n","        sample = df_raw.select(\n","            col(\"api_response.data.type\").alias(\"data_type\"),\n","            col(\"api_response.data.id\").alias(\"data_id\"),\n","            col(\"api_response.data.attributes.title\").alias(\"data_title\"),\n","            col(\"api_response.data.attributes.last-update\").alias(\"data_last_update\"),\n","            col(\"api_response.data.attributes.description\").alias(\"data_description\")\n","        ).first()\n","        \n","        if sample:\n","            return {\n","                \"category\": category,\n","                \"widget\": widget,\n","                \"time_trunc\": time_trunc,\n","                \"data_type\": sample[\"data_type\"],\n","                \"data_id\": sample[\"data_id\"],\n","                \"data_title\": sample[\"data_title\"],\n","                \"data_last_update\": sample[\"data_last_update\"],\n","                \"data_description\": sample[\"data_description\"],\n","                \"metadata_extraction_timestamp\": dt.now()  \n","            }\n","    except Exception as e:\n","        print(f\"\\t‚ö†Ô∏è No se pudo extraer metadata: {str(e)}\")\n","    \n","    return None\n","\n","\n","def save_metadata(metadata_dict):\n","    \"\"\"Guarda metadatos en la tabla brz_redata_metadata\"\"\"\n","    if not metadata_dict:\n","        return\n","    \n","    schema = StructType([\n","        StructField(\"category\", StringType(), True),\n","        StructField(\"widget\", StringType(), True),\n","        StructField(\"time_trunc\", StringType(), True),\n","        StructField(\"data_type\", StringType(), True),\n","        StructField(\"data_id\", StringType(), True),\n","        StructField(\"data_title\", StringType(), True),\n","        StructField(\"data_last_update\", StringType(), True),\n","        StructField(\"data_description\", StringType(), True),\n","        StructField(\"metadata_extraction_timestamp\", TimestampType(), True)\n","    ])\n","    \n","    df_meta = spark.createDataFrame([tuple(metadata_dict.values())], schema=schema)\n","    \n","    try:\n","        # Verificar si ya existe este registro\n","        if spark._jsparkSession.catalog().tableExists(\"brz_redata_metadata\"):\n","            existing = spark.table(\"brz_redata_metadata\").filter(\n","                (col(\"category\") == metadata_dict[\"category\"]) &\n","                (col(\"widget\") == metadata_dict[\"widget\"]) &\n","                (col(\"time_trunc\") == metadata_dict[\"time_trunc\"])\n","            )\n","            if existing.count() > 0:\n","                print(f\"\\tüìã Metadata ya existe para {metadata_dict['category']}/{metadata_dict['widget']}/{metadata_dict['time_trunc']}\")\n","                return\n","        \n","        df_meta.write.format(\"delta\").mode(\"append\").saveAsTable(\"brz_redata_metadata\")\n","        print(f\"\\t‚úÖ Metadata guardada: {metadata_dict['category']}/{metadata_dict['widget']}/{metadata_dict['time_trunc']}\")\n","    except Exception as e:\n","        print(f\"\\t‚õî Error guardando metadata: {str(e)}\")\n","\n","\n","def parse_datetime_local(datetime_str):\n","    \"\"\"\n","    ‚úÖ SOLUCI√ìN AL PROBLEMA DE FECHAS\n","    Convierte datetime preservando la fecha local sin convertir a UTC\n","    \n","    Entrada:  \"2024-10-01T00:00:00.000+02:00\"\n","    Salida:   \"2024-10-01 00:00:00\" (mantiene fecha/hora local)\n","    \"\"\"\n","    # Extraer solo la parte de fecha y hora, ignorar offset\n","    return regexp_replace(datetime_str, r\"^(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}).*$\", \"$1\")\n","\n","\n","def process_balance_electrico(df_raw, category, widget, time_trunc):\n","    \"\"\"Procesa balance/balance-electrico (estructura con content anidado)\"\"\"\n","    df_meta = df_raw.select(\n","        col(\"request_metadata.geo_id\").alias(\"geo_id\"),\n","        col(\"request_metadata.geo_name\").alias(\"geo_name\"),\n","        col(\"request_metadata.geo_limit\").alias(\"geo_limit\"),\n","        col(\"request_metadata.time_trunc\").alias(\"time_trunc\"),\n","        col(\"request_metadata.ingestion_timestamp\").alias(\"ingestion_timestamp\"),\n","        col(\"api_response.included\").alias(\"included\")\n","    )\n","    \n","    df_included = df_meta.select(\"*\", explode(\"included\").alias(\"series\")).drop(\"included\")\n","    \n","    df_step1 = df_included.select(\n","        \"geo_id\", \"geo_name\", \"geo_limit\", \"time_trunc\", \"ingestion_timestamp\",\n","        col(\"series.type\").alias(\"series_type\"),\n","        col(\"series.id\").alias(\"series_id\"),\n","        col(\"series.attributes.title\").alias(\"series_title\"),\n","        col(\"series.attributes.last-update\").alias(\"series_last_update\"),\n","        col(\"series.attributes.description\").alias(\"series_description\"),\n","        col(\"series.attributes.magnitude\").alias(\"series_magnitude\"),\n","        col(\"series.attributes.content\").alias(\"content\")\n","    )\n","    \n","    df_step2 = df_step1.select(\"*\", explode(\"content\").alias(\"metric\")).drop(\"content\")\n","    \n","    df_step3 = df_step2.select(\n","        \"geo_id\", \"geo_name\", \"geo_limit\", \"time_trunc\", \"ingestion_timestamp\",\n","        \"series_type\", \"series_id\", \"series_title\", \"series_last_update\", \"series_description\", \"series_magnitude\",\n","        col(\"metric.type\").alias(\"metric_type\"),\n","        col(\"metric.id\").alias(\"metric_id\"),\n","        col(\"metric.groupId\").alias(\"metric_group_id\"),\n","        col(\"metric.attributes.title\").alias(\"metric_title\"),\n","        col(\"metric.attributes.description\").alias(\"metric_description\"),\n","        col(\"metric.attributes.color\").alias(\"metric_color\"),\n","        col(\"metric.attributes.icon\").alias(\"metric_icon\"),\n","        col(\"metric.attributes.type\").alias(\"metric_attribute_type\"),\n","        col(\"metric.attributes.magnitude\").alias(\"metric_magnitude\"),\n","        col(\"metric.attributes.composite\").alias(\"is_composite\"),\n","        col(\"metric.attributes.last-update\").alias(\"metric_last_update\"),\n","        col(\"metric.attributes.values\").alias(\"values\")\n","    )\n","    \n","    df_step4 = df_step3.select(\"*\", explode(\"values\").alias(\"val\")).drop(\"values\")\n","    \n","    # ‚úÖ FIX FECHAS: Usar parse_datetime_local\n","    df_final = df_step4.select(\n","        \"geo_id\", \"geo_name\", \"geo_limit\", \"time_trunc\",\n","        \"series_type\", \"series_id\", \"series_title\", \"series_last_update\", \"series_description\", \"series_magnitude\",\n","        \"metric_type\", \"metric_id\", \"metric_group_id\", \"metric_title\", \"metric_description\",\n","        \"metric_color\", \"metric_icon\", \"metric_attribute_type\", \"metric_magnitude\", \"is_composite\", \"metric_last_update\",\n","        to_timestamp(parse_datetime_local(col(\"val.datetime\"))).alias(\"datetime\"),  # ‚úÖ CORREGIDO\n","        col(\"val.value\").cast(\"double\").alias(\"value\"),\n","        col(\"val.percentage\").cast(\"double\").alias(\"percentage\"),\n","        col(\"ingestion_timestamp\").cast(\"timestamp\").alias(\"ingestion_timestamp\")\n","    )\n","    \n","    return df_final.filter(col(\"datetime\").isNotNull() & col(\"value\").isNotNull())\n","\n","\n","def process_standard_widget(df_raw, category, widget, time_trunc):\n","    \"\"\"Procesa widgets est√°ndar (sin content anidado): demanda, generacion, mercados\"\"\"\n","    df_meta = df_raw.select(\n","        col(\"request_metadata.geo_id\").alias(\"geo_id\"),\n","        col(\"request_metadata.geo_name\").alias(\"geo_name\"),\n","        col(\"request_metadata.geo_limit\").alias(\"geo_limit\"),\n","        col(\"request_metadata.time_trunc\").alias(\"time_trunc\"),\n","        col(\"request_metadata.ingestion_timestamp\").alias(\"ingestion_timestamp\"),\n","        col(\"api_response.included\").alias(\"included\")\n","    )\n","    \n","    df_included = df_meta.select(\"*\", explode(\"included\").alias(\"series\")).drop(\"included\")\n","    \n","    df_step1 = df_included.select(\n","        \"geo_id\", \"geo_name\", \"geo_limit\", \"time_trunc\", \"ingestion_timestamp\",\n","        col(\"series.type\").alias(\"series_type\"),\n","        col(\"series.id\").alias(\"series_id\"),\n","        col(\"series.groupId\").alias(\"series_group_id\"),\n","        col(\"series.attributes.title\").alias(\"series_title\"),\n","        col(\"series.attributes.description\").alias(\"series_description\"),\n","        col(\"series.attributes.color\").alias(\"series_color\"),\n","        col(\"series.attributes.icon\").alias(\"series_icon\"),\n","        col(\"series.attributes.type\").alias(\"series_attribute_type\"),\n","        col(\"series.attributes.magnitude\").alias(\"series_magnitude\"),\n","        col(\"series.attributes.composite\").alias(\"is_composite\"),\n","        col(\"series.attributes.last-update\").alias(\"series_last_update\"),\n","        col(\"series.attributes.values\").alias(\"values\")\n","    )\n","    \n","    df_step2 = df_step1.select(\"*\", explode(\"values\").alias(\"val\")).drop(\"values\")\n","    \n","    # ‚úÖ FIX FECHAS: Usar parse_datetime_local\n","    df_final = df_step2.select(\n","        \"geo_id\", \"geo_name\", \"geo_limit\", \"time_trunc\",\n","        \"series_type\", \"series_id\", \"series_group_id\", \"series_title\", \"series_description\",\n","        \"series_color\", \"series_icon\", \"series_attribute_type\", \"series_magnitude\",\n","        \"is_composite\", \"series_last_update\",\n","        to_timestamp(parse_datetime_local(col(\"val.datetime\"))).alias(\"datetime\"),  # ‚úÖ CORREGIDO\n","        col(\"val.value\").cast(\"double\").alias(\"value\"),\n","        col(\"val.percentage\").cast(\"double\").alias(\"percentage\"),\n","        col(\"ingestion_timestamp\").cast(\"timestamp\").alias(\"ingestion_timestamp\")\n","    )\n","    \n","    return df_final.filter(col(\"datetime\").isNotNull() & col(\"value\").isNotNull())\n","\n","\n","def process_widget_by_time_trunc(category, widget, time_trunc):\n","    \"\"\"\n","    Procesa un widget espec√≠fico para un time_trunc dado\n","    ‚úÖ SOLO PROCESA 'month' - ignora 'day'\n","    \"\"\"\n","    if time_trunc != \"month\":\n","        return None  # ‚úÖ SOLO MENSUALES\n","    \n","    print(f\"\\nüì¶ {category}/{widget}/{time_trunc}\")\n","    \n","    api_name = f\"{category}/{widget}/{time_trunc}\"\n","    json_paths = get_new_json_files(category, widget, time_trunc)\n","    \n","    if not json_paths:\n","        print(f\"  ‚úÖ Sin archivos nuevos\")\n","        return None\n","    \n","    print(f\"  üìÑ {len(json_paths)} archivos nuevos a procesar\")\n","    \n","    df_raw = spark.read.option(\"multiline\", \"true\").json(json_paths)\n","    \n","    # Extraer y guardar metadata\n","    metadata = extract_metadata(df_raw, category, widget, time_trunc)\n","    if metadata:\n","        save_metadata(metadata)\n","    \n","    # Procesar seg√∫n la estructura del widget\n","    if category == \"balance\" and widget == \"balance-electrico\":\n","        df_final = process_balance_electrico(df_raw, category, widget, time_trunc)\n","    else:\n","        df_final = process_standard_widget(df_raw, category, widget, time_trunc)\n","    \n","    # Nombre de tabla con sufijo time_trunc\n","    table_name = f\"brz_redata_{category}_{widget}_{time_trunc}\".replace(\"-\", \"_\")\n","    \n","    df_final.write.format(\"delta\") \\\n","        .mode(\"append\") \\\n","        .option(\"mergeSchema\", \"true\") \\\n","        .saveAsTable(table_name)\n","    \n","    count = df_final.count()\n","    print(f\"  ‚úÖ {count} registros a√±adidos a {table_name}\")\n","    \n","    ingestion_ts = dt.now()  \n","    for json_path in json_paths:\n","        save_json_log(\n","            api_name=api_name,\n","            source_file=json_path,\n","            target_table=table_name,\n","            ingestion_date=ingestion_ts,\n","            lakehouse_name=lakehouse_name\n","        )\n","    \n","    return table_name"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"6669dea6-fad1-497f-a439-dbf04a00876f","normalized_state":"finished","queued_time":"2025-10-07T12:59:15.9956007Z","session_start_time":null,"execution_start_time":"2025-10-07T12:59:37.6409756Z","execution_finish_time":"2025-10-07T12:59:38.0689518Z","parent_msg_id":"62f78e3e-efda-483f-9738-d403c3484c72"},"text/plain":"StatementMeta(, 6669dea6-fad1-497f-a439-dbf04a00876f, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"833388f2-8461-4f81-9d17-0f8c5d5b3f61"},{"cell_type":"code","source":["# ============================================\n","# Main\n","# ============================================\n","\n","def main():\n","    print(\"üöÄ JSON ‚Üí Delta v3 (SOLO MENSUALES + FIX FECHAS)\")\n","    print(f\"üìÇ {bronze_path_param}\\n\")\n","    \n","    if category_param and widget_param:\n","        categories_to_process = [(category_param, [widget_param])]\n","    elif category_param:\n","        widgets = list_widgets(category_param)\n","        categories_to_process = [(category_param, widgets)]\n","    else:\n","        categories = list_categories()\n","        categories_to_process = [(cat, list_widgets(cat)) for cat in categories]\n","    \n","    tables_updated = []\n","    \n","    for category, widgets in categories_to_process:\n","        for widget in widgets:\n","            # ‚úÖ SOLO MENSUALES\n","            try:\n","                table = process_widget_by_time_trunc(category, widget, \"month\")\n","                if table:\n","                    tables_updated.append(table)\n","            except Exception as e:\n","                print(f\"  ‚õî Error en {category}/{widget}/month: {str(e)}\")\n","    \n","    print(f\"\\nüìä Resumen:\")\n","    print(f\"  - Tablas actualizadas: {len(tables_updated)}\")\n","    print(f\"  - Tablas √∫nicas: {len(set(tables_updated))}\")\n","    if tables_updated:\n","        print(f\"  - Tablas: {', '.join(sorted(set(tables_updated)))}\")\n","    \n","    return {\"tables_updated\": len(tables_updated), \"tables\": list(set(tables_updated))}\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a7950bcf-644c-42af-b3d1-60b9c8f8143a"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"0fd09a67-0164-4fb6-838e-02a27c823afc"}],"default_lakehouse":"0fd09a67-0164-4fb6-838e-02a27c823afc","default_lakehouse_name":"lh_bronze","default_lakehouse_workspace_id":"ecf938c4-c449-48de-a07c-1d968a72b3d1"}}},"nbformat":4,"nbformat_minor":5}