{"cells":[{"cell_type":"code","source":["import requests, json, time, logging, unicodedata, re\n","from datetime import datetime, timedelta\n","from pyspark.sql import SparkSession\n","from typing import Dict, Optional\n","from notebookutils import mssparkutils"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0c1b63c1-9108-48ea-b871-7fa29f0aeb04"},{"cell_type":"code","source":["# Parámetros del notebook\n","start_date_param = \"\"\n","end_date_param = \"\"\n","max_combinations_param = \"\"\n","lakehouse_path_param = \"Files/bronze/REDATA\"\n","schedule_type_param = \"full\"\n","\n","import sys\n","\n","# Obtener parámetros del pipeline\n","if 'start_date' in locals() or 'start_date' in globals():\n","    start_date_param = start_date if 'start_date' in locals() else globals().get('start_date', \"\")\n","if 'end_date' in locals() or 'end_date' in globals():\n","    end_date_param = end_date if 'end_date' in locals() else globals().get('end_date', \"\")\n","if 'max_combinations' in locals() or 'max_combinations' in globals():\n","    max_combinations_param = max_combinations if 'max_combinations' in locals() else globals().get('max_combinations', \"\")\n","if 'lakehouse_path' in locals() or 'lakehouse_path' in globals():\n","    lakehouse_path_param = lakehouse_path if 'lakehouse_path' in locals() else globals().get('lakehouse_path', \"Files/bronze/REDATA\")\n","if 'schedule_type' in locals() or 'schedule_type' in globals():\n","    schedule_type_param = schedule_type if 'schedule_type' in locals() else globals().get('schedule_type', \"full\")\n","\n","# Debug\n","print(f\"Parámetros recibidos:\")\n","print(f\"  start_date: '{start_date_param}'\")\n","print(f\"  end_date: '{end_date_param}'\")\n","print(f\"  max_combinations: '{max_combinations_param}'\")\n","print(f\"  lakehouse_path: '{lakehouse_path_param}'\")\n","print(f\"  schedule_type: '{schedule_type_param}'\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2f7e4c03-4cf6-481e-89f7-6e00d5c78e78"},{"cell_type":"code","source":["# Logging simple en consola\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","# Spark session\n","spark = SparkSession.builder.appName(\"REData_API_Explorer\").getOrCreate()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"69c8235f-e690-4799-bffa-ac9cca85cb48"},{"cell_type":"code","source":["# Cargar configuración externa\n","def load_config():\n","    config_path = \"Files/config/redata_config.json\"\n","    try:\n","        content = mssparkutils.fs.head(config_path, 100000)\n","        return json.loads(content)\n","    except:\n","        logger.warning(\"No se pudo cargar config, usando valores por defecto\")\n","        return {\n","            \"ccaa_ids\": {\"Península\": 8741, \"Islas Canarias\": 8742},\n","            \"geo_map\": {\"peninsular\": \"Península\", \"canarias\": \"Islas Canarias\"},\n","            \"api_config\": {\"balance\": [\"balance-electrico\"]}\n","        }"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c43088a2-2c89-4663-a8e7-804b156b9f79"},{"cell_type":"code","source":["class REDataAPIExplorer:\n","    def __init__(self, base_lakehouse_path: str, start_date: str = None, end_date: str = None):\n","        self.base_url = \"https://apidatos.ree.es\"\n","        self.base_lakehouse_path = base_lakehouse_path\n","        self.session = requests.Session()\n","        self.headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n","        \n","        # Cargar configuración\n","        config = load_config()\n","        self.ccaa_ids = config[\"ccaa_ids\"]\n","        self.geo_map = config[\"geo_map\"]\n","        self.api_config = config[\"api_config\"]\n","        \n","        # Fechas\n","        if start_date and end_date:\n","            self.start_date = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n","            self.end_date = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n","        else:\n","            self.end_date = datetime.now()\n","            self.start_date = self.end_date - timedelta(days=365)\n","        \n","        self.geo_limits = [\"peninsular\", \"canarias\", \"baleares\", \"ceuta\", \"melilla\", \"ccaa\"]\n","        self.logs_success = f\"{self.base_lakehouse_path}/logs/success.log\"\n","        self.logs_error = f\"{self.base_lakehouse_path}/logs/error.log\"\n","    \n","    def _slugify(self, text: str) -> str:\n","        t = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n","        return re.sub(r\"[^a-z0-9\\-]\", \"\", t.lower().replace(\" \", \"-\"))\n","    \n","    def _clean_timestamp(self, timestamp: str) -> str:\n","        return timestamp.lower().replace(\":\", \"-\").replace(\".\", \"-\")\n","    \n","    # CAMBIO 2: Método para resolver geo_id y nombre\n","    def _resolve_geo(self, geo_limit: str, ccaa_name: Optional[str] = None) -> tuple:\n","        \"\"\"Retorna (geo_id, geo_name)\"\"\"\n","        if geo_limit == \"ccaa\" and ccaa_name:\n","            return self.ccaa_ids[ccaa_name], ccaa_name\n","        else:\n","            mapped_name = self.geo_map[geo_limit]\n","            return self.ccaa_ids[mapped_name], mapped_name\n","    \n","    def build_api_url(self, lang, category, widget, time_trunc, geo_limit, ccaa_name=None):\n","        start_str = self.start_date.strftime(\"%Y-%m-%dT%H:%M\")\n","        end_str = self.end_date.strftime(\"%Y-%m-%dT%H:%M\")\n","        base = f\"{self.base_url}/{lang}/datos/{category}/{widget}\"\n","        \n","        geo_id, _ = self._resolve_geo(geo_limit, ccaa_name)\n","        \n","        params = {\n","            \"start_date\": start_str,\n","            \"end_date\": end_str,\n","            \"time_trunc\": time_trunc,\n","            \"geo_trunc\": \"electric_system\",\n","            \"geo_limit\": geo_limit,\n","            \"geo_ids\": str(geo_id)\n","        }\n","        \n","        query = \"&\".join([f\"{k}={v}\" for k, v in params.items()])\n","        return f\"{base}?{query}\"\n","    \n","    def make_api_request(self, url: str):\n","        try:\n","            r = self.session.get(url, headers=self.headers, timeout=20)\n","            if r.status_code == 200:\n","                return True, r.json()\n","            return False, {}\n","        except Exception:\n","            return False, {}\n","    \n","    # CAMBIO 3: Enriquecer JSON con metadata de petición\n","    def save_to_bronze(self, data, category, widget, region, timestamp, time_trunc, \n","                       geo_id, geo_name, geo_limit, ccaa_name):\n","        # Agregar metadata al JSON\n","        enriched_data = {\n","            \"request_metadata\": {\n","                \"geo_id\": geo_id,\n","                \"geo_name\": geo_name,\n","                \"geo_limit\": geo_limit,\n","                \"ccaa_name\": ccaa_name,\n","                \"category\": category,\n","                \"widget\": widget,\n","                \"time_trunc\": time_trunc,\n","                \"ingestion_timestamp\": timestamp,\n","                \"start_date\": self.start_date.isoformat(),\n","                \"end_date\": self.end_date.isoformat()\n","            },\n","            \"api_response\": data\n","        }\n","        \n","        clean_ts = self._clean_timestamp(timestamp)\n","        fname = f\"brz-{region}-{category}-{widget}-{time_trunc}-{clean_ts}.json\"\n","        path = f\"{self.base_lakehouse_path}/data/{category}/{widget}/{time_trunc}/{fname}\"\n","        mssparkutils.fs.put(path, json.dumps(enriched_data, indent=2, ensure_ascii=False), overwrite=True)\n","        logger.info(f\"✅ Guardado: {path}\")\n","    \n","    def log(self, path, msg: str):\n","        try:\n","            mssparkutils.fs.put(path, msg + \"\\n\", overwrite=False)\n","        except:\n","            try:\n","                old = mssparkutils.fs.head(path, 1000000)\n","            except:\n","                old = \"\"\n","            mssparkutils.fs.put(path, old + msg + \"\\n\", overwrite=True)\n","    \n","    def explore_single(self, lang, category, widget, time_trunc, geo_limit, ccaa_name=None):\n","        ts = datetime.utcnow().isoformat(timespec=\"microseconds\") + \"Z\"\n","        url = self.build_api_url(lang, category, widget, time_trunc, geo_limit, ccaa_name)\n","        success, data = self.make_api_request(url)\n","        has_data = success and bool(data.get(\"included\"))\n","        \n","        region = self._slugify(ccaa_name) if geo_limit == \"ccaa\" and ccaa_name else geo_limit\n","        geo_id, geo_name = self._resolve_geo(geo_limit, ccaa_name)\n","        \n","        if has_data:\n","            self.save_to_bronze(data, category, widget, region, ts, time_trunc,\n","                              geo_id, geo_name, geo_limit, ccaa_name)\n","            self.log(self.logs_success, f\"{ts} OK {url}\")\n","        else:\n","            self.log(self.logs_error, f\"{ts} FAIL {url}\")\n","        \n","        return {\"url\": url, \"success\": success, \"has_data\": has_data}\n","    \n","    def explore_all(self, max_combinations=None, schedule_type=\"full\"):\n","        # Determinar estrategia según tipo de schedule\n","        if schedule_type == \"daily\":\n","            time_truncs = [\"day\"]\n","            geo_limits = [\"peninsular\", \"canarias\", \"baleares\", \"ceuta\", \"melilla\"]\n","        elif schedule_type == \"monthly\":\n","            time_truncs = [\"month\"]\n","            geo_limits = self.geo_limits\n","        else:  # full\n","            time_truncs = [\"day\", \"month\"]\n","            geo_limits = self.geo_limits\n","        \n","        combos = []\n","        for cat, widgets in self.api_config.items():\n","            for w in widgets:\n","                if schedule_type == \"full\":\n","                    # Diarios para agregaciones\n","                    for g in [\"peninsular\", \"canarias\", \"baleares\", \"ceuta\", \"melilla\"]:\n","                        combos.append((\"es\", cat, w, \"day\", g, None))\n","                    # Mensuales para todas incluyendo CCAA\n","                    for g in geo_limits:\n","                        if g == \"ccaa\":\n","                            for ccaa_name in self.ccaa_ids.keys():\n","                                if ccaa_name != \"Península\":\n","                                    combos.append((\"es\", cat, w, \"month\", g, ccaa_name))\n","                        else:\n","                            combos.append((\"es\", cat, w, \"month\", g, None))\n","                else:\n","                    for t in time_truncs:\n","                        for g in geo_limits:\n","                            if g == \"ccaa\":\n","                                for ccaa_name in self.ccaa_ids.keys():\n","                                    if ccaa_name != \"Península\":\n","                                        combos.append((\"es\", cat, w, t, g, ccaa_name))\n","                            else:\n","                                combos.append((\"es\", cat, w, t, g, None))\n","        \n","        if max_combinations:\n","            combos = combos[:max_combinations]\n","        \n","        logger.info(f\"Ejecutando {len(combos)} combinaciones\")\n","        results = []\n","        for c in combos:\n","            results.append(self.explore_single(*c))\n","            time.sleep(0.2)\n","        return results\n","    \n","    def analyze_results(self, results):\n","        total = len(results)\n","        ok = sum(r[\"success\"] for r in results)\n","        with_data = sum(r[\"has_data\"] for r in results)\n","        return {\"total\": total, \"ok\": ok, \"with_data\": with_data, \"failures\": total - ok}"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"63dedec3-cf05-4dd4-8b17-39a9f4f373ea"},{"cell_type":"code","source":["def main():\n","    max_comb = int(max_combinations_param.strip()) if max_combinations_param and max_combinations_param.strip().isdigit() else None\n","    \n","    explorer = REDataAPIExplorer(\n","        base_lakehouse_path=lakehouse_path_param,\n","        start_date=start_date_param if start_date_param else None,\n","        end_date=end_date_param if end_date_param else None\n","    )\n","    \n","    print(f\"🚀 Iniciando ingesta REData\")\n","    print(f\"📅 Rango: {explorer.start_date} - {explorer.end_date}\")\n","    print(f\"📢 Max combinaciones: {max_comb or 'Todas'}\")\n","    print(f\"⚙️ Schedule: {schedule_type_param}\")\n","    \n","    results = explorer.explore_all(max_combinations=max_comb, schedule_type=schedule_type_param)\n","    summary = explorer.analyze_results(results)\n","    print(\"📊 Resumen:\", summary)\n","    return summary\n","\n","if __name__ == \"__main__\":\n","    main()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7e0ec85f-f862-4f29-90be-f24500528ef6"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"ba66af83-4141-46ca-9c93-77547fefd59e"}],"default_lakehouse":"ba66af83-4141-46ca-9c93-77547fefd59e","default_lakehouse_name":"lh_energy_bronze","default_lakehouse_workspace_id":"19a8a038-bfb3-459c-81a7-84f41e7b6509"}}},"nbformat":4,"nbformat_minor":5}