{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35364bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1139332763.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    This notebook migrates existing Parquet files to Delta format and establishes the foundation for automated daily/monthly pipelines.\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Delta Migration and Pipeline Setup for OMIE Data\n",
    "\n",
    "This notebook migrates existing Parquet files to Delta format and establishes the foundation for automated daily/monthly pipelines.\n",
    "\n",
    "## Key Changes\n",
    "- **Delta Format**: ACID transactions, versioning, time travel\n",
    "- **Incremental Processing**: Track last processed files\n",
    "- **Pipeline Ready**: Designed for automated execution\n",
    "- **Change Detection**: Prevent duplicate processing\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Bronze Layer (Delta):\n",
    "bronze/OMIE/delta_tables/\n",
    "  - daily_prices/          # Daily price data (partitioned by year/month)\n",
    "  - metadata/              # Processing metadata and change tracking\n",
    "  - staging/               # Temporary staging for new files\n",
    "pipelines/\n",
    "  - daily_update.py        # Daily incremental updates\n",
    "  - monthly_maintenance.py # Monthly full validation\n",
    "```\n",
    "\n",
    "## Setup Instructions\n",
    "1. Run migration cells to convert existing Parquet to Delta\n",
    "2. Set up change detection and metadata tracking\n",
    "3. Test incremental processing logic\n",
    "4. Deploy as Fabric pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0155c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark with Delta Lake support\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import hashlib\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Import Spark and Delta Lake\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Initialize Spark session with Delta support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OMIE_Delta_Migration\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print('‚úÖ Spark with Delta Lake initialized')\n",
    "print(f'   Spark version: {spark.version}')\n",
    "\n",
    "# Detect environment and set paths\n",
    "if os.path.exists('/lakehouse/default/Files'):\n",
    "    LAKEHOUSE_ROOT = Path('/lakehouse/default/Files')\n",
    "    print(f'üè≠ Detected Fabric environment: {LAKEHOUSE_ROOT}')\n",
    "else:\n",
    "    # For local testing\n",
    "    LAKEHOUSE_ROOT = Path('Files')\n",
    "    print(f'üíª Local development mode: {LAKEHOUSE_ROOT}')\n",
    "\n",
    "# Define new Delta structure\n",
    "BRONZE_DIR = LAKEHOUSE_ROOT / 'bronze' / 'OMIE'\n",
    "DELTA_DIR = BRONZE_DIR / 'delta_tables'\n",
    "STAGING_DIR = DELTA_DIR / 'staging'\n",
    "METADATA_DIR = DELTA_DIR / 'metadata'\n",
    "DAILY_PRICES_DIR = DELTA_DIR / 'daily_prices'\n",
    "\n",
    "print(f'üìÅ Delta Lake structure will be created at: {DELTA_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5d3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Delta directories and scan existing Parquet files\n",
    "print(\"üìÅ Creating Delta directory structure...\")\n",
    "\n",
    "# Create Delta directories\n",
    "for dir_path in [DELTA_DIR, STAGING_DIR, METADATA_DIR, DAILY_PRICES_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"   ‚úÖ Created: {dir_path}\")\n",
    "\n",
    "print(\"\\nüîç Scanning for existing Parquet files...\")\n",
    "\n",
    "# Find all existing Parquet files\n",
    "existing_files = []\n",
    "if BRONZE_DIR.exists():\n",
    "    for year_dir in BRONZE_DIR.iterdir():\n",
    "        if year_dir.is_dir() and year_dir.name.isdigit():\n",
    "            year = int(year_dir.name)\n",
    "            parquet_files = list(year_dir.glob('*.parquet'))\n",
    "            \n",
    "            for file_path in parquet_files:\n",
    "                if 'manifest' not in file_path.name:  # Skip manifest files\n",
    "                    # Extract date from filename\n",
    "                    date_match = re.search(r'(\\d{8})', file_path.name)\n",
    "                    extraction_date = date_match.group(1) if date_match else None\n",
    "                    \n",
    "                    # Calculate file checksum for change detection\n",
    "                    file_checksum = None\n",
    "                    try:\n",
    "                        with open(file_path, 'rb') as f:\n",
    "                            file_checksum = hashlib.md5(f.read()).hexdigest()\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    existing_files.append({\n",
    "                        'file_path': str(file_path),\n",
    "                        'year': year,\n",
    "                        'filename': file_path.name,\n",
    "                        'file_size': file_path.stat().st_size if file_path.exists() else 0,\n",
    "                        'extraction_date': extraction_date,\n",
    "                        'file_checksum': file_checksum\n",
    "                    })\n",
    "\n",
    "print(f\"üìä Found {len(existing_files)} Parquet files to migrate:\")\n",
    "for year in sorted(set(f['year'] for f in existing_files)):\n",
    "    year_files = [f for f in existing_files if f['year'] == year]\n",
    "    total_size = sum(f['file_size'] for f in year_files) / (1024*1024)  # MB\n",
    "    print(f\"   üìÖ {year}: {len(year_files)} files ({total_size:.1f} MB)\")\n",
    "\n",
    "# Store file inventory for reference\n",
    "file_inventory = {\n",
    "    'scan_timestamp': datetime.now().isoformat(),\n",
    "    'total_files': len(existing_files),\n",
    "    'files_by_year': {str(year): len([f for f in existing_files if f['year'] == year]) \n",
    "                     for year in sorted(set(f['year'] for f in existing_files))},\n",
    "    'files': existing_files\n",
    "}\n",
    "\n",
    "# Save inventory\n",
    "inventory_path = METADATA_DIR / 'file_inventory.json'\n",
    "with open(inventory_path, 'w') as f:\n",
    "    json.dump(file_inventory, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ File inventory saved: {inventory_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db396b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Migration using per-year glob reads to avoid per-file HEAD requests\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Optional manual overrides (set these to GUIDs if auto-detection fails)\n",
    "WORKSPACE_ID_OVERRIDE = None  # e.g., \"ecf938c4-c449-48de-a07c-1d968a72b3d1\"\n",
    "LAKEHOUSE_ID_OVERRIDE = None  # e.g., \"12345678-aaaa-bbbb-cccc-1234567890ab\"\n",
    "\n",
    "# Resolve OneLake ABFSS path using Workspace and Lakehouse IDs to avoid friendly-name restrictions\n",
    "try:\n",
    "    from notebookutils import mssparkutils  # Fabric utility\n",
    "except Exception:\n",
    "    mssparkutils = None\n",
    "\n",
    "def _get_env_id(getter_names):\n",
    "    if not mssparkutils:\n",
    "        return None\n",
    "    for name in getter_names:\n",
    "        try:\n",
    "            fn = getattr(mssparkutils.env, name, None)\n",
    "            if callable(fn):\n",
    "                val = fn()\n",
    "                if val:\n",
    "                    return val\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def resolve_onelake_abfss(rel_path: str) -> str:\n",
    "    \"\"\"Return ABFSS GUID-based path if possible, else pass-through.\n",
    "    rel_path should begin with 'Files/...'\n",
    "    \"\"\"\n",
    "    # Respect manual overrides first\n",
    "    ws_id = WORKSPACE_ID_OVERRIDE or _get_env_id([\"getWorkspaceId\", \"getWorkspaceGUID\", \"getWorkspaceGuid\"])\n",
    "    lakehouse_id = LAKEHOUSE_ID_OVERRIDE or _get_env_id([\"getLakehouseId\", \"getArtifactId\", \"getItemId\"])  # try common names\n",
    "    if ws_id and lakehouse_id and rel_path:\n",
    "        rel_path = rel_path.replace(\"\\\\\", \"/\").lstrip(\"/\")\n",
    "        return f\"abfss://{ws_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/{rel_path}\"\n",
    "    return rel_path\n",
    "\n",
    "# Parameters\n",
    "# Prefer GUID-based ABFSS path to bypass FriendlyNameSupportDisabled\n",
    "friendly_base = \"Files/bronze/OMIE\"\n",
    "base_parquet_dir = resolve_onelake_abfss(friendly_base)\n",
    "print(f\"Base parquet dir resolved to: {base_parquet_dir}\")\n",
    "\n",
    "def _load_inventory_year_files(year: int):\n",
    "    \"\"\"Optional fallback: read previously saved file inventory and build per-file ABFSS paths.\"\"\"\n",
    "    try:\n",
    "        inv_path = str((METADATA_DIR / 'file_inventory.json').as_posix())\n",
    "        with open(inv_path, 'r') as f:\n",
    "            inv = json.load(f)\n",
    "        files = []\n",
    "        for rec in inv.get('files', []):\n",
    "            if rec.get('year') == year and rec.get('filename'):\n",
    "                files.append(resolve_onelake_abfss(f\"Files/bronze/OMIE/{year}/{rec['filename']}\"))\n",
    "        return files\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def read_year_df(year: int):\n",
    "    pattern = f\"{base_parquet_dir}/{year}/*.parquet\"\n",
    "    print(f\"‚û°Ô∏è Reading year {year} with pattern: {pattern}\")\n",
    "    try:\n",
    "        df = spark.read.parquet(pattern)\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Glob read failed for {year}: {e}\\n   ‚ñ∂Ô∏è  Trying per-file fallback via inventory (if available)...\")\n",
    "        files = _load_inventory_year_files(year)\n",
    "        if not files:\n",
    "            raise\n",
    "        print(f\"   ‚ÑπÔ∏è  Loading {len(files)} file(s) for {year} using GUID path\")\n",
    "        df = spark.read.parquet(files)\n",
    "\n",
    "    # Normalize columns\n",
    "    cols = [c.lower() for c in df.columns]\n",
    "    df = df.toDF(*cols)\n",
    "\n",
    "    # Expected fields & derived\n",
    "    df = df.withColumn(\"extraction_year\", F.lit(year).cast(\"int\"))\n",
    "\n",
    "    # Parse date from any available column\n",
    "    date_col = None\n",
    "    for candidate in [\"extraction_date\", \"_extraction_date\", \"extraction_date_parsed\"]:\n",
    "        if candidate in df.columns:\n",
    "            date_col = candidate\n",
    "            break\n",
    "\n",
    "    if date_col is None:\n",
    "        # Try to infer from filename if available\n",
    "        if \"_source_file\" in df.columns:\n",
    "            df = df.withColumn(\n",
    "                \"extraction_date_parsed\",\n",
    "                F.regexp_extract(F.col(\"_source_file\"), r\"(20\\d{6})\", 1)\n",
    "            )\n",
    "        else:\n",
    "            df = df.withColumn(\"extraction_date_parsed\", F.lit(None).cast(\"string\"))\n",
    "    else:\n",
    "        df = df.withColumn(\"extraction_date_parsed\", F.col(date_col).cast(\"string\"))\n",
    "\n",
    "    # Normalize price column for OMIE\n",
    "    if \"marginalpdbc\" in df.columns:\n",
    "        df = df.withColumn(\"marginal_price_eur_mwh\", F.col(\"marginalpdbc\").cast(\"double\"))\n",
    "    elif \"marginal_price_eur_mwh\" not in df.columns:\n",
    "        df = df.withColumn(\"marginal_price_eur_mwh\", F.lit(None).cast(\"double\"))\n",
    "\n",
    "    # Add basic quality & metadata\n",
    "    now_ts = F.current_timestamp()\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"price_valid\", (F.col(\"marginal_price_eur_mwh\").isNotNull()) & (F.col(\"marginal_price_eur_mwh\") >= 0))\n",
    "        .withColumn(\"price_category\", F.when(F.col(\"marginal_price_eur_mwh\") < 50, F.lit(\"LOW\")).when(F.col(\"marginal_price_eur_mwh\") < 150, F.lit(\"MID\")).otherwise(F.lit(\"HIGH\")))\n",
    "        .withColumn(\"data_quality_score\", F.when(F.col(\"price_valid\"), F.lit(1.0)).otherwise(F.lit(0.5)))\n",
    "        .withColumn(\"table_created_at\", now_ts)\n",
    "    )\n",
    "\n",
    "    # Ensure source_file exists for lineage\n",
    "    if \"_source_file\" in df.columns:\n",
    "        df = df.withColumn(\"source_file\", F.col(\"_source_file\"))\n",
    "    elif \"source_file\" not in df.columns:\n",
    "        df = df.withColumn(\"source_file\", F.lit(None).cast(\"string\"))\n",
    "\n",
    "    # Partition helpers\n",
    "    df = df.withColumn(\"extraction_month\", F.substring(F.col(\"extraction_date_parsed\"), 1, 6))\n",
    "    return df\n",
    "\n",
    "# Years to process\n",
    "years = sorted([int(y) for y in YEARS_TO_PROCESS]) if 'YEARS_TO_PROCESS' in locals() else [2023, 2024, 2025]\n",
    "print(f\"Years to process: {years}\")\n",
    "\n",
    "# Read and union per year\n",
    "dfs = []\n",
    "for y in years:\n",
    "    try:\n",
    "        yf = read_year_df(y)\n",
    "        cnt = yf.count()\n",
    "        print(f\"   ‚úÖ Year {y}: {cnt:,} rows\")\n",
    "        if cnt > 0:\n",
    "            dfs.append(yf)\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Skipping year {y} due to error: {e}\")\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No data loaded from parquet. Check paths and permissions.\")\n",
    "\n",
    "full_df = dfs[0]\n",
    "for d in dfs[1:]:\n",
    "    full_df = full_df.unionByName(d, allowMissingColumns=True)\n",
    "\n",
    "print(f\"Total rows after union: {full_df.count():,}\")\n",
    "\n",
    "# Write to unified Delta table (partitioned by year and month)\n",
    "unified_table = TARGET_DELTA_TABLE if 'TARGET_DELTA_TABLE' in locals() else \"brz_omie_daily_unified\"\n",
    "print(f\"Writing to Delta table: {unified_table}\")\n",
    "(\n",
    "    full_df\n",
    "    .repartition(\"extraction_year\", \"extraction_month\")\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"extraction_year\", \"extraction_month\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(unified_table)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Unified Delta write completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metadata tables for change detection and pipeline tracking\n",
    "def setup_metadata_tables():\n",
    "    \"\"\"Create Delta tables for tracking processing metadata and changes\"\"\"\n",
    "    \n",
    "    print(\"üìù Setting up metadata tables for change detection...\")\n",
    "    \n",
    "    # 1. Processing Log Table - tracks each file processing event\n",
    "    processing_log_schema = StructType([\n",
    "        StructField(\"file_name\", StringType(), False),\n",
    "        StructField(\"file_url\", StringType(), True),\n",
    "        StructField(\"file_size\", LongType(), True),\n",
    "        StructField(\"file_checksum\", StringType(), True),\n",
    "        StructField(\"extraction_date\", StringType(), True),\n",
    "        StructField(\"extraction_year\", IntegerType(), True),\n",
    "        StructField(\"processed_at\", TimestampType(), False),\n",
    "        StructField(\"processing_status\", StringType(), False),\n",
    "        StructField(\"row_count\", LongType(), True),\n",
    "        StructField(\"delta_version\", LongType(), True),\n",
    "        StructField(\"pipeline_run_id\", StringType(), True),\n",
    "        StructField(\"processing_duration_seconds\", DoubleType(), True),\n",
    "        StructField(\"error_message\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Create empty processing log\n",
    "    empty_log = spark.createDataFrame([], processing_log_schema)\n",
    "    processing_log_path = METADATA_DIR / 'processing_log'\n",
    "    \n",
    "    try:\n",
    "        empty_log.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(str(processing_log_path))\n",
    "        print(f\"   ‚úÖ Processing log table: {processing_log_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed to create processing log: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 2. Last Processed Table - tracks the state of incremental processing\n",
    "    last_processed_schema = StructType([\n",
    "        StructField(\"data_source\", StringType(), False),\n",
    "        StructField(\"last_processed_date\", StringType(), False),\n",
    "        StructField(\"last_processed_file\", StringType(), True),\n",
    "        StructField(\"last_update_timestamp\", TimestampType(), False),\n",
    "        StructField(\"files_processed\", IntegerType(), True),\n",
    "        StructField(\"total_rows\", LongType(), True),\n",
    "        StructField(\"current_delta_version\", LongType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Initialize with migration state\n",
    "    if migration_results:\n",
    "        latest_date = \"20250101\"\n",
    "        total_files = len([r for r in migration_results if r['status'] == 'success'])\n",
    "        total_rows = sum(r['rows'] for r in migration_results if r['status'] == 'success')\n",
    "        \n",
    "        # Find latest date from successfully migrated files\n",
    "        for result in migration_results:\n",
    "            if result['status'] == 'success':\n",
    "                file_path = result['file_path']\n",
    "                date_match = re.search(r'(\\d{8})', Path(file_path).name)\n",
    "                if date_match:\n",
    "                    file_date = date_match.group(1)\n",
    "                    if file_date > latest_date:\n",
    "                        latest_date = file_date\n",
    "        \n",
    "        initial_data = [(\n",
    "            \"OMIE_daily_prices\",\n",
    "            latest_date,\n",
    "            f\"migration_completed_{total_files}_files\",\n",
    "            datetime.now(),\n",
    "            total_files,\n",
    "            total_rows,\n",
    "            1  # Initial version after migration\n",
    "        )]\n",
    "    else:\n",
    "        initial_data = [(\n",
    "            \"OMIE_daily_prices\",\n",
    "            \"20230101\", \n",
    "            None,\n",
    "            datetime.now(),\n",
    "            0,\n",
    "            0,\n",
    "            0\n",
    "        )]\n",
    "    \n",
    "    last_processed_df = spark.createDataFrame(initial_data, last_processed_schema)\n",
    "    last_processed_path = METADATA_DIR / 'last_processed'\n",
    "    \n",
    "    try:\n",
    "        last_processed_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(str(last_processed_path))\n",
    "        print(f\"   ‚úÖ Last processed table: {last_processed_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed to create last processed table: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 3. File Change Tracking - for detecting new/modified files\n",
    "    file_tracking_schema = StructType([\n",
    "        StructField(\"file_url\", StringType(), False),\n",
    "        StructField(\"file_name\", StringType(), False),\n",
    "        StructField(\"file_size\", LongType(), True),\n",
    "        StructField(\"file_checksum\", StringType(), True),\n",
    "        StructField(\"last_modified\", TimestampType(), True),\n",
    "        StructField(\"first_seen\", TimestampType(), False),\n",
    "        StructField(\"last_checked\", TimestampType(), False),\n",
    "        StructField(\"processing_status\", StringType(), False),\n",
    "        StructField(\"extraction_date\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    empty_tracking = spark.createDataFrame([], file_tracking_schema)\n",
    "    file_tracking_path = METADATA_DIR / 'file_change_tracking'\n",
    "    \n",
    "    try:\n",
    "        empty_tracking.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(str(file_tracking_path))\n",
    "        print(f\"   ‚úÖ File change tracking table: {file_tracking_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed to create file tracking table: {e}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"‚úÖ All metadata tables created successfully!\")\n",
    "    \n",
    "    return {\n",
    "        'processing_log': str(processing_log_path),\n",
    "        'last_processed': str(last_processed_path),\n",
    "        'file_change_tracking': str(file_tracking_path)\n",
    "    }\n",
    "\n",
    "# Setup metadata tables\n",
    "metadata_tables = setup_metadata_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4526ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Delta table and test operations\n",
    "def verify_delta_table():\n",
    "    \"\"\"Verify the Delta table was created correctly and test key operations\"\"\"\n",
    "    \n",
    "    print(\"üß™ Verifying Delta table and testing operations...\")\n",
    "    \n",
    "    try:\n",
    "        # Read Delta table\n",
    "        delta_df = spark.read.format(\"delta\").load(str(DAILY_PRICES_DIR))\n",
    "        \n",
    "        # Basic statistics\n",
    "        total_rows = delta_df.count()\n",
    "        total_columns = len(delta_df.columns)\n",
    "        \n",
    "        print(f\"‚úÖ Delta table verification:\")\n",
    "        print(f\"   üìä Total rows: {total_rows:,}\")\n",
    "        print(f\"   üìã Total columns: {total_columns}\")\n",
    "        \n",
    "        # Schema verification\n",
    "        print(f\"\\nüìã Delta table schema:\")\n",
    "        delta_df.printSchema()\n",
    "        \n",
    "        # Partition information\n",
    "        print(f\"\\nüìÅ Partition summary:\")\n",
    "        partition_summary = delta_df.groupBy(\"partition_year\", \"partition_month\") \\\n",
    "            .agg(F.count(\"*\").alias(\"row_count\"),\n",
    "                 F.min(\"extraction_date_parsed\").alias(\"min_date\"),\n",
    "                 F.max(\"extraction_date_parsed\").alias(\"max_date\")) \\\n",
    "            .orderBy(\"partition_year\", \"partition_month\")\n",
    "        \n",
    "        partition_summary.show(20, truncate=False)\n",
    "        \n",
    "        # Data quality checks\n",
    "        print(f\"\\nüîç Data quality summary:\")\n",
    "        quality_stats = delta_df.agg(\n",
    "            F.count(\"*\").alias(\"total_records\"),\n",
    "            F.sum(F.when(F.col(\"price_valid\"), 1).otherwise(0)).alias(\"valid_prices\"),\n",
    "            F.avg(\"marginal_price_eur_mwh\").alias(\"avg_price\"),\n",
    "            F.min(\"marginal_price_eur_mwh\").alias(\"min_price\"),\n",
    "            F.max(\"marginal_price_eur_mwh\").alias(\"max_price\"),\n",
    "            F.countDistinct(\"source_file\").alias(\"unique_files\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"   Total records: {quality_stats['total_records']:,}\")\n",
    "        print(f\"   Valid prices: {quality_stats['valid_prices']:,} ({quality_stats['valid_prices']/quality_stats['total_records']*100:.1f}%)\")\n",
    "        print(f\"   Average price: {quality_stats['avg_price']:.2f} ‚Ç¨/MWh\")\n",
    "        print(f\"   Price range: {quality_stats['min_price']:.2f} - {quality_stats['max_price']:.2f} ‚Ç¨/MWh\")\n",
    "        print(f\"   Unique files: {quality_stats['unique_files']}\")\n",
    "        \n",
    "        # Test Delta features\n",
    "        print(f\"\\n‚è∞ Testing Delta features:\")\n",
    "        \n",
    "        # 1. Table history\n",
    "        try:\n",
    "            history_df = spark.sql(f\"DESCRIBE HISTORY delta.`{DAILY_PRICES_DIR}`\")\n",
    "            version_count = history_df.count()\n",
    "            print(f\"   üìú Table versions: {version_count}\")\n",
    "            \n",
    "            # Show recent operations\n",
    "            print(f\"   Recent operations:\")\n",
    "            history_df.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\") \\\n",
    "                .orderBy(F.desc(\"version\")) \\\n",
    "                .show(3, truncate=False)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  History check failed: {e}\")\n",
    "        \n",
    "        # 2. Time travel (if multiple versions exist)\n",
    "        try:\n",
    "            v0_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(str(DAILY_PRICES_DIR))\n",
    "            v0_count = v0_df.count()\n",
    "            print(f\"   üïê Version 0 rows: {v0_count:,}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Time travel test failed: {e}\")\n",
    "        \n",
    "        # 3. Sample data\n",
    "        print(f\"\\nüîç Sample data (latest 5 records):\")\n",
    "        delta_df.orderBy(F.desc(\"migrated_at\")) \\\n",
    "            .select(\"extraction_date_parsed\", \"marginal_price_eur_mwh\", \"price_category\", \n",
    "                   \"source_file\", \"partition_year\", \"partition_month\") \\\n",
    "            .show(5, truncate=False)\n",
    "        \n",
    "        # 4. Test metadata tables\n",
    "        if metadata_tables:\n",
    "            print(f\"\\nüìä Metadata tables status:\")\n",
    "            for table_name, table_path in metadata_tables.items():\n",
    "                try:\n",
    "                    meta_df = spark.read.format(\"delta\").load(table_path)\n",
    "                    row_count = meta_df.count()\n",
    "                    print(f\"   {table_name}: {row_count} records\")\n",
    "                    \n",
    "                    if table_name == \"last_processed\" and row_count > 0:\n",
    "                        print(\"   Latest processing state:\")\n",
    "                        meta_df.show(truncate=False)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå {table_name}: Error - {e}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ All verifications completed successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Delta table verification failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Run verification\n",
    "verification_success = verify_delta_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96941b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification adapted to unified table and OMIE fields\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "unified_table = TARGET_DELTA_TABLE if 'TARGET_DELTA_TABLE' in locals() else \"brz_omie_daily_unified\"\n",
    "print(f\"Verifying table: {unified_table}\")\n",
    "\n",
    "try:\n",
    "    df = spark.table(unified_table)\n",
    "    print(f\"Rows: {df.count():,}\")\n",
    "    df.select(\n",
    "        F.min(\"extraction_date_parsed\").alias(\"min_date\"),\n",
    "        F.max(\"extraction_date_parsed\").alias(\"max_date\"),\n",
    "        F.min(\"marginal_price_eur_mwh\").alias(\"min_price\"),\n",
    "        F.max(\"marginal_price_eur_mwh\").alias(\"max_price\"),\n",
    "        F.avg(\"marginal_price_eur_mwh\").alias(\"avg_price\")\n",
    "    ).show(truncate=False)\n",
    "\n",
    "    spark.sql(f\"DESCRIBE HISTORY {unified_table}\").show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"Verification error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be438e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate configuration and summary for pipeline development\n",
    "def generate_migration_summary():\n",
    "    \"\"\"Generate a comprehensive summary and export configuration for pipelines\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"üéâ DELTA MIGRATION COMPLETED\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Migration summary\n",
    "    if migration_results:\n",
    "        success_count = len([r for r in migration_results if r['status'] == 'success'])\n",
    "        failed_count = len([r for r in migration_results if r['status'] == 'failed'])\n",
    "        total_rows = sum(r['rows'] for r in migration_results if r['status'] == 'success')\n",
    "        \n",
    "        print(f\"\\nüìä Migration Results:\")\n",
    "        print(f\"   ‚úÖ Successful files: {success_count}\")\n",
    "        print(f\"   ‚ùå Failed files: {failed_count}\")\n",
    "        print(f\"   üìà Total rows migrated: {total_rows:,}\")\n",
    "        \n",
    "        # Show year breakdown\n",
    "        print(f\"\\nüìÖ Data by year:\")\n",
    "        for year in sorted(set(r['year'] for r in migration_results if r['status'] == 'success')):\n",
    "            year_rows = sum(r['rows'] for r in migration_results if r['year'] == year and r['status'] == 'success')\n",
    "            year_files = len([r for r in migration_results if r['year'] == year and r['status'] == 'success'])\n",
    "            print(f\"   {year}: {year_files} files, {year_rows:,} rows\")\n",
    "    \n",
    "    # Directory structure\n",
    "    print(f\"\\nüìÅ Delta Lake Structure Created:\")\n",
    "    print(f\"   Lakehouse root: {LAKEHOUSE_ROOT}\")\n",
    "    print(f\"   Delta tables: {DELTA_DIR}\")\n",
    "    print(f\"   Daily prices: {DAILY_PRICES_DIR}\")\n",
    "    print(f\"   Metadata: {METADATA_DIR}\")\n",
    "    print(f\"   Staging: {STAGING_DIR}\")\n",
    "    \n",
    "    # Current table status\n",
    "    if verification_success:\n",
    "        try:\n",
    "            delta_df = spark.read.format(\"delta\").load(str(DAILY_PRICES_DIR))\n",
    "            total_rows = delta_df.count()\n",
    "            partitions = delta_df.select(\"partition_year\", \"partition_month\").distinct().count()\n",
    "            \n",
    "            print(f\"\\nüìä Delta Table Status:\")\n",
    "            print(f\"   Total rows: {total_rows:,}\")\n",
    "            print(f\"   Partitions: {partitions}\")\n",
    "            print(f\"   Columns: {len(delta_df.columns)}\")\n",
    "            \n",
    "            # Date range\n",
    "            date_range = delta_df.agg(\n",
    "                F.min(\"extraction_date_parsed\").alias(\"min_date\"),\n",
    "                F.max(\"extraction_date_parsed\").alias(\"max_date\")\n",
    "            ).collect()[0]\n",
    "            \n",
    "            if date_range['min_date'] and date_range['max_date']:\n",
    "                print(f\"   Date range: {date_range['min_date']} to {date_range['max_date']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Could not read table status: {e}\")\n",
    "    \n",
    "    # Export configuration for pipelines\n",
    "    config = {\n",
    "        'migration': {\n",
    "            'completed_at': datetime.now().isoformat(),\n",
    "            'lakehouse_root': str(LAKEHOUSE_ROOT),\n",
    "            'delta_tables_dir': str(DELTA_DIR),\n",
    "            'daily_prices_table': str(DAILY_PRICES_DIR),\n",
    "            'metadata_dir': str(METADATA_DIR),\n",
    "            'staging_dir': str(STAGING_DIR),\n",
    "            'success': verification_success and optimization_success\n",
    "        },\n",
    "        'tables': {\n",
    "            'daily_prices': str(DAILY_PRICES_DIR),\n",
    "            'processing_log': metadata_tables['processing_log'] if metadata_tables else None,\n",
    "            'last_processed': metadata_tables['last_processed'] if metadata_tables else None,\n",
    "            'file_change_tracking': metadata_tables['file_change_tracking'] if metadata_tables else None\n",
    "        },\n",
    "        'pipeline_config': {\n",
    "            'omie_base_url': 'https://www.omie.es',\n",
    "            'target_years': [2023, 2024, 2025],\n",
    "            'file_patterns': ['marginalpdbc'],\n",
    "            'update_schedule': {\n",
    "                'daily': '0 6 * * *',  # 6 AM UTC daily\n",
    "                'monthly': '0 2 1 * *'  # 2 AM on 1st of month\n",
    "            }\n",
    "        },\n",
    "        'spark_config': {\n",
    "            'app_name': 'OMIE_Pipeline',\n",
    "            'delta_enabled': True,\n",
    "            'adaptive_query_enabled': True,\n",
    "            'coalesce_partitions_enabled': True,\n",
    "            'delta_optimizations': {\n",
    "                'autoOptimize': True,\n",
    "                'autoCompact': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save configuration\n",
    "    config_path = METADATA_DIR / 'pipeline_config.json'\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüìã Pipeline configuration exported: {config_path}\")\n",
    "    \n",
    "    # Next steps\n",
    "    print(f\"\\nüîÑ Next Steps:\")\n",
    "    print(f\"   1. ‚úÖ Delta format migration complete\")\n",
    "    print(f\"   2. üîÑ Create daily incremental update pipeline\")\n",
    "    print(f\"   3. üîÑ Create monthly maintenance pipeline\")\n",
    "    print(f\"   4. üîÑ Set up change detection for new files\")\n",
    "    print(f\"   5. üîÑ Create managed Delta tables in catalog\")\n",
    "    print(f\"   6. üîÑ Build Fabric Data Factory pipelines\")\n",
    "    print(f\"   7. üîÑ Test end-to-end pipeline execution\")\n",
    "    \n",
    "    print(f\"\\nüí° Ready for Pipeline Development!\")\n",
    "    print(f\"   The unified Delta table provides:\")\n",
    "    print(f\"   - ACID transactions and versioning\")\n",
    "    print(f\"   - Optimized partitioning by year/month\") \n",
    "    print(f\"   - Built-in change detection metadata\")\n",
    "    print(f\"   - Time travel and rollback capabilities\")\n",
    "    print(f\"   - Ready for incremental updates\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Generate final summary and configuration\n",
    "final_config = generate_migration_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
