{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fc0e62",
   "metadata": {},
   "source": [
    "# OMIE Daily Table Ingestion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Creates Delta tables from OMIE Bronze layer Parquet files.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Input:** Parquet files in `Files/bronze/OMIE/{year}/`  \n",
    "\n",
    "\n",
    "**Output:** 3 Delta tables (one per year): `brz_omie_daily_2023`, `brz_omie_daily_2024`, `brz_omie_daily_2025`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Following the same pattern as OPEN-METEO colleagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43980eb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import json, os\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"üöÄ OMIE Daily Table Ingestion Started\")\n",
    "print(f\"‚úÖ Spark session active: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Recibe par√°metros del pipeline\n",
    "# -------------------------------\n",
    "# Esperamos que la primera actividad devuelva JSON con:\n",
    "# {\"start_date\": \"...\", \"end_date\": \"...\", \"years_touched\": [2023,2024,2025]}\n",
    "\n",
    "try:\n",
    "    dbutils.widgets.text(\"updated_json\", \"\")\n",
    "    updated_json = dbutils.widgets.get(\"updated_json\")\n",
    "    \n",
    "    if not updated_json.strip():\n",
    "        print(\"‚ö†Ô∏è  No se recibieron par√°metros del pipeline. Usando configuraci√≥n por defecto.\")\n",
    "        # Configuraci√≥n por defecto: procesar todos los a√±os\n",
    "        years_to_process = [2023, 2024, 2025]\n",
    "    else:\n",
    "        payload = json.loads(updated_json)\n",
    "        years_to_process = payload.get(\"years_touched\", [2023, 2024, 2025])\n",
    "        \n",
    "except:\n",
    "    # Fallback para ejecuci√≥n local/manual\n",
    "    print(\"üíª Ejecuci√≥n local detectada. Procesando todos los a√±os.\")\n",
    "    years_to_process = [2023, 2024, 2025]\n",
    "    \n",
    "if not years_to_process:\n",
    "    print(\"‚ùå No hay a√±os para procesar. Terminando notebook.\")\n",
    "    try:\n",
    "        dbutils.notebook.exit(\"No hay trabajo\")\n",
    "    except:\n",
    "        print(\"Terminando ejecuci√≥n local.\")\n",
    "        \n",
    "print(f\"üîπ A√±os a procesar: {years_to_process}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cef93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Construir rutas base seg√∫n a√±os\n",
    "# -------------------------------\n",
    "\n",
    "# Detectar el entorno y ajustar las rutas\n",
    "try:\n",
    "    # Intentar detectar si estamos en un lakehouse espec√≠fico\n",
    "    current_lakehouse = mssparkutils.fs.ls(\"/\")\n",
    "    \n",
    "    # En Fabric, usar rutas relativas al lakehouse actual\n",
    "    BASE_DIR = \"/Files/bronze/OMIE\"\n",
    "    base_paths = [f\"{BASE_DIR}/{str(y)}\" for y in years_to_process]\n",
    "    \n",
    "    print(f\"üìÅ Directorio base (Fabric): {BASE_DIR}\")\n",
    "    print(f\"üè¢ Entorno: Microsoft Fabric Lakehouse\")\n",
    "    \n",
    "except:\n",
    "    # Fallback para desarrollo local\n",
    "    BASE_DIR = \"Files/bronze/OMIE\"\n",
    "    base_paths = [os.path.join(BASE_DIR, str(y)) for y in years_to_process]\n",
    "    \n",
    "    print(f\"üìÅ Directorio base (Local): {BASE_DIR}\")\n",
    "    print(f\"üíª Entorno: Desarrollo local\")\n",
    "\n",
    "print(f\"üìÇ Rutas a procesar: {base_paths}\")\n",
    "\n",
    "# Funci√≥n mejorada para verificar rutas\n",
    "def check_path_safely(path):\n",
    "    \"\"\"Verifica una ruta de forma segura con m√∫ltiples m√©todos\"\"\"\n",
    "    try:\n",
    "        # M√©todo 1: Usar mssparkutils.fs.ls\n",
    "        files = mssparkutils.fs.ls(path)\n",
    "        parquet_files = [f for f in files if f.name.endswith(\".parquet\")]\n",
    "        return True, len(parquet_files), \"mssparkutils\"\n",
    "    \n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            # M√©todo 2: Usar spark directamente para verificar si hay archivos\n",
    "            test_pattern = f\"{path}/*.parquet\"\n",
    "            df_test = spark.read.parquet(test_pattern)\n",
    "            # Si llegamos aqu√≠, hay al menos un archivo Parquet\n",
    "            file_count = len(mssparkutils.fs.ls(path)) if path.startswith(\"/\") else 1\n",
    "            return True, file_count, \"spark_direct\"\n",
    "        \n",
    "        except Exception as e2:\n",
    "            # M√©todo 3: Verificar con diferentes formatos de path\n",
    "            alternative_paths = []\n",
    "            \n",
    "            if not path.startswith(\"/\"):\n",
    "                alternative_paths.append(f\"/{path}\")\n",
    "            if not path.startswith(\"abfss://\"):\n",
    "                alternative_paths.append(f\"Files/bronze/OMIE/{os.path.basename(path)}\")\n",
    "            \n",
    "            for alt_path in alternative_paths:\n",
    "                try:\n",
    "                    files = mssparkutils.fs.ls(alt_path)\n",
    "                    parquet_files = [f for f in files if f.name.endswith(\".parquet\")]\n",
    "                    if parquet_files:\n",
    "                        return True, len(parquet_files), f\"alternative:{alt_path}\"\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return False, 0, f\"failed: {str(e1)[:100]}\"\n",
    "\n",
    "# Verificar que existen las rutas\n",
    "available_paths = []\n",
    "for path in base_paths:\n",
    "    year = os.path.basename(path)\n",
    "    print(f\"\\nüîç Verificando a√±o {year}: {path}\")\n",
    "    \n",
    "    is_available, file_count, method = check_path_safely(path)\n",
    "    \n",
    "    if is_available and file_count > 0:\n",
    "        available_paths.append((path, file_count))\n",
    "        print(f\"‚úÖ {path}: {file_count} archivos Parquet (m√©todo: {method})\")\n",
    "    elif is_available and file_count == 0:\n",
    "        print(f\"‚ö†Ô∏è  {path}: Directorio accesible pero sin archivos Parquet (m√©todo: {method})\")\n",
    "    else:\n",
    "        print(f\"‚ùå {path}: No accesible - {method}\")\n",
    "\n",
    "if not available_paths:\n",
    "    print(\"\\n‚ùå No se encontraron archivos Parquet en ninguna ruta.\")\n",
    "    print(\"üí° Posibles soluciones:\")\n",
    "    print(\"   1. Verificar que el notebook omie.ipynb se ha ejecutado correctamente\")\n",
    "    print(\"   2. Confirmar que est√°s en el lakehouse correcto\")\n",
    "    print(\"   3. Verificar las rutas de los archivos Parquet\")\n",
    "    \n",
    "    # Intentar mostrar lo que hay en el directorio bronze base\n",
    "    try:\n",
    "        bronze_base = \"/Files/bronze/OMIE\" if BASE_DIR.startswith(\"/\") else \"Files/bronze/OMIE\"\n",
    "        print(f\"\\nüîç Contenido del directorio base {bronze_base}:\")\n",
    "        base_files = mssparkutils.fs.ls(bronze_base)\n",
    "        for f in base_files:\n",
    "            print(f\"   {'üìÅ' if f.isDir else 'üìÑ'} {f.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå No se pudo acceder al directorio base: {e}\")\n",
    "    \n",
    "    try:\n",
    "        dbutils.notebook.exit(\"Sin archivos\")\n",
    "    except:\n",
    "        print(\"Terminando ejecuci√≥n local.\")\n",
    "\n",
    "print(f\"\\nüéØ Rutas v√°lidas encontradas: {len(available_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bb3016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Diagn√≥stico del entorno Fabric y verificaci√≥n de prerequisitos\n",
    "# -------------------------------\n",
    "\n",
    "print(\"üîç Diagnosticando entorno de Microsoft Fabric...\")\n",
    "\n",
    "# Variables para tracking del diagn√≥stico\n",
    "bronze_data_exists = False\n",
    "years_with_data = []\n",
    "lakehouse_root = None\n",
    "\n",
    "try:\n",
    "    # Verificar directorio ra√≠z\n",
    "    root_contents = mssparkutils.fs.ls(\"/\")\n",
    "    print(f\"üìÇ Contenido directorio ra√≠z:\")\n",
    "    for item in root_contents[:10]:  # Mostrar solo los primeros 10\n",
    "        print(f\"   {'üìÅ' if item.isDir else 'üìÑ'} {item.name}\")\n",
    "    \n",
    "    # En Fabric, buscar el lakehouse correcto explorando los directorios GUID\n",
    "    print(f\"\\nüîç Buscando estructura lakehouse en directorios GUID...\")\n",
    "    \n",
    "    # Verificar si existe Files directamente en root\n",
    "    files_exists = any(item.name == \"Files\" and item.isDir for item in root_contents)\n",
    "    \n",
    "    if files_exists:\n",
    "        lakehouse_root = \"/Files\"\n",
    "        print(f\"‚úÖ Encontrado Files en root: {lakehouse_root}\")\n",
    "    else:\n",
    "        # Buscar Files dentro de los directorios GUID (lakehouses)\n",
    "        for guid_item in root_contents:\n",
    "            if guid_item.isDir:\n",
    "                try:\n",
    "                    guid_contents = mssparkutils.fs.ls(f\"/{guid_item.name}\")\n",
    "                    if any(item.name == \"Files\" and item.isDir for item in guid_contents):\n",
    "                        lakehouse_root = f\"/{guid_item.name}/Files\"\n",
    "                        print(f\"‚úÖ Encontrado lakehouse en: {lakehouse_root}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    if lakehouse_root:\n",
    "        print(f\"üìÅ Usando lakehouse root: {lakehouse_root}\")\n",
    "        \n",
    "        # Verificar contenido de Files\n",
    "        files_contents = mssparkutils.fs.ls(lakehouse_root)\n",
    "        print(f\"üìÇ Contenido de {lakehouse_root}:\")\n",
    "        for item in files_contents:\n",
    "            print(f\"   {'üìÅ' if item.isDir else 'üìÑ'} {item.name}\")\n",
    "        \n",
    "        # Verificar si existe bronze\n",
    "        bronze_exists = any(item.name == \"bronze\" and item.isDir for item in files_contents)\n",
    "        print(f\"üìÅ Directorio 'bronze' existe: {'‚úÖ' if bronze_exists else '‚ùå'}\")\n",
    "        \n",
    "        if bronze_exists:\n",
    "            # Verificar contenido de bronze\n",
    "            bronze_path = f\"{lakehouse_root}/bronze\"\n",
    "            bronze_contents = mssparkutils.fs.ls(bronze_path)\n",
    "            print(f\"üìÇ Contenido de {bronze_path}:\")\n",
    "            for item in bronze_contents:\n",
    "                print(f\"   {'üìÅ' if item.isDir else 'üìÑ'} {item.name}\")\n",
    "            \n",
    "            # Verificar si existe OMIE\n",
    "            omie_exists = any(item.name == \"OMIE\" and item.isDir for item in bronze_contents)\n",
    "            print(f\"üìÅ Directorio 'OMIE' existe: {'‚úÖ' if omie_exists else '‚ùå'}\")\n",
    "            \n",
    "            if omie_exists:\n",
    "                # Verificar contenido de OMIE\n",
    "                omie_path = f\"{bronze_path}/OMIE\"\n",
    "                omie_contents = mssparkutils.fs.ls(omie_path)\n",
    "                print(f\"üìÇ Contenido de {omie_path}:\")\n",
    "                \n",
    "                for item in omie_contents:\n",
    "                    print(f\"   {'üìÅ' if item.isDir else 'üìÑ'} {item.name}\")\n",
    "                    \n",
    "                    # Si es directorio num√©rico (a√±o), verificar su contenido\n",
    "                    if item.isDir and item.name.isdigit():\n",
    "                        try:\n",
    "                            year_path = f\"{omie_path}/{item.name}\"\n",
    "                            year_contents = mssparkutils.fs.ls(year_path)\n",
    "                            parquet_count = sum(1 for f in year_contents if f.name.endswith(\".parquet\"))\n",
    "                            total_files = len(year_contents)\n",
    "                            \n",
    "                            print(f\"      üì¶ {item.name}: {total_files} archivos total, {parquet_count} Parquet\")\n",
    "                            \n",
    "                            if parquet_count > 0:\n",
    "                                years_with_data.append(int(item.name))\n",
    "                                bronze_data_exists = True\n",
    "                                \n",
    "                                # Mostrar algunos archivos de ejemplo\n",
    "                                parquet_files = [f.name for f in year_contents if f.name.endswith(\".parquet\")][:3]\n",
    "                                print(f\"         Ejemplos: {', '.join(parquet_files)}\")\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"      ‚ùå {item.name}: Error accediendo - {str(e)[:50]}...\")\n",
    "            else:\n",
    "                print(\"‚ùå No se encontr√≥ el directorio OMIE en bronze\")\n",
    "        else:\n",
    "            print(\"‚ùå No se encontr√≥ el directorio bronze\")\n",
    "    else:\n",
    "        print(\"‚ùå No se encontr√≥ estructura lakehouse v√°lida\")\n",
    "        \n",
    "        # Como fallback, intentar buscar directamente por los paths que mencionaste\n",
    "        print(f\"\\nüîç Intentando paths alternativos mencionados por el usuario...\")\n",
    "        alternative_paths = [\n",
    "            \"Files/bronze/OMIE\",\n",
    "            \"/Files/bronze/OMIE\"\n",
    "        ]\n",
    "        \n",
    "        for alt_path in alternative_paths:\n",
    "            try:\n",
    "                print(f\"   üîç Probando: {alt_path}\")\n",
    "                omie_contents = mssparkutils.fs.ls(alt_path)\n",
    "                \n",
    "                print(f\"   ‚úÖ Acceso exitoso a {alt_path}\")\n",
    "                lakehouse_root = \"/Files\" if alt_path.startswith(\"/\") else \"Files\"\n",
    "                \n",
    "                for item in omie_contents:\n",
    "                    print(f\"      {'üìÅ' if item.isDir else 'üìÑ'} {item.name}\")\n",
    "                    \n",
    "                    if item.isDir and item.name.isdigit():\n",
    "                        try:\n",
    "                            year_path = f\"{alt_path}/{item.name}\"\n",
    "                            year_contents = mssparkutils.fs.ls(year_path)\n",
    "                            parquet_count = sum(1 for f in year_contents if f.name.endswith(\".parquet\"))\n",
    "                            \n",
    "                            if parquet_count > 0:\n",
    "                                years_with_data.append(int(item.name))\n",
    "                                bronze_data_exists = True\n",
    "                                print(f\"         ‚úÖ {item.name}: {parquet_count} archivos Parquet\")\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            print(f\"         ‚ùå {item.name}: {str(e)[:50]}...\")\n",
    "                \n",
    "                break  # Si encontramos uno que funciona, parar\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå {alt_path}: {str(e)[:50]}...\")\n",
    "                continue\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error en diagn√≥stico: {e}\")\n",
    "    print(\"üí° Esto puede indicar que el notebook no est√° ejecut√°ndose en un lakehouse v√°lido\")\n",
    "\n",
    "# Resultado del diagn√≥stico\n",
    "print(f\"\\nüìä Resultado del diagn√≥stico:\")\n",
    "print(f\"   üéØ Datos Bronze OMIE encontrados: {'‚úÖ' if bronze_data_exists else '‚ùå'}\")\n",
    "print(f\"   üìÖ A√±os con datos: {sorted(years_with_data) if years_with_data else 'Ninguno'}\")\n",
    "print(f\"   üìÅ Lakehouse root: {lakehouse_root if lakehouse_root else 'No detectado'}\")\n",
    "\n",
    "if not bronze_data_exists:\n",
    "    print(f\"\\nüö® PROBLEMA DETECTADO:\")\n",
    "    print(f\"   ‚ùå No se encontraron archivos Parquet en el directorio Bronze OMIE\")\n",
    "    print(f\"   üí° SOLUCI√ìN:\")\n",
    "    print(f\"      1. Verificar que est√°s en el lakehouse correcto\")\n",
    "    print(f\"      2. Si los datos existen, ajustar las rutas en el siguiente paso\")\n",
    "    print(f\"      3. O ejecutar el notebook 'omie.ipynb' para crear los datos Bronze\")\n",
    "    \n",
    "    # No terminar autom√°ticamente, permitir ajustes manuales\n",
    "    print(f\"\\n‚ö†Ô∏è  Continuando para permitir ajustes manuales de rutas...\")\n",
    "        \n",
    "else:\n",
    "    print(f\"\\n‚úÖ Prerequisitos cumplidos!\")\n",
    "    print(f\"   üì¶ Datos Bronze disponibles para a√±os: {sorted(years_with_data)}\")\n",
    "    print(f\"   üéØ Listo para crear tablas Delta\")\n",
    "\n",
    "# Hacer disponible el lakehouse root para celdas posteriores\n",
    "if lakehouse_root:\n",
    "    globals()['DETECTED_LAKEHOUSE_ROOT'] = lakehouse_root\n",
    "\n",
    "print(f\"\\nüèÅ Diagn√≥stico completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Escritura de tablas Delta por a√±o (robusto, con ABFSS GUID)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Utilidades para resolver ruta ABFSS con GUIDs y evitar FriendlyNameSupportDisabled\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    from notebookutils import mssparkutils  # Disponible en Fabric\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "except Exception:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    mssparkutils = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Overrides manuales opcionales (rellenar si la auto-detecci√≥n falla)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "WORKSPACE_ID_OVERRIDE = None  # p.ej. \"ecf938c4-c449-48de-a07c-1d968a72b3d1\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LAKEHOUSE_ID_OVERRIDE = None  # p.ej. \"12345678-aaaa-bbbb-cccc-1234567890ab\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _get_env_id(getter_names):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if not mssparkutils:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for name in getter_names:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            fn = getattr(mssparkutils.env, name, None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if callable(fn):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                val = fn()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                if val:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    return val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        except Exception:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def resolve_onelake_abfss(rel_path: str) -> str:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"Devuelve ruta ABFSS con GUIDs si es posible, si no, devuelve la original.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ws_id = WORKSPACE_ID_OVERRIDE or _get_env_id([\"getWorkspaceId\", \"getWorkspaceGUID\", \"getWorkspaceGuid\"]) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    lakehouse_id = LAKEHOUSE_ID_OVERRIDE or _get_env_id([\"getLakehouseId\", \"getArtifactId\", \"getItemId\"]) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if ws_id and lakehouse_id and rel_path:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        rel_path = rel_path.replace(\"\\\\\", \"/\").lstrip(\"/\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return f\"abfss://{ws_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/{rel_path}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return rel_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Helpers: sanitizar nombres de columnas para Delta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _sanitize_col_name(name: str) -> str:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # a min√∫sculas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    n = (name or \"\").strip().lower()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # reemplazar caracteres inv√°lidos para Delta: espacios, comas, punto y coma, llaves, par√©ntesis, tabs, saltos, igual y tambi√©n dos puntos\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    n = re.sub(r\"[\\s,;{}()=\\n\\t:]+\", \"_\", n)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # quitar guiones bajos al inicio/fin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    n = n.strip(\"_\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # si empieza por n√∫mero, prefijar\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if n and n[0].isdigit():\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        n = f\"c_{n}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # fallback por si queda vac√≠o\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return n or \"column\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sanitize_columns(cols):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    seen = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    new_cols = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for c in cols:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        base = _sanitize_col_name(c)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        candidate = base\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # asegurar unicidad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        while candidate in seen:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            seen[base] = seen.get(base, 1) + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            candidate = f\"{base}_{seen[base]}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        seen[candidate] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        new_cols.append(candidate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return new_cols\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# A√±os a procesar\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "years = years_to_process if 'years_to_process' in locals() and years_to_process else [2023, 2024, 2025]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Directorio base friendly y ABFSS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "friendly_base = \"Files/bronze/OMIE\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BASE_DIR_ABFSS = resolve_onelake_abfss(friendly_base)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"üìÅ Base (ABFSS): {BASE_DIR_ABFSS}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Lista de resultados para el resumen final\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'tables_created' not in locals():\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    tables_created = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "total_records_processed = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for year in sorted(set(int(y) for y in years)):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pattern = f\"{BASE_DIR_ABFSS}/{year}/*.parquet\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"\\nüì¶ A√±o {year} | Leyendo patr√≥n: {pattern}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Lectura primaria con patr√≥n (ABFSS GUID)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        df_raw = spark.read.parquet(pattern)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"   ‚ö†Ô∏è  Fall√≥ lectura por patr√≥n: {e}\\n   ‚ñ∂Ô∏è  Intentando fallback por lista de archivos...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Fallback: intentar listar archivos y leer de una lista expl√≠cita\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            year_dir = f\"{BASE_DIR_ABFSS}/{year}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            files = [f.path for f in mssparkutils.fs.ls(year_dir) if f.name.endswith('.parquet')] if mssparkutils else []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if not files:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # Segundo fallback: intentar la ruta friendly solo para listar\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                friendly_dir = f\"/{friendly_base}/{year}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                files = [f.path for f in mssparkutils.fs.ls(friendly_dir) if f.name.endswith('.parquet')] if mssparkutils else []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if files:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                print(f\"   ‚ÑπÔ∏è  Cargando {len(files)} archivo(s) expl√≠citos\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                df_raw = spark.read.parquet(files)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                print(\"   ‚ùå No se encontraron archivos para fallback\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        except Exception as e2:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            print(f\"   ‚ùå Fallback por lista de archivos tambi√©n fall√≥: {e2}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Normalizar y sanitizar columnas antes de escribir a Delta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    original_cols = df_raw.columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sanitized_cols = sanitize_columns(original_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if sanitized_cols != original_cols:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"   üî§ Renombrando columnas para compatibilidad Delta:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for oc, nc in zip(original_cols, sanitized_cols):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if oc != nc:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                print(f\"      ‚Ä¢ '{oc}' -> '{nc}'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df = df_raw.toDF(*sanitized_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Validaci√≥n final de nombres: si queda alguno inv√°lido, re-renombrar forzosamente\n",
    "\n",
    "\n",
    "    invalid_pattern = r\"[\\s,;{}()=\\n\\t:]+\"\n",
    "\n",
    "\n",
    "    remaining_invalid = [c for c in df.columns if re.search(invalid_pattern, c)]\n",
    "\n",
    "\n",
    "    if remaining_invalid:\n",
    "\n",
    "\n",
    "        print(f\"   ‚ö†Ô∏è Persisten nombres inv√°lidos: {remaining_invalid} -> aplicando renombrado forzoso\")\n",
    "\n",
    "\n",
    "        current_cols = list(df.columns)\n",
    "\n",
    "\n",
    "        rename_map = {}\n",
    "\n",
    "\n",
    "        used = set()\n",
    "\n",
    "\n",
    "\n",
    "        def _unique(target):\n",
    "\n",
    "\n",
    "            base = target\n",
    "\n",
    "\n",
    "            i = 1\n",
    "\n",
    "\n",
    "            while base in used:\n",
    "\n",
    "\n",
    "                base = f\"{target}_{i}\"\n",
    "\n",
    "\n",
    "                i += 1\n",
    "\n",
    "\n",
    "            used.add(base)\n",
    "\n",
    "\n",
    "            return base\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for oc in current_cols:\n",
    "\n",
    "\n",
    "            nc = _sanitize_col_name(oc)\n",
    "\n",
    "\n",
    "            nc = _unique(nc)\n",
    "\n",
    "\n",
    "            rename_map[oc] = nc\n",
    "\n",
    "\n",
    "        for oc, nc in rename_map.items():\n",
    "\n",
    "\n",
    "            if oc != nc:\n",
    "\n",
    "\n",
    "                df = df.withColumnRenamed(oc, nc)\n",
    "\n",
    "\n",
    "        print(f\"   ‚úÖ Columnas finales: {df.columns}\")\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "\n",
    "        print(f\"   ‚úÖ Columnas finales: {df.columns}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Precio OMIE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if 'marginalpdbc' in df.columns:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        df = df.withColumn('marginal_price_eur_mwh', F.col('marginalpdbc').cast('double'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    elif 'marginal_price_eur_mwh' not in df.columns:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        df = df.withColumn('marginal_price_eur_mwh', F.lit(None).cast('double'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # A√±o y fecha de extracci√≥n\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df = df.withColumn('extraction_year', F.lit(int(year)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if 'extraction_date' in df.columns:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        df = df.withColumn('extraction_date_parsed', F.col('extraction_date').cast('string'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    elif 'extraction_date_parsed' not in df.columns:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        df = df.withColumn('extraction_date_parsed', F.regexp_extract(F.input_file_name(), r'(20\\d{6})', 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Calidad y metadatos\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df = (\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        .withColumn('price_valid', (F.col('marginal_price_eur_mwh').isNotNull()) & (F.col('marginal_price_eur_mwh') >= 0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        .withColumn('price_category', F.when(F.col('marginal_price_eur_mwh') < 50, F.lit('LOW'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                      .when(F.col('marginal_price_eur_mwh') < 150, F.lit('MID'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                      .otherwise(F.lit('HIGH')))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        .withColumn('data_quality_score', F.when(F.col('price_valid'), F.lit(1.0)).otherwise(F.lit(0.5)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        .withColumn('table_created_at', F.current_timestamp())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        .withColumn('source_file', F.input_file_name())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        .withColumn('year', F.col('extraction_year'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    table_name = f\"brz_omie_daily_{year}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"   üíæ Guardando como tabla Delta: {table_name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    (\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        df.write.format('delta')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          .mode('overwrite')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          .option('overwriteSchema', 'true')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          .partitionBy('year')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          .saveAsTable(table_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Contar y acumular\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        recs = spark.table(table_name).count()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        recs = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    total_records_processed += recs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    tables_created.append({\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        'table_name': table_name,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        'year': int(year),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        'records': recs,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        'source_files': pattern,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        'total_files_available': 'N/A',\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        'processing_status': 'Tabla creada'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n‚úÖ Tablas creadas/actualizadas: {len(tables_created)} | Registros totales: {total_records_processed:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd8eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Resumen final y validaci√≥n\n",
    "# -------------------------------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Si no hay tables_created del paso anterior, buscar tablas existentes\n",
    "if 'tables_created' not in locals() or not tables_created:\n",
    "    print(\"üîç Buscando tablas OMIE existentes...\")\n",
    "    \n",
    "    # Buscar tablas que ya existen\n",
    "    try:\n",
    "        existing_tables = spark.sql(\"SHOW TABLES\").collect()\n",
    "        omie_tables = [row.tableName for row in existing_tables if row.tableName.startswith('brz_omie_daily_')]\n",
    "        \n",
    "        if omie_tables:\n",
    "            print(f\"‚úÖ Encontradas {len(omie_tables)} tablas OMIE existentes:\")\n",
    "            for table in sorted(omie_tables):\n",
    "                print(f\"   üìÖ {table}\")\n",
    "            \n",
    "            # Reconstruir la lista tables_created para el resumen\n",
    "            tables_created = []\n",
    "            total_records_processed = 0\n",
    "            \n",
    "            for table_name in omie_tables:\n",
    "                try:\n",
    "                    year = int(table_name.split('_')[-1])  # Extraer a√±o del nombre\n",
    "                    count = spark.table(table_name).count()\n",
    "                    total_records_processed += count\n",
    "                    \n",
    "                    tables_created.append({\n",
    "                        \"table_name\": table_name,\n",
    "                        \"year\": year,\n",
    "                        \"records\": count,\n",
    "                        \"source_files\": \"N/A (tabla existente)\",\n",
    "                        \"total_files_available\": \"N/A (tabla existente)\",\n",
    "                        \"processing_status\": \"Tabla ya exist√≠a\"\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Error accediendo a {table_name}: {e}\")\n",
    "        else:\n",
    "            print(\"‚ùå No se encontraron tablas OMIE existentes\")\n",
    "            tables_created = []\n",
    "            total_records_processed = 0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error buscando tablas existentes: {e}\")\n",
    "        tables_created = []\n",
    "        total_records_processed = 0\n",
    "\n",
    "if tables_created:\n",
    "    print(f\"\\nüìã Resumen de tablas OMIE creadas:\")\n",
    "    \n",
    "    for table_info in sorted(tables_created, key=lambda t: t['year']):\n",
    "        table_name = table_info[\"table_name\"]\n",
    "        year = table_info[\"year\"]\n",
    "        records = table_info[\"records\"]\n",
    "        source_files = table_info[\"source_files\"]\n",
    "        \n",
    "        print(f\"\\nüìÖ {table_name}:\")\n",
    "        print(f\"   üìä Registros: {records:,}\")\n",
    "        print(f\"   üì¶ Archivos origen: {source_files}\")\n",
    "        \n",
    "        # Validaci√≥n r√°pida de la tabla\n",
    "        try:\n",
    "            table_df = spark.table(table_name)\n",
    "            \n",
    "            # Estad√≠sticas b√°sicas\n",
    "            quality_stats = table_df.select(\n",
    "                F.avg(\"data_quality_score\").alias(\"avg_quality\"),\n",
    "                F.count(\"source_file\").alias(\"total_records\"),\n",
    "                F.countDistinct(\"source_file\").alias(\"unique_files\"),\n",
    "                F.min(\"extraction_date_parsed\").alias(\"min_date\"),\n",
    "                F.max(\"extraction_date_parsed\").alias(\"max_date\")\n",
    "            ).collect()[0]\n",
    "            \n",
    "            print(f\"   üéØ Calidad promedio: {quality_stats['avg_quality']:.2f}\")\n",
    "            print(f\"   üìÅ Archivos √∫nicos: {quality_stats['unique_files']}\")\n",
    "            \n",
    "            if quality_stats['min_date'] and quality_stats['max_date']:\n",
    "                print(f\"   üìÖ Rango fechas: {quality_stats['min_date']} a {quality_stats['max_date']}\")\n",
    "            \n",
    "            # Mostrar estad√≠sticas de precios para OMIE\n",
    "            if 'marginal_price_eur_mwh' in [field.name for field in table_df.schema.fields]:\n",
    "                price_stats = table_df.select(\n",
    "                    F.min(\"marginal_price_eur_mwh\").alias(\"min_price\"),\n",
    "                    F.max(\"marginal_price_eur_mwh\").alias(\"max_price\"),\n",
    "                    F.avg(\"marginal_price_eur_mwh\").alias(\"avg_price\"),\n",
    "                    F.countDistinct(\"price_category\").alias(\"price_categories\")\n",
    "                ).collect()[0]\n",
    "                \n",
    "                print(f\"   üí∞ Precio min: {price_stats['min_price']:.2f} ‚Ç¨/MWh\")\n",
    "                print(f\"   üí∞ Precio max: {price_stats['max_price']:.2f} ‚Ç¨/MWh\")\n",
    "                print(f\"   üí∞ Precio promedio: {price_stats['avg_price']:.2f} ‚Ç¨/MWh\")\n",
    "                print(f\"   üìä Categor√≠as de precio: {price_stats['price_categories']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Error en validaci√≥n: {e}\")\n",
    "    \n",
    "    # Crear tabla resumen\n",
    "    print(f\"\\nüìä Estad√≠sticas consolidadas:\")\n",
    "    print(f\"   üìÅ Total tablas: {len(tables_created)}\")\n",
    "    print(f\"   üìÖ A√±os cubiertos: {sorted([t['year'] for t in tables_created])}\")\n",
    "    print(f\"   üìà Total registros: {total_records_processed:,}\")\n",
    "    \n",
    "    # Verificar que todas las tablas son accesibles\n",
    "    print(f\"\\nüîç Verificaci√≥n de acceso a tablas:\")\n",
    "    for table_info in tables_created:\n",
    "        try:\n",
    "            spark.sql(f\"SELECT COUNT(*) FROM {table_info['table_name']}\").collect()\n",
    "            print(f\"   ‚úÖ {table_info['table_name']}: Accesible\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå {table_info['table_name']}: Error - {e}\")\n",
    "    \n",
    "    # Mostrar muestra de datos de la primera tabla\n",
    "    if tables_created:\n",
    "        sample_table = sorted(tables_created, key=lambda t: t['year'])[0][\"table_name\"]\n",
    "        print(f\"\\nüîç Muestra de datos de {sample_table}:\")\n",
    "        try:\n",
    "            spark.table(sample_table).select(\n",
    "                \"source_file\", \"extraction_year\", \"extraction_date_parsed\", \n",
    "                \"marginal_price_eur_mwh\", \"price_category\", \"price_valid\",\n",
    "                \"data_quality_score\", \"table_created_at\"\n",
    "            ).show(5, truncate=False)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Error mostrando muestra: {e}\")\n",
    "            # Fallback con columnas b√°sicas\n",
    "            try:\n",
    "                spark.table(sample_table).show(3, truncate=False)\n",
    "            except Exception as e2:\n",
    "                print(f\"   ‚ùå Error con fallback: {e2}\")\n",
    "    \n",
    "    result_message = f\"A√±os procesados: {[t['year'] for t in tables_created]}\"\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No se crearon tablas\")\n",
    "    result_message = \"Sin tablas creadas\"\n",
    "\n",
    "print(f\"\\nüèÅ Finalizando notebook...\")\n",
    "print(f\"üìù Resultado: {result_message}\")\n",
    "\n",
    "# Salida del notebook para pipeline\n",
    "try:\n",
    "    dbutils.notebook.exit(result_message)\n",
    "except:\n",
    "    print(f\"‚úÖ Ejecuci√≥n local completada: {result_message}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
