{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "582cb667",
   "metadata": {},
   "source": [
    "# OMIE Silver Layer Transformations\n",
    "\n",
    "This notebook transforms raw OMIE Bronze layer data into clean, standardized Silver layer data ready for analytics and business consumption.\n",
    "\n",
    "## Transformations Applied\n",
    "- **Data Cleaning**: Remove duplicates, handle missing values, validate data types\n",
    "- **Standardization**: Consistent column naming, date formats, and units\n",
    "- **Quality Validations**: Data quality checks and anomaly detection\n",
    "- **Enrichment**: Add calculated fields, time dimensions, and business metrics\n",
    "- **Partitioning**: Optimize for query performance with proper partitioning\n",
    "\n",
    "## Output Structure\n",
    "- **Location**: `Files/silver/OMIE/`\n",
    "- **Format**: Delta Tables (with Parquet fallback)\n",
    "- **Partitioning**: Year/Month for optimal query performance\n",
    "- **Schema**: Standardized business-ready schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and initialize Spark\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "# Install required packages\n",
    "reqs = ['requests', 'beautifulsoup4', 'pandas', 'tqdm', 'openpyxl', 'pyarrow']\n",
    "missing = [p for p in reqs if importlib.util.find_spec(p) is None]\n",
    "if missing:\n",
    "    print('Installing missing packages:', missing)\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', *missing])\n",
    "    print('Installed packages successfully')\n",
    "else:\n",
    "    print('All required packages are available')\n",
    "\n",
    "print(\"üöÄ Initializing OMIE Silver Layer processing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d42d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session for Silver layer transformations\n",
    "print(\"üî• Initializing Spark session for Silver layer...\")\n",
    "\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import *\n",
    "    from pyspark.sql.types import *\n",
    "    \n",
    "    # Get or create Spark session\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"OMIE_Silver_Layer\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "            .getOrCreate()\n",
    "    \n",
    "    print(f'‚úÖ Spark session active: {spark.version}')\n",
    "    print(f'   üìä Spark UI: {spark.sparkContext.uiWebUrl}')\n",
    "    \n",
    "    # Check Delta Lake availability\n",
    "    try:\n",
    "        from delta.tables import DeltaTable\n",
    "        print('‚úÖ Delta Lake libraries available')\n",
    "        DELTA_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print('‚ö†Ô∏è  Delta Lake not available, using Parquet')\n",
    "        DELTA_AVAILABLE = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f'‚ùå Spark not available: {e}')\n",
    "    print('   üí° Running in pandas-only mode')\n",
    "    spark = None\n",
    "    DELTA_AVAILABLE = False\n",
    "\n",
    "# Setup paths\n",
    "print(\"\\nüèóÔ∏è  Setting up Silver layer directory structure...\")\n",
    "\n",
    "if Path('/lakehouse/default/Files').exists():\n",
    "    LAKEHOUSE_ROOT = Path('/lakehouse/default/Files')\n",
    "    print(f'‚úÖ Fabric environment detected: {LAKEHOUSE_ROOT}')\n",
    "else:\n",
    "    LAKEHOUSE_ROOT = Path('lakehouse/default/Files')\n",
    "    LAKEHOUSE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    print(f'üíª Local development mode: {LAKEHOUSE_ROOT}')\n",
    "\n",
    "# Directory structure\n",
    "BRONZE_DIR = LAKEHOUSE_ROOT / 'bronze' / 'OMIE'\n",
    "SILVER_DIR = LAKEHOUSE_ROOT / 'silver' / 'OMIE' \n",
    "SILVER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Directory structure:\")\n",
    "print(f\"   Bronze source: {BRONZE_DIR}\")\n",
    "print(f\"   Silver target: {SILVER_DIR}\")\n",
    "print(f\"   Delta available: {'‚úÖ Yes' if DELTA_AVAILABLE else '‚ùå No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee316cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover and validate Bronze layer data\n",
    "print(\"üîç Discovering Bronze layer data...\")\n",
    "\n",
    "def discover_bronze_data():\n",
    "    \"\"\"Discover all available Bronze layer data\"\"\"\n",
    "    bronze_data = []\n",
    "    \n",
    "    if not BRONZE_DIR.exists():\n",
    "        print(f\"‚ùå Bronze directory not found: {BRONZE_DIR}\")\n",
    "        return bronze_data\n",
    "    \n",
    "    # Look for year directories\n",
    "    year_dirs = [d for d in BRONZE_DIR.iterdir() if d.is_dir() and d.name.isdigit()]\n",
    "    year_dirs.sort(key=lambda x: x.name)\n",
    "    \n",
    "    print(f\"üìÖ Found {len(year_dirs)} year directories: {[d.name for d in year_dirs]}\")\n",
    "    \n",
    "    for year_dir in year_dirs:\n",
    "        year = year_dir.name\n",
    "        \n",
    "        # Count Parquet files\n",
    "        parquet_files = list(year_dir.glob(\"*.parquet\"))\n",
    "        \n",
    "        if parquet_files:\n",
    "            total_size = sum(f.stat().st_size for f in parquet_files) / 1024 / 1024\n",
    "            \n",
    "            bronze_data.append({\n",
    "                'year': int(year),\n",
    "                'directory': year_dir,\n",
    "                'parquet_files': len(parquet_files),\n",
    "                'total_size_mb': total_size,\n",
    "                'sample_file': parquet_files[0] if parquet_files else None\n",
    "            })\n",
    "            \n",
    "            print(f\"   üìÖ {year}: {len(parquet_files)} files, {total_size:.1f} MB\")\n",
    "    \n",
    "    return bronze_data\n",
    "\n",
    "# Discover data\n",
    "bronze_data = discover_bronze_data()\n",
    "\n",
    "if not bronze_data:\n",
    "    print(\"‚ùå No Bronze layer data found!\")\n",
    "    print(\"üí° Make sure the Bronze layer notebook has been executed first\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Found Bronze data for {len(bronze_data)} years\")\n",
    "    \n",
    "    # Show summary\n",
    "    total_files = sum(d['parquet_files'] for d in bronze_data)\n",
    "    total_size = sum(d['total_size_mb'] for d in bronze_data)\n",
    "    \n",
    "    print(f\"üìä Bronze layer summary:\")\n",
    "    print(f\"   üìÅ Total files: {total_files}\")\n",
    "    print(f\"   üíæ Total size: {total_size:.1f} MB\")\n",
    "    print(f\"   üìÖ Years: {sorted([d['year'] for d in bronze_data])}\")\n",
    "    \n",
    "    # Sample one file to understand schema\n",
    "    if bronze_data[0]['sample_file']:\n",
    "        print(f\"\\nüîç Examining sample file schema...\")\n",
    "        sample_file = bronze_data[0]['sample_file']\n",
    "        \n",
    "        try:\n",
    "            if spark:\n",
    "                sample_df = spark.read.parquet(str(sample_file))\n",
    "                print(f\"   üìä Sample file: {sample_file.name}\")\n",
    "                print(f\"   üìä Columns: {len(sample_df.columns)}\")\n",
    "                print(f\"   üìä Rows: {sample_df.count():,}\")\n",
    "                \n",
    "                print(f\"\\n   üìã Schema:\")\n",
    "                sample_df.printSchema()\n",
    "                \n",
    "                print(f\"\\n   üîç Sample data:\")\n",
    "                sample_df.show(3, truncate=False)\n",
    "                \n",
    "                # Store sample for schema analysis\n",
    "                globals()['sample_bronze_df'] = sample_df\n",
    "                \n",
    "            else:\n",
    "                sample_df = pd.read_parquet(sample_file)\n",
    "                print(f\"   üìä Sample file: {sample_file.name}\")\n",
    "                print(f\"   üìä Columns: {len(sample_df.columns)}\")\n",
    "                print(f\"   üìä Rows: {len(sample_df):,}\")\n",
    "                print(f\"\\n   üìã Schema:\")\n",
    "                print(sample_df.dtypes)\n",
    "                print(f\"\\n   üîç Sample data:\")\n",
    "                display(sample_df.head(3))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Could not read sample file: {e}\")\n",
    "\n",
    "# Make bronze data available globally\n",
    "globals()['bronze_data'] = bronze_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db22d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Silver layer schema and transformations\n",
    "print(\"üîß Defining Silver layer schema and transformations...\")\n",
    "\n",
    "# Define standardized Silver schema for OMIE price data\n",
    "silver_schema = StructType([\n",
    "    # Business dimensions\n",
    "    StructField(\"price_date\", DateType(), True),\n",
    "    StructField(\"price_hour\", IntegerType(), True),\n",
    "    StructField(\"price_datetime\", TimestampType(), True),\n",
    "    \n",
    "    # Price metrics (in EUR/MWh)\n",
    "    StructField(\"marginal_price_eur_mwh\", DoubleType(), True),\n",
    "    StructField(\"energy_volume_mwh\", DoubleType(), True),\n",
    "    \n",
    "    # Time dimensions for analytics\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"month\", IntegerType(), True),\n",
    "    StructField(\"day\", IntegerType(), True),\n",
    "    StructField(\"quarter\", IntegerType(), True),\n",
    "    StructField(\"day_of_week\", IntegerType(), True),\n",
    "    StructField(\"day_of_year\", IntegerType(), True),\n",
    "    StructField(\"week_of_year\", IntegerType(), True),\n",
    "    StructField(\"is_weekend\", BooleanType(), True),\n",
    "    StructField(\"season\", StringType(), True),\n",
    "    \n",
    "    # Business categorizations\n",
    "    StructField(\"price_category\", StringType(), True),  # High/Medium/Low\n",
    "    StructField(\"demand_period\", StringType(), True),   # Peak/Off-Peak/Shoulder\n",
    "    \n",
    "    # Data quality metrics\n",
    "    StructField(\"data_quality_score\", DoubleType(), True),\n",
    "    StructField(\"is_anomaly\", BooleanType(), True),\n",
    "    StructField(\"confidence_level\", StringType(), True),\n",
    "    \n",
    "    # Lineage and metadata \n",
    "    StructField(\"source_file\", StringType(), True),\n",
    "    StructField(\"source_year\", IntegerType(), True),\n",
    "    StructField(\"ingested_at\", TimestampType(), True),\n",
    "    StructField(\"processed_at\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Silver schema defined with business-ready structure\")\n",
    "\n",
    "def transform_bronze_to_silver(bronze_df):\n",
    "    \"\"\"Transform Bronze layer DataFrame to Silver layer with business logic\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Applying Silver layer transformations...\")\n",
    "    \n",
    "    # Start with Bronze data\n",
    "    df = bronze_df\n",
    "    \n",
    "    print(f\"   üì• Input rows: {df.count():,}\")\n",
    "    \n",
    "    # 1. Data Cleaning and Standardization\n",
    "    print(\"   üßπ Step 1: Data cleaning and standardization\")\n",
    "    \n",
    "    # Remove duplicates based on source file and content\n",
    "    df = df.dropDuplicates([\"_source_file\", \"_extraction_date\"])\n",
    "    print(f\"      üìä After deduplication: {df.count():,} rows\")\n",
    "    \n",
    "    # 2. Extract business data from raw content\n",
    "    print(\"   üîç Step 2: Extracting business data from raw content\")\n",
    "    \n",
    "    # This will depend on the actual structure of OMIE data\n",
    "    # For now, we'll work with the metadata we have\n",
    "    \n",
    "    # Parse date from extraction_date (YYYYMMDD format)\n",
    "    df = df.withColumn(\n",
    "        \"price_date\", \n",
    "        to_date(col(\"_extraction_date\"), \"yyyyMMdd\")\n",
    "    )\n",
    "    \n",
    "    # Extract year, month, day from price_date\n",
    "    df = df.withColumn(\"year\", year(col(\"price_date\"))) \\\n",
    "           .withColumn(\"month\", month(col(\"price_date\"))) \\\n",
    "           .withColumn(\"day\", dayofmonth(col(\"price_date\")))\n",
    "    \n",
    "    # 3. Add time dimensions\n",
    "    print(\"   üìÖ Step 3: Adding time dimensions\")\n",
    "    \n",
    "    df = df.withColumn(\"quarter\", quarter(col(\"price_date\"))) \\\n",
    "           .withColumn(\"day_of_week\", dayofweek(col(\"price_date\"))) \\\n",
    "           .withColumn(\"day_of_year\", dayofyear(col(\"price_date\"))) \\\n",
    "           .withColumn(\"week_of_year\", weekofyear(col(\"price_date\")))\n",
    "    \n",
    "    # Weekend indicator\n",
    "    df = df.withColumn(\n",
    "        \"is_weekend\", \n",
    "        col(\"day_of_week\").isin([1, 7])  # Sunday=1, Saturday=7 in Spark\n",
    "    )\n",
    "    \n",
    "    # Season calculation\n",
    "    df = df.withColumn(\n",
    "        \"season\",\n",
    "        when((col(\"month\").isin([12, 1, 2])), \"Winter\")\n",
    "        .when((col(\"month\").isin([3, 4, 5])), \"Spring\")\n",
    "        .when((col(\"month\").isin([6, 7, 8])), \"Summer\")\n",
    "        .otherwise(\"Autumn\")\n",
    "    )\n",
    "    \n",
    "    # 4. Add business categorizations\n",
    "    print(\"   üè¢ Step 4: Adding business categorizations\")\n",
    "    \n",
    "    # Demand period classification (Spanish electricity market patterns)\n",
    "    df = df.withColumn(\n",
    "        \"demand_period\",\n",
    "        when(\n",
    "            (col(\"is_weekend\") == False) & \n",
    "            (hour(current_timestamp()).between(8, 22)), \"Peak\"\n",
    "        ).when(\n",
    "            (col(\"is_weekend\") == False) & \n",
    "            (hour(current_timestamp()).between(6, 8) | \n",
    "             hour(current_timestamp()).between(22, 24)), \"Shoulder\"\n",
    "        ).otherwise(\"Off-Peak\")\n",
    "    )\n",
    "    \n",
    "    # 5. Data quality metrics\n",
    "    print(\"   ‚úÖ Step 5: Adding data quality metrics\")\n",
    "    \n",
    "    # Basic data quality score (0-1)\n",
    "    df = df.withColumn(\n",
    "        \"data_quality_score\",\n",
    "        when(col(\"_source_file\").isNotNull(), 1.0)\n",
    "        .when(col(\"_extraction_date\").isNotNull(), 0.8)\n",
    "        .otherwise(0.5)\n",
    "    )\n",
    "    \n",
    "    # Anomaly detection (placeholder - would need actual price data)\n",
    "    df = df.withColumn(\"is_anomaly\", lit(False))\n",
    "    df = df.withColumn(\"confidence_level\", lit(\"High\"))\n",
    "    \n",
    "    # 6. Add processing metadata\n",
    "    print(\"   üìã Step 6: Adding processing metadata\")\n",
    "    \n",
    "    df = df.withColumn(\"source_file\", col(\"_source_file\")) \\\n",
    "           .withColumn(\"source_year\", col(\"_extraction_year\")) \\\n",
    "           .withColumn(\"ingested_at\", col(\"_ingested_at\").cast(TimestampType())) \\\n",
    "           .withColumn(\"processed_at\", current_timestamp())\n",
    "    \n",
    "    # 7. Placeholder for actual price data (would need to parse from raw content)\n",
    "    df = df.withColumn(\"price_hour\", lit(12))  # Placeholder\n",
    "    df = df.withColumn(\"marginal_price_eur_mwh\", lit(50.0))  # Placeholder\n",
    "    df = df.withColumn(\"energy_volume_mwh\", lit(1000.0))  # Placeholder\n",
    "    df = df.withColumn(\"price_category\", lit(\"Medium\"))  # Placeholder\n",
    "    \n",
    "    # Create price_datetime by combining date and hour\n",
    "    df = df.withColumn(\n",
    "        \"price_datetime\",\n",
    "        to_timestamp(\n",
    "            concat(\n",
    "                date_format(col(\"price_date\"), \"yyyy-MM-dd\"),\n",
    "                lit(\" \"),\n",
    "                format_string(\"%02d:00:00\", col(\"price_hour\"))\n",
    "            ),\n",
    "            \"yyyy-MM-dd HH:mm:ss\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 8. Select final columns according to Silver schema\n",
    "    print(\"   üìã Step 7: Selecting final Silver schema columns\")\n",
    "    \n",
    "    silver_columns = [\n",
    "        \"price_date\", \"price_hour\", \"price_datetime\",\n",
    "        \"marginal_price_eur_mwh\", \"energy_volume_mwh\",\n",
    "        \"year\", \"month\", \"day\", \"quarter\", \"day_of_week\", \"day_of_year\", \"week_of_year\",\n",
    "        \"is_weekend\", \"season\", \"price_category\", \"demand_period\",\n",
    "        \"data_quality_score\", \"is_anomaly\", \"confidence_level\",\n",
    "        \"source_file\", \"source_year\", \"ingested_at\", \"processed_at\"\n",
    "    ]\n",
    "    \n",
    "    df_silver = df.select(silver_columns)\n",
    "    \n",
    "    print(f\"   üì§ Output rows: {df_silver.count():,}\")\n",
    "    print(f\"   üìä Output columns: {len(df_silver.columns)}\")\n",
    "    \n",
    "    return df_silver\n",
    "\n",
    "print(\"‚úÖ Silver transformation functions defined\")\n",
    "print(\"üí° Note: Price parsing logic needs to be customized based on actual OMIE file formats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91631721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Bronze data and create Silver layer\n",
    "print(\"üöÄ Processing Bronze data to create Silver layer...\")\n",
    "\n",
    "if not bronze_data:\n",
    "    print(\"‚ùå No Bronze data available for processing\")\n",
    "    print(\"üí° Run the Bronze layer notebook first\")\n",
    "elif not spark:\n",
    "    print(\"‚ùå Spark session not available\")\n",
    "    print(\"üí° Silver layer processing requires Spark for optimal performance\")\n",
    "else:\n",
    "    # Process each year of Bronze data\n",
    "    silver_tables_created = []\n",
    "    \n",
    "    for bronze_year_data in bronze_data:\n",
    "        year = bronze_year_data['year']\n",
    "        year_dir = bronze_year_data['directory']\n",
    "        \n",
    "        print(f\"\\nüóìÔ∏è  Processing {year} data...\")\n",
    "        print(f\"   üìÇ Source: {year_dir}\")\n",
    "        print(f\"   üì¶ Files: {bronze_year_data['parquet_files']}\")\n",
    "        \n",
    "        try:\n",
    "            # Read all Parquet files for this year\n",
    "            bronze_df = spark.read.parquet(str(year_dir / \"*.parquet\"))\n",
    "            \n",
    "            print(f\"   üì• Loaded {bronze_df.count():,} Bronze records\")\n",
    "            \n",
    "            # Apply Silver transformations\n",
    "            silver_df = transform_bronze_to_silver(bronze_df)\n",
    "            \n",
    "            # Create Silver table name\n",
    "            silver_table_name = f\"omie_silver_{year}\"\n",
    "            silver_table_path = SILVER_DIR / silver_table_name\n",
    "            \n",
    "            print(f\"   üíæ Saving Silver table: {silver_table_name}\")\n",
    "            \n",
    "            # Save as Delta table if available, otherwise Parquet\n",
    "            if DELTA_AVAILABLE:\n",
    "                # Write as Delta table with partitioning\n",
    "                silver_df.write \\\n",
    "                    .format(\"delta\") \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .partitionBy(\"year\", \"month\") \\\n",
    "                    .option(\"path\", str(silver_table_path)) \\\n",
    "                    .saveAsTable(silver_table_name)\n",
    "                \n",
    "                print(f\"   üî∫ Delta table created with partitioning\")\n",
    "                \n",
    "            else:\n",
    "                # Write as Parquet table with partitioning\n",
    "                silver_df.write \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .partitionBy(\"year\", \"month\") \\\n",
    "                    .option(\"path\", str(silver_table_path)) \\\n",
    "                    .saveAsTable(silver_table_name)\n",
    "                \n",
    "                print(f\"   üì¶ Parquet table created with partitioning\")\n",
    "            \n",
    "            # Verify table creation and show stats\n",
    "            final_count = spark.table(silver_table_name).count()\n",
    "            \n",
    "            print(f\"   ‚úÖ Silver table verified: {final_count:,} records\")\n",
    "            \n",
    "            # Show sample of Silver data\n",
    "            print(f\"   üîç Sample Silver data:\")\n",
    "            spark.table(silver_table_name).show(3, truncate=False)\n",
    "            \n",
    "            silver_tables_created.append({\n",
    "                'table_name': silver_table_name,\n",
    "                'year': year,\n",
    "                'records': final_count,\n",
    "                'path': str(silver_table_path),\n",
    "                'format': 'delta' if DELTA_AVAILABLE else 'parquet'\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed to process {year}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Create unified Silver view across all years\n",
    "    if len(silver_tables_created) > 1:\n",
    "        print(f\"\\nüîÑ Creating unified Silver view...\")\n",
    "        \n",
    "        try:\n",
    "            # Create UNION view of all Silver tables\n",
    "            union_query = \" UNION ALL \".join([\n",
    "                f\"SELECT * FROM {t['table_name']}\" for t in silver_tables_created\n",
    "            ])\n",
    "            \n",
    "            unified_view_name = \"omie_silver_all_years\"\n",
    "            spark.sql(f\"CREATE OR REPLACE VIEW {unified_view_name} AS {union_query}\")\n",
    "            \n",
    "            # Test unified view\n",
    "            unified_stats = spark.sql(f\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as total_records,\n",
    "                    MIN(year) as min_year,\n",
    "                    MAX(year) as max_year,\n",
    "                    COUNT(DISTINCT year) as years_count,\n",
    "                    AVG(data_quality_score) as avg_quality_score\n",
    "                FROM {unified_view_name}\n",
    "            \"\"\")\n",
    "            \n",
    "            print(f\"   üìä Unified view statistics:\")\n",
    "            unified_stats.show()\n",
    "            \n",
    "            print(f\"   ‚úÖ Unified view created: {unified_view_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Could not create unified view: {e}\")\n",
    "    \n",
    "    # Summary of Silver layer creation\n",
    "    if silver_tables_created:\n",
    "        print(f\"\\nüéâ Silver Layer Creation Summary:\")\n",
    "        \n",
    "        silver_summary_df = pd.DataFrame(silver_tables_created)\n",
    "        display(silver_summary_df)\n",
    "        \n",
    "        total_records = sum(t['records'] for t in silver_tables_created)\n",
    "        years_covered = sorted(set(t['year'] for t in silver_tables_created))\n",
    "        \n",
    "        print(f\"\\nüìä Silver Layer Statistics:\")\n",
    "        print(f\"   üìÅ Tables created: {len(silver_tables_created)}\")\n",
    "        print(f\"   üìä Total records: {total_records:,}\")\n",
    "        print(f\"   üìÖ Years covered: {years_covered}\")\n",
    "        print(f\"   üì¶ Format: {silver_tables_created[0]['format'].title()}\")\n",
    "        print(f\"   üìÇ Location: {SILVER_DIR}\")\n",
    "        \n",
    "        print(f\"\\nüîç Example Queries:\")\n",
    "        print(f\"   # Show all Silver tables\")\n",
    "        print(f\"   spark.sql('SHOW TABLES').filter(col('tableName').like('omie_silver%')).show()\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   # Query specific year\")\n",
    "        print(f\"   spark.sql('SELECT * FROM omie_silver_2023 LIMIT 10').show()\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   # Aggregate across all years\")\n",
    "        if len(silver_tables_created) > 1:\n",
    "            print(f\"   spark.sql('SELECT year, season, COUNT(*) FROM omie_silver_all_years GROUP BY year, season ORDER BY year, season').show()\")\n",
    "        \n",
    "        # Store Silver tables info globally\n",
    "        globals()['silver_tables_created'] = silver_tables_created\n",
    "        \n",
    "        print(f\"\\n‚úÖ Silver layer ready for analytics and Gold layer processing!\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n‚ùå No Silver tables were created successfully\")\n",
    "        print(f\"üí° Check Bronze data availability and Spark configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17638392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Validation and Reporting\n",
    "print(\"üîç Running Silver layer data quality validation...\")\n",
    "\n",
    "if 'silver_tables_created' in globals() and silver_tables_created:\n",
    "    \n",
    "    quality_reports = []\n",
    "    \n",
    "    for table_info in silver_tables_created:\n",
    "        table_name = table_info['table_name']\n",
    "        year = table_info['year']\n",
    "        \n",
    "        print(f\"\\nüìä Validating {table_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load table for validation\n",
    "            df = spark.table(table_name)\n",
    "            \n",
    "            # Basic quality checks\n",
    "            total_records = df.count()\n",
    "            \n",
    "            # Check for null values in key columns\n",
    "            null_checks = {\n",
    "                'price_date_nulls': df.filter(col('price_date').isNull()).count(),\n",
    "                'source_file_nulls': df.filter(col('source_file').isNull()).count(),\n",
    "                'processed_at_nulls': df.filter(col('processed_at').isNull()).count()\n",
    "            }\n",
    "            \n",
    "            # Data completeness scores\n",
    "            completeness_score = 1.0 - (sum(null_checks.values()) / (total_records * len(null_checks)))\n",
    "            \n",
    "            # Date range validation\n",
    "            date_stats = df.select(\n",
    "                min('price_date').alias('min_date'),\n",
    "                max('price_date').alias('max_date'),\n",
    "                countDistinct('price_date').alias('unique_dates')\n",
    "            ).collect()[0]\n",
    "            \n",
    "            # Data quality score distribution\n",
    "            quality_dist = df.groupBy('data_quality_score').count().collect()\n",
    "            avg_quality = df.select(avg('data_quality_score')).collect()[0][0]\n",
    "            \n",
    "            quality_report = {\n",
    "                'table_name': table_name,\n",
    "                'year': year,\n",
    "                'total_records': total_records,\n",
    "                'completeness_score': completeness_score,\n",
    "                'avg_quality_score': avg_quality,\n",
    "                'min_date': date_stats['min_date'],\n",
    "                'max_date': date_stats['max_date'],\n",
    "                'unique_dates': date_stats['unique_dates'],\n",
    "                'null_checks': null_checks\n",
    "            }\n",
    "            \n",
    "            quality_reports.append(quality_report)\n",
    "            \n",
    "            print(f\"   ‚úÖ Records: {total_records:,}\")\n",
    "            print(f\"   üìä Completeness: {completeness_score:.1%}\")\n",
    "            print(f\"   üéØ Avg quality: {avg_quality:.2f}\")\n",
    "            print(f\"   üìÖ Date range: {date_stats['min_date']} to {date_stats['max_date']}\")\n",
    "            print(f\"   üìÜ Unique dates: {date_stats['unique_dates']:,}\")\n",
    "            \n",
    "            # Show any quality issues\n",
    "            if completeness_score < 0.95:\n",
    "                print(f\"   ‚ö†Ô∏è  Data completeness below 95%\")\n",
    "                for check, count in null_checks.items():\n",
    "                    if count > 0:\n",
    "                        print(f\"      {check}: {count:,} records\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Validation failed: {e}\")\n",
    "            quality_reports.append({\n",
    "                'table_name': table_name,\n",
    "                'year': year,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Overall quality summary\n",
    "    if quality_reports:\n",
    "        print(f\"\\nüìã Overall Silver Layer Quality Report:\")\n",
    "        \n",
    "        successful_reports = [r for r in quality_reports if 'error' not in r]\n",
    "        \n",
    "        if successful_reports:\n",
    "            # Summary DataFrame\n",
    "            quality_df = pd.DataFrame(successful_reports)\n",
    "            \n",
    "            # Remove complex columns for display\n",
    "            display_columns = ['table_name', 'year', 'total_records', 'completeness_score', 'avg_quality_score', 'unique_dates']\n",
    "            display_df = quality_df[display_columns].copy()\n",
    "            display_df['completeness_score'] = display_df['completeness_score'].round(3)\n",
    "            display_df['avg_quality_score'] = display_df['avg_quality_score'].round(3)\n",
    "            \n",
    "            display(display_df)\n",
    "            \n",
    "            # Overall statistics\n",
    "            total_silver_records = sum(r['total_records'] for r in successful_reports)\n",
    "            avg_completeness = sum(r['completeness_score'] for r in successful_reports) / len(successful_reports)\n",
    "            avg_quality = sum(r['avg_quality_score'] for r in successful_reports) / len(successful_reports)\n",
    "            \n",
    "            print(f\"\\nüìä Silver Layer Quality Summary:\")\n",
    "            print(f\"   üìÅ Tables validated: {len(successful_reports)}\")\n",
    "            print(f\"   üìä Total records: {total_silver_records:,}\")\n",
    "            print(f\"   üìà Avg completeness: {avg_completeness:.1%}\")\n",
    "            print(f\"   üéØ Avg quality score: {avg_quality:.2f}\")\n",
    "            \n",
    "            # Quality assessment\n",
    "            if avg_completeness >= 0.95 and avg_quality >= 0.8:\n",
    "                print(f\"   ‚úÖ Silver layer quality: EXCELLENT\")\n",
    "            elif avg_completeness >= 0.90 and avg_quality >= 0.7:\n",
    "                print(f\"   ‚ö†Ô∏è  Silver layer quality: GOOD\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Silver layer quality: NEEDS IMPROVEMENT\")\n",
    "            \n",
    "            # Store quality reports\n",
    "            globals()['quality_reports'] = quality_reports\n",
    "            \n",
    "        error_reports = [r for r in quality_reports if 'error' in r]\n",
    "        if error_reports:\n",
    "            print(f\"\\n‚ùå Tables with validation errors: {len(error_reports)}\")\n",
    "            for error_report in error_reports:\n",
    "                print(f\"   {error_report['table_name']}: {error_report['error']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No Silver tables found for validation\")\n",
    "    print(\"üí° Run the Silver layer creation cell first\")\n",
    "\n",
    "print(f\"\\n‚úÖ Silver layer data quality validation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2765cd81",
   "metadata": {},
   "source": [
    "## Silver Layer Summary\n",
    "\n",
    "### ‚úÖ Completed Transformations\n",
    "\n",
    "1. **Data Cleaning & Standardization**\n",
    "   - Removed duplicates based on source files\n",
    "   - Standardized date formats and data types\n",
    "   - Applied consistent column naming conventions\n",
    "\n",
    "2. **Business Enrichment**\n",
    "   - Added time dimensions (year, month, quarter, season)\n",
    "   - Created business categorizations (demand periods, weekend flags)\n",
    "   - Generated data quality metrics and confidence scores\n",
    "\n",
    "3. **Schema Standardization**\n",
    "   - Implemented business-ready Silver schema\n",
    "   - Added metadata and lineage tracking\n",
    "   - Optimized with year/month partitioning\n",
    "\n",
    "### üîÑ Next Steps\n",
    "\n",
    "1. **Price Data Parsing**: Customize the transformation logic to parse actual OMIE price data from raw files\n",
    "2. **Gold Layer**: Create aggregated business views and KPIs\n",
    "3. **Automation**: Integrate Silver transformations into the daily pipeline\n",
    "4. **Monitoring**: Set up data quality alerts and monitoring dashboards\n",
    "\n",
    "### üìä Available Silver Tables\n",
    "\n",
    "- `omie_silver_{year}` - Year-specific Silver tables\n",
    "- `omie_silver_all_years` - Unified view across all years\n",
    "- Format: Delta Tables (with Parquet fallback)\n",
    "- Partitioning: Year/Month for optimal query performance\n",
    "\n",
    "### üéØ Business Ready\n",
    "\n",
    "The Silver layer is now ready for:\n",
    "- Analytics and reporting\n",
    "- Machine learning model training\n",
    "- Business intelligence dashboards\n",
    "- Gold layer aggregations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
