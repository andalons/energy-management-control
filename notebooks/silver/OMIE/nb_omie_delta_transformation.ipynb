{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb97990",
   "metadata": {},
   "source": [
    "# OMIE Delta Transformation (Bronze to Silver)\n",
    "\n",
    "Transforms OMIE Bronze Delta tables into clean Silver layer tables.\n",
    "\n",
    "**Input:** Delta tables `brz_OMIE_2023`, `brz_OMIE_2024`, `brz_OMIE_2025`  \n",
    "**Output:** Silver tables `slv_OMIE_2023`, `slv_OMIE_2024`, `slv_OMIE_2025`\n",
    "\n",
    "Following the same pattern as REDATA colleagues with nested JSON processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7dd5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from notebookutils import mssparkutils\n",
    "import json\n",
    "\n",
    "# Parámetros configurables\n",
    "bronze_table_prefix = \"brz_OMIE\"\n",
    "silver_table_prefix = \"slv_OMIE\"\n",
    "years_param = [2023, 2024, 2025]\n",
    "\n",
    "# Permitir override desde variables globales/locales\n",
    "if 'years' in locals() or 'years' in globals():\n",
    "    years_param = years if 'years' in locals() else globals().get('years', years_param)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"OMIE_Bronze_to_Silver\").getOrCreate()\n",
    "\n",
    "print(\"🚀 OMIE Delta Transformation Started\")\n",
    "print(f\"✅ Spark session: {spark.version}\")\n",
    "print(f\"🎯 Años a procesar: {years_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa299c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_bronze_tables():\n",
    "    \"\"\"Lista todas las tablas Bronze OMIE disponibles\"\"\"\n",
    "    try:\n",
    "        all_tables = spark.sql(\"SHOW TABLES\").collect()\n",
    "        bronze_tables = [\n",
    "            row.tableName for row in all_tables \n",
    "            if row.tableName.startswith(bronze_table_prefix)\n",
    "        ]\n",
    "        return bronze_tables\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error listando tablas: {e}\")\n",
    "        return []\n",
    "\n",
    "def validate_bronze_table(table_name):\n",
    "    \"\"\"Valida que una tabla Bronze existe y tiene datos\"\"\"\n",
    "    try:\n",
    "        count = spark.table(table_name).count()\n",
    "        return count > 0, count\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error validando {table_name}: {e}\")\n",
    "        return False, 0\n",
    "\n",
    "# Descubrir tablas Bronze disponibles\n",
    "available_bronze_tables = list_bronze_tables()\n",
    "print(f\"📋 Tablas Bronze encontradas: {available_bronze_tables}\")\n",
    "\n",
    "# Filtrar por años solicitados\n",
    "tables_to_process = []\n",
    "for year in years_param:\n",
    "    table_name = f\"{bronze_table_prefix}_{year}\"\n",
    "    if table_name in available_bronze_tables:\n",
    "        is_valid, record_count = validate_bronze_table(table_name)\n",
    "        if is_valid:\n",
    "            tables_to_process.append((table_name, year, record_count))\n",
    "            print(f\"✅ {table_name}: {record_count:,} registros\")\n",
    "        else:\n",
    "            print(f\"⚠️  {table_name}: Sin datos válidos\")\n",
    "    else:\n",
    "        print(f\"❌ {table_name}: No encontrada\")\n",
    "\n",
    "if not tables_to_process:\n",
    "    print(\"❌ No hay tablas Bronze válidas para procesar\")\n",
    "    try:\n",
    "        dbutils.notebook.exit(\"Sin tablas Bronze\")\n",
    "    except:\n",
    "        print(\"Terminando ejecución local\")\n",
    "\n",
    "print(f\"\\n🎯 Tablas a transformar: {len(tables_to_process)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_omie_price_data(raw_content):\n",
    "    \"\"\"Extrae datos de precios de OMIE desde contenido raw\"\"\"\n",
    "    # Esta función necesitará ser customizada según el formato real de OMIE\n",
    "    # Por ahora, implementación placeholder que puede ser extendida\n",
    "    \n",
    "    if raw_content is None:\n",
    "        return None\n",
    "    \n",
    "    # Placeholder: asumir que raw_content contiene datos CSV-like\n",
    "    # En realidad, necesitarás parsear el formato específico de OMIE\n",
    "    try:\n",
    "        # Ejemplo: si OMIE usa formato CSV con headers específicos\n",
    "        lines = raw_content.split('\\n')\n",
    "        \n",
    "        # Buscar líneas que contengan datos de precios\n",
    "        price_data = []\n",
    "        for line in lines:\n",
    "            if ',' in line and any(char.isdigit() for char in line):\n",
    "                parts = line.split(',')\n",
    "                if len(parts) >= 2:\n",
    "                    # Intentar extraer hora y precio\n",
    "                    try:\n",
    "                        hour = int(parts[0]) if parts[0].isdigit() else None\n",
    "                        price = float(parts[1].replace(',', '.')) if parts[1].replace(',', '.').replace('.', '').isdigit() else None\n",
    "                        if hour is not None and price is not None:\n",
    "                            price_data.append((hour, price))\n",
    "                    except:\n",
    "                        continue\n",
    "        \n",
    "        return price_data if price_data else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def transform_bronze_to_silver(bronze_table_name, year):\n",
    "    \"\"\"Transforma una tabla Bronze OMIE a Silver con lógica de negocio\"\"\"\n",
    "    \n",
    "    print(f\"\\n🔄 Transformando {bronze_table_name} → Silver\")\n",
    "    \n",
    "    # Cargar datos Bronze\n",
    "    df_bronze = spark.table(bronze_table_name)\n",
    "    \n",
    "    print(f\"   📥 Registros Bronze: {df_bronze.count():,}\")\n",
    "    \n",
    "    # Mostrar esquema Bronze\n",
    "    print(f\"   📋 Esquema Bronze:\")\n",
    "    df_bronze.printSchema()\n",
    "    \n",
    "    # Transformaciones Silver\n",
    "    print(f\"   🔧 Aplicando transformaciones Silver...\")\n",
    "    \n",
    "    # 1. Limpieza y estandarización básica\n",
    "    df_clean = df_bronze.filter(\n",
    "        col(\"source_file\").isNotNull() & \n",
    "        (col(\"source_file\") != \"unknown\")\n",
    "    )\n",
    "    \n",
    "    # 2. Extraer información de fecha desde extraction_date\n",
    "    df_with_dates = df_clean.withColumn(\n",
    "        \"price_date\",\n",
    "        coalesce(\n",
    "            col(\"extraction_date_parsed\"),\n",
    "            to_date(col(\"extraction_date\"), \"yyyyMMdd\")\n",
    "        )\n",
    "    ).withColumn(\n",
    "        \"year\", year(col(\"price_date\"))\n",
    "    ).withColumn(\n",
    "        \"month\", month(col(\"price_date\"))\n",
    "    ).withColumn(\n",
    "        \"day\", dayofmonth(col(\"price_date\"))\n",
    "    ).withColumn(\n",
    "        \"quarter\", quarter(col(\"price_date\"))\n",
    "    ).withColumn(\n",
    "        \"day_of_week\", dayofweek(col(\"price_date\"))\n",
    "    ).withColumn(\n",
    "        \"is_weekend\", col(\"day_of_week\").isin([1, 7])  # Sunday=1, Saturday=7\n",
    "    ).withColumn(\n",
    "        \"season\",\n",
    "        when(col(\"month\").isin([12, 1, 2]), \"Winter\")\n",
    "        .when(col(\"month\").isin([3, 4, 5]), \"Spring\")\n",
    "        .when(col(\"month\").isin([6, 7, 8]), \"Summer\")\n",
    "        .otherwise(\"Autumn\")\n",
    "    )\n",
    "    \n",
    "    # 3. Clasificación de períodos de demanda (mercado eléctrico español)\n",
    "    df_with_periods = df_with_dates.withColumn(\n",
    "        \"demand_period\",\n",
    "        when(\n",
    "            (col(\"is_weekend\") == False) & \n",
    "            (hour(current_timestamp()).between(8, 22)), \"Peak\"\n",
    "        ).when(\n",
    "            (col(\"is_weekend\") == False) & \n",
    "            (\n",
    "                hour(current_timestamp()).between(6, 8) | \n",
    "                hour(current_timestamp()).between(22, 24)\n",
    "            ), \"Shoulder\"\n",
    "        ).otherwise(\"Off-Peak\")\n",
    "    )\n",
    "    \n",
    "    # 4. Placeholder para datos de precios reales\n",
    "    # NOTA: Esta lógica debe ser customizada según el formato real de OMIE\n",
    "    df_with_prices = df_with_periods.withColumn(\n",
    "        \"price_hour\", lit(12)  # Placeholder - extraer de raw_content\n",
    "    ).withColumn(\n",
    "        \"marginal_price_eur_mwh\", lit(50.0)  # Placeholder - extraer de raw_content\n",
    "    ).withColumn(\n",
    "        \"energy_volume_mwh\", lit(1000.0)  # Placeholder - extraer de raw_content\n",
    "    ).withColumn(\n",
    "        \"price_category\",\n",
    "        when(col(\"marginal_price_eur_mwh\") > 80, \"High\")\n",
    "        .when(col(\"marginal_price_eur_mwh\") > 40, \"Medium\")\n",
    "        .otherwise(\"Low\")\n",
    "    )\n",
    "    \n",
    "    # 5. Crear timestamp combinado de fecha y hora\n",
    "    df_with_timestamp = df_with_prices.withColumn(\n",
    "        \"price_datetime\",\n",
    "        to_timestamp(\n",
    "            concat(\n",
    "                date_format(col(\"price_date\"), \"yyyy-MM-dd\"),\n",
    "                lit(\" \"),\n",
    "                format_string(\"%02d:00:00\", col(\"price_hour\"))\n",
    "            ),\n",
    "            \"yyyy-MM-dd HH:mm:ss\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 6. Métricas de calidad mejoradas\n",
    "    df_with_quality = df_with_timestamp.withColumn(\n",
    "        \"data_completeness_score\",\n",
    "        when(col(\"price_date\").isNotNull(), 1.0)\n",
    "        .when(col(\"extraction_date\").isNotNull(), 0.8)\n",
    "        .otherwise(0.3)\n",
    "    ).withColumn(\n",
    "        \"is_anomaly\", lit(False)  # Placeholder para detección de anomalías\n",
    "    ).withColumn(\n",
    "        \"confidence_level\", lit(\"High\")\n",
    "    )\n",
    "    \n",
    "    # 7. Metadatos de procesamiento\n",
    "    df_final = df_with_quality.withColumn(\n",
    "        \"silver_processed_at\", current_timestamp()\n",
    "    ).withColumn(\n",
    "        \"silver_processing_version\", lit(\"1.0\")\n",
    "    )\n",
    "    \n",
    "    # 8. Seleccionar columnas finales para Silver\n",
    "    silver_columns = [\n",
    "        # Dimensiones de tiempo\n",
    "        \"price_date\", \"price_hour\", \"price_datetime\",\n",
    "        \"year\", \"month\", \"day\", \"quarter\", \"day_of_week\", \"is_weekend\", \"season\",\n",
    "        \n",
    "        # Métricas de precio\n",
    "        \"marginal_price_eur_mwh\", \"energy_volume_mwh\", \"price_category\",\n",
    "        \n",
    "        # Clasificaciones de negocio\n",
    "        \"demand_period\",\n",
    "        \n",
    "        # Calidad de datos\n",
    "        \"data_completeness_score\", \"is_anomaly\", \"confidence_level\",\n",
    "        \n",
    "        # Lineage y metadatos\n",
    "        \"source_file\", \"source_url\", \"extraction_year\", \"extraction_date\",\n",
    "        \"ingested_at\", \"silver_processed_at\", \"silver_processing_version\"\n",
    "    ]\n",
    "    \n",
    "    df_silver = df_final.select(*silver_columns)\n",
    "    \n",
    "    # Deduplicar por archivo fuente y fecha\n",
    "    df_deduped = df_silver.dropDuplicates([\"source_file\", \"price_date\"])\n",
    "    \n",
    "    final_count = df_deduped.count()\n",
    "    print(f\"   📤 Registros Silver: {final_count:,}\")\n",
    "    \n",
    "    return df_deduped, final_count\n",
    "\n",
    "print(\"✅ Funciones de transformación definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b22f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Procesar cada tabla Bronze a Silver\n",
    "# -------------------------------\n",
    "\n",
    "silver_tables_created = []\n",
    "total_silver_records = 0\n",
    "\n",
    "for bronze_table, year, bronze_count in tables_to_process:\n",
    "    print(f\"\\n📦 Procesando {bronze_table} (año {year})\")\n",
    "    \n",
    "    try:\n",
    "        # Aplicar transformaciones\n",
    "        df_silver, silver_count = transform_bronze_to_silver(bronze_table, year)\n",
    "        \n",
    "        # Nombre de tabla Silver\n",
    "        silver_table_name = f\"{silver_table_prefix}_{year}\"\n",
    "        \n",
    "        print(f\"   💾 Guardando tabla Silver: {silver_table_name}\")\n",
    "        \n",
    "        # Guardar como tabla Delta con particionado optimizado\n",
    "        df_silver.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .partitionBy(\"year\", \"month\") \\\n",
    "            .saveAsTable(silver_table_name)\n",
    "        \n",
    "        # Verificar tabla creada\n",
    "        final_verification = spark.table(silver_table_name).count()\n",
    "        \n",
    "        print(f\"   ✅ {silver_table_name} creada ({final_verification:,} registros)\")\n",
    "        \n",
    "        # Mostrar muestra de datos Silver\n",
    "        print(f\"   🔍 Muestra Silver:\")\n",
    "        spark.table(silver_table_name).select(\n",
    "            \"price_date\", \"price_hour\", \"marginal_price_eur_mwh\", \n",
    "            \"season\", \"demand_period\", \"data_completeness_score\"\n",
    "        ).show(3, truncate=False)\n",
    "        \n",
    "        silver_tables_created.append({\n",
    "            \"bronze_table\": bronze_table,\n",
    "            \"silver_table\": silver_table_name,\n",
    "            \"year\": year,\n",
    "            \"bronze_records\": bronze_count,\n",
    "            \"silver_records\": final_verification\n",
    "        })\n",
    "        \n",
    "        total_silver_records += final_verification\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error procesando {bronze_table}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(f\"\\n🎉 Transformación Bronze→Silver completada!\")\n",
    "print(f\"📊 Tablas Silver creadas: {len(silver_tables_created)}\")\n",
    "print(f\"📈 Total registros Silver: {total_silver_records:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a230c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Validación y estadísticas finales\n",
    "# -------------------------------\n",
    "\n",
    "if silver_tables_created:\n",
    "    print(f\"📋 Resumen de transformación OMIE Bronze→Silver:\")\n",
    "    \n",
    "    for table_info in silver_tables_created:\n",
    "        bronze_table = table_info[\"bronze_table\"]\n",
    "        silver_table = table_info[\"silver_table\"]\n",
    "        year = table_info[\"year\"]\n",
    "        bronze_records = table_info[\"bronze_records\"]\n",
    "        silver_records = table_info[\"silver_records\"]\n",
    "        \n",
    "        transformation_ratio = (silver_records / bronze_records * 100) if bronze_records > 0 else 0\n",
    "        \n",
    "        print(f\"\\n📅 Año {year}:\")\n",
    "        print(f\"   🔧 {bronze_table} → {silver_table}\")\n",
    "        print(f\"   📊 {bronze_records:,} → {silver_records:,} registros ({transformation_ratio:.1f}%)\")\n",
    "        \n",
    "        # Estadísticas de calidad Silver\n",
    "        try:\n",
    "            silver_df = spark.table(silver_table)\n",
    "            \n",
    "            quality_stats = silver_df.select(\n",
    "                avg(\"data_completeness_score\").alias(\"avg_completeness\"),\n",
    "                countDistinct(\"source_file\").alias(\"unique_files\"),\n",
    "                min(\"price_date\").alias(\"min_date\"),\n",
    "                max(\"price_date\").alias(\"max_date\"),\n",
    "                avg(\"marginal_price_eur_mwh\").alias(\"avg_price\")\n",
    "            ).collect()[0]\n",
    "            \n",
    "            print(f\"   🎯 Completeness promedio: {quality_stats['avg_completeness']:.2f}\")\n",
    "            print(f\"   📁 Archivos únicos: {quality_stats['unique_files']}\")\n",
    "            print(f\"   📅 Rango: {quality_stats['min_date']} a {quality_stats['max_date']}\")\n",
    "            print(f\"   💰 Precio promedio: {quality_stats['avg_price']:.2f} EUR/MWh\")\n",
    "            \n",
    "            # Distribución por período de demanda\n",
    "            demand_dist = silver_df.groupBy(\"demand_period\").count().collect()\n",
    "            print(f\"   ⚡ Distribución demanda: {dict((row.demand_period, row.count) for row in demand_dist)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️  Error en estadísticas: {e}\")\n",
    "    \n",
    "    # Crear vista unificada Silver\n",
    "    if len(silver_tables_created) > 1:\n",
    "        print(f\"\\n🔄 Creando vista Silver unificada...\")\n",
    "        \n",
    "        try:\n",
    "            union_query = \" UNION ALL \".join([\n",
    "                f\"SELECT * FROM {t['silver_table']}\" for t in silver_tables_created\n",
    "            ])\n",
    "            \n",
    "            unified_view_name = \"slv_OMIE_all_years\"\n",
    "            spark.sql(f\"CREATE OR REPLACE VIEW {unified_view_name} AS {union_query}\")\n",
    "            \n",
    "            # Estadísticas consolidadas\n",
    "            unified_stats = spark.sql(f\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as total_records,\n",
    "                    MIN(year) as min_year,\n",
    "                    MAX(year) as max_year,\n",
    "                    AVG(data_completeness_score) as avg_quality,\n",
    "                    AVG(marginal_price_eur_mwh) as avg_price\n",
    "                FROM {unified_view_name}\n",
    "            \"\"\")\n",
    "            \n",
    "            print(f\"   📊 Vista unificada: {unified_view_name}\")\n",
    "            unified_stats.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️  Error creando vista unificada: {e}\")\n",
    "    \n",
    "    # Resumen ejecutivo\n",
    "    print(f\"\\n📊 Resumen Ejecutivo:\")\n",
    "    print(f\"   🏗️  Tablas Silver: {len(silver_tables_created)}\")\n",
    "    print(f\"   📅 Años procesados: {sorted([t['year'] for t in silver_tables_created])}\")\n",
    "    print(f\"   📈 Total registros: {total_silver_records:,}\")\n",
    "    print(f\"   🎯 Transformación: Bronze → Silver completada\")\n",
    "    \n",
    "    result_summary = {\n",
    "        \"tables_created\": len(silver_tables_created),\n",
    "        \"years_processed\": [t['year'] for t in silver_tables_created],\n",
    "        \"total_records\": total_silver_records\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No se crearon tablas Silver\")\n",
    "    result_summary = {\"error\": \"No tables created\"}\n",
    "\n",
    "print(f\"\\n🏁 Finalizando transformación Delta...\")\n",
    "\n",
    "# Salida para pipeline\n",
    "try:\n",
    "    dbutils.notebook.exit(json.dumps(result_summary))\n",
    "except:\n",
    "    print(f\"✅ Ejecución local completada: {result_summary}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
